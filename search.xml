<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>[AAIS] E04. Transfer Learning</title>
      <link href="2021/01/27/aais-04/"/>
      <url>2021/01/27/aais-04/</url>
      
        <content type="html"><![CDATA[<h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><br><h2 id="About-VGG16"><a href="#About-VGG16" class="headerlink" title="About VGG16"></a>About VGG16</h2><hr><p><a href="https://analysisbugs.tistory.com/65?category=839091">https://analysisbugs.tistory.com/65?category=839091</a><br><br><a href="https://bskyvision.com/504">https://bskyvision.com/504</a><br><br><a href="https://medium.com/@msmapark2/vgg16-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a">https://medium.com/@msmapark2/vgg16-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a</a></p><p>이전의 모델들은 대부분 7x7 filter, 11x11 filter를 사용하였으나 이부분부터 3x3 filter를 사용하였습니다.</p><p>예시로 10x10을 7x7 filter를 통과시키면 feature map이 4x4가 만들어지는데 필요한 파라미터 수는 7x7 = 49이고<br>10x10모델을 3x3 filter를 한번통과시키면 8x8 map &gt; 두번통과시키면 6x6 map &gt; 세번통과시키면 4x4 featur map이 만들어지는데 필요한 파라미터수는 3x3x3 = 27로 크게 줄어듭니다.<br><br><br><br></p><blockquote><p>Q.이렇게 할 경우 왜 좋은가? : 3x3일경우 패딩이 1이라서 경계선의 추론이 더 쉬워진다?  + relu를 세번통과하기 때문에 <strong>비 선형성이 증가하게 된다.</strong></p></blockquote><br><br><hr><h2 id="transfer-learning"><a href="#transfer-learning" class="headerlink" title="transfer learning"></a>transfer learning</h2><ol><li>모델을 가져와서 ‘include_top=False’를 통해 분류층(classification)을 삭제시킨다.</li><li>‘model.trainable = False’로 설정시켜서 동결(freezing)을 시킨다.</li><li>global_average_pooling과 dense layer를 쌓은 후 컴파일한다.</li><li>ImageDataGenerator를 통해 적은량의 데이터를 data augmentation을 진행시킨 후 학습시킨다.</li><li>‘model.trainable = True’로 설정시켜서 동결을 해제한다.</li><li>learning_rate을 매우 낮게 설정하여 fine-tuning하기</li></ol><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><pre><code>Mon Jan 25 13:30:28 2021+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 || N/A   41C    P0    49W / 300W |    947MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 || N/A   39C    P0    40W / 300W |     12MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 || N/A   40C    P0    52W / 300W |   5134MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 || N/A   39C    P0    38W / 300W |     38MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================|+-----------------------------------------------------------------------------+</code></pre><hr><h3 id="1-classification층을-삭제"><a href="#1-classification층을-삭제" class="headerlink" title="1. classification층을 삭제"></a>1. classification층을 삭제</h3><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.application.VGG16(weights,input_shape,include_top)</span><br></pre></td></tr></table></figure><p><code>include_top</code>은 최상단 Layer인 fully connected layer를 포함시키는지 여부를 전달시키는 것으로 pre-trained된 것을 새롭게 prediction하기위해 이 layer는 제거시켜줍니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">base_model = keras.applications.VGG16(</span><br><span class="line">    weights=<span class="string">&#x27;imagenet&#x27;</span>,  <span class="comment"># Load weights pre-trained on ImageNet.</span></span><br><span class="line">    input_shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>),</span><br><span class="line">    include_top=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;vgg16&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================input_1 (InputLayer)         [(None, 224, 224, 3)]     0_________________________________________________________________block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792_________________________________________________________________block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928_________________________________________________________________block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0_________________________________________________________________block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856_________________________________________________________________block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584_________________________________________________________________block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0_________________________________________________________________block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168_________________________________________________________________block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080_________________________________________________________________block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080_________________________________________________________________block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0_________________________________________________________________block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160_________________________________________________________________block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808_________________________________________________________________block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808_________________________________________________________________block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0_________________________________________________________________block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808_________________________________________________________________block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808_________________________________________________________________block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808_________________________________________________________________block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0=================================================================Total params: 14,714,688Trainable params: 14,714,688Non-trainable params: 0_________________________________________________________________</code></pre><p>이전과 다르게 <code>classification</code> layer이 삭제되었다. (<code>Flatten</code> ~ <code>Dense</code>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_model.trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure><h3 id="2-기존의-네트워크-동결시키기"><a href="#2-기존의-네트워크-동결시키기" class="headerlink" title="2. 기존의 네트워크 동결시키기"></a>2. 기존의 네트워크 동결시키기</h3><p><code>trainable</code>옵션을 False로 하여 새로운 학습을 할 때에 기존의 학습된 네트워크가 같이 학습되지 않도록 합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inputs = keras.Input(shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># Separately from setting trainable on the model, we set training to False</span></span><br><span class="line">x = base_model(inputs, training=<span class="literal">False</span>)</span><br><span class="line">x = keras.layers.GlobalAveragePooling2D()(x)</span><br><span class="line"><span class="comment"># A Dense classifier with a single unit (binary classification)</span></span><br><span class="line">outputs = keras.layers.Dense(<span class="number">1</span>)(x)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================input_2 (InputLayer)         [(None, 224, 224, 3)]     0_________________________________________________________________vgg16 (Model)                (None, 7, 7, 512)         14714688_________________________________________________________________global_average_pooling2d (Gl (None, 512)               0_________________________________________________________________dense (Dense)                (None, 1)                 513=================================================================Total params: 14,715,201Trainable params: 513Non-trainable params: 14,714,688_________________________________________________________________</code></pre><h4 id="about-layer"><a href="#about-layer" class="headerlink" title="about layer"></a>about layer</h4><ol><li>GlobalAveragePooing2D</li></ol><ul><li>기존의 fully convolutional Network는 많은 parameter수와 계산이 오래걸리고, 위치의 정보가 모두 사라지는 단점이 있습니다.</li><li>같은 chennel의 feature들의 평균을 내어 채널의 개수만큼 원소를 가지는 벡터를 출력 (이경우에는 입력 층이 (7,7,512)이므로 (512,)를 출력시키는 layer)</li><li>fully connected layer를 없애기 위한 방법</li><li><a href="https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/keras/_impl/keras/layers/pooling.py">코드</a></li></ul><ol start="2"><li>Dense(1)</li></ol><ul><li>binary로 클래스를 분류할 것이기 때문에 0과 1로 classification</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Important to use binary crossentropy and binary accuracy as we now have a binary classification problem</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>), metrics=[keras.metrics.BinaryAccuracy()])</span><br></pre></td></tr></table></figure><h3 id="before-data-augmentation"><a href="#before-data-augmentation" class="headerlink" title="before data augmentation"></a>before data augmentation</h3><hr><p><img src="/image/check.jpg" alt="check"></p><p>위 그림과같이 <code>ImageDataGenerator.flow_from_directory</code>를 사용할 때에 hidden folder가 있는지 확인해야합니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dog</span><br><span class="line">        test</span><br><span class="line">                bo</span><br><span class="line">                not_bo</span><br><span class="line">        train</span><br><span class="line">                bo</span><br><span class="line">                not_bo</span><br></pre></td></tr></table></figure><p>위와 같이 directory별로 class를 구분하기 때문에 <code>.ipynb_checkpoints</code>폴더를 삭제해야합니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="comment"># create a data generator</span></span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">        samplewise_center=<span class="literal">True</span>,  <span class="comment"># set each sample mean to 0</span></span><br><span class="line">        rotation_range=<span class="number">10</span>,  <span class="comment"># randomly rotate images in the range (degrees, 0 to 180)</span></span><br><span class="line">        zoom_range = <span class="number">0.1</span>, <span class="comment"># Randomly zoom image</span></span><br><span class="line">        width_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images horizontally (fraction of total width)</span></span><br><span class="line">        height_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images vertically (fraction of total height)</span></span><br><span class="line">        horizontal_flip=<span class="literal">True</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">        vertical_flip=<span class="literal">False</span>) <span class="comment"># we don&#x27;t expect Bo to be upside-down so we will not flip vertically</span></span><br></pre></td></tr></table></figure><h2 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load and iterate training dataset</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">train_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/train/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                        save_to_dir=&#x27;../sungjin/dog2/train/&#x27;, //2700여개의 파일이 생성됨</span></span><br><span class="line">                                       seed = seed,</span><br><span class="line">                                       target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                       color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                       class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                       batch_size=<span class="number">8</span>)</span><br><span class="line"><span class="comment"># load and iterate test dataset</span></span><br><span class="line">test_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/test/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                       save_to_dir=&#x27;../sungjin/dog2/test/&#x27;, // 1600여개의 파일이 생성됨</span></span><br><span class="line">                                      seed = seed,</span><br><span class="line">                                      target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                      color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                      class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                      batch_size=<span class="number">8</span>)</span><br></pre></td></tr></table></figure><pre><code>Found 139 images belonging to 2 classes.Found 20 images belonging to 2 classes.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_it, steps_per_epoch=<span class="number">12</span>, validation_data=test_it, validation_steps=<span class="number">4</span>, epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/2012/12 [==============================] - 3s 225ms/step - loss: 1.4831 - binary_accuracy: 0.6562 - val_loss: 3.5233 - val_binary_accuracy: 0.5357Epoch 2/2012/12 [==============================] - 2s 178ms/step - loss: 1.1998 - binary_accuracy: 0.7473 - val_loss: 1.9760 - val_binary_accuracy: 0.6667Epoch 3/2012/12 [==============================] - 2s 161ms/step - loss: 0.4746 - binary_accuracy: 0.8462 - val_loss: 1.3999 - val_binary_accuracy: 0.7143Epoch 4/2012/12 [==============================] - 2s 172ms/step - loss: 0.3132 - binary_accuracy: 0.8646 - val_loss: 1.0363 - val_binary_accuracy: 0.7857Epoch 5/2012/12 [==============================] - 2s 175ms/step - loss: 0.2785 - binary_accuracy: 0.9011 - val_loss: 1.0750 - val_binary_accuracy: 0.7917Epoch 6/2012/12 [==============================] - 2s 178ms/step - loss: 0.0651 - binary_accuracy: 0.9670 - val_loss: 1.3541 - val_binary_accuracy: 0.7143Epoch 7/2012/12 [==============================] - 2s 198ms/step - loss: 0.1798 - binary_accuracy: 0.9167 - val_loss: 0.9755 - val_binary_accuracy: 0.8214Epoch 8/2012/12 [==============================] - 2s 166ms/step - loss: 0.0315 - binary_accuracy: 0.9890 - val_loss: 0.5196 - val_binary_accuracy: 0.9167Epoch 9/2012/12 [==============================] - 2s 179ms/step - loss: 0.0822 - binary_accuracy: 0.9341 - val_loss: 0.3601 - val_binary_accuracy: 0.8929Epoch 10/2012/12 [==============================] - 2s 181ms/step - loss: 0.0169 - binary_accuracy: 0.9896 - val_loss: 0.3563 - val_binary_accuracy: 0.9286Epoch 11/2012/12 [==============================] - 2s 191ms/step - loss: 0.0437 - binary_accuracy: 0.9890 - val_loss: 0.4602 - val_binary_accuracy: 0.8333Epoch 12/2012/12 [==============================] - 2s 158ms/step - loss: 0.0405 - binary_accuracy: 0.9890 - val_loss: 0.2497 - val_binary_accuracy: 0.8929Epoch 13/2012/12 [==============================] - 2s 191ms/step - loss: 0.0083 - binary_accuracy: 1.0000 - val_loss: 0.4642 - val_binary_accuracy: 0.8571Epoch 14/2012/12 [==============================] - 2s 163ms/step - loss: 0.0133 - binary_accuracy: 0.9890 - val_loss: 0.2259 - val_binary_accuracy: 0.9167Epoch 15/2012/12 [==============================] - 2s 178ms/step - loss: 0.0050 - binary_accuracy: 1.0000 - val_loss: 0.2705 - val_binary_accuracy: 0.8929Epoch 16/2012/12 [==============================] - 2s 185ms/step - loss: 0.0027 - binary_accuracy: 1.0000 - val_loss: 0.2617 - val_binary_accuracy: 0.8929Epoch 17/2012/12 [==============================] - 2s 179ms/step - loss: 0.0203 - binary_accuracy: 0.9890 - val_loss: 0.1751 - val_binary_accuracy: 0.9583Epoch 18/2012/12 [==============================] - 2s 173ms/step - loss: 0.0055 - binary_accuracy: 1.0000 - val_loss: 0.0705 - val_binary_accuracy: 1.0000Epoch 19/2012/12 [==============================] - 2s 199ms/step - loss: 0.0015 - binary_accuracy: 1.0000 - val_loss: 0.0275 - val_binary_accuracy: 1.0000Epoch 20/2012/12 [==============================] - 2s 189ms/step - loss: 0.0056 - binary_accuracy: 1.0000 - val_loss: 0.1403 - val_binary_accuracy: 0.9583</code></pre><h2 id="기존의-네트워크-동결-해제-및-fine-tuning"><a href="#기존의-네트워크-동결-해제-및-fine-tuning" class="headerlink" title="기존의 네트워크 동결 해제 및 fine-tuning"></a>기존의 네트워크 동결 해제 및 fine-tuning</h2><p>새롭게 컴파일할 때 기존의 네트워크를 unfreezing하여 fine-tuning(미세조정)한다. <br><br>단, 이때 다음과 같은 세가지의 전략이 있다.</p><p align = "center"><img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FH4rr7%2FbtqvBNchX4H%2FRmikaKBNm2N6VYD9JbI800%2Fimg.png", width="700px"></p><hr><ol><li>전체모델을 학습시키기</li></ol><ul><li>이 방법은 pre-trained model의 구조만 사용하고 새로운 데이터셋에 맞게 전부 새로 학습시키는 방법입니다. 큰 사이즈의 데이터셋이 필요합니다.</li></ul><br><ol start="2"><li>일부의 층만 학습시키고 나머지는 동결시키기</li></ol><ul><li>초기 레이어는 추상적인(일반적인)특징을 추출하고 후기 레이어는 구체적이고 특유한 특징을 추출하는데 이런 특성을 이용해서 선택적으로 재학습시킵니다.</li><li>데이터셋이 작고 모델의 파라미터가 많다면 오버피팅이 될 수 있으므로 적은양의 계층을 학습시킵니다.</li><li>데이터셋이 크고 그에 비해 모델의 파라미터가 적다면 오버피팅의 가능성이 적으므로 더 많은 양의 계층을 학습시킵니다.</li></ul><br><ol start="3"><li>전체 동결시키기 (classification만 학습시키기)</li></ol><ul><li>기존의 네트워크는 건들지 않고 그대로 두면서 특징 추출 메커니즘으로써 활용하고, classifier만 재학습시키는 방법을 씁니다.</li><li>데이터셋이 너무 작을때나 새로운 문제가 이전 모델이 이미 학습한 데이터셋과 매우 비슷할 때 사용합니다.</li></ul><hr><p align = "center"><img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FxjNT2%2FbtqvC7gR5Vw%2FWW1IlF9M9DTCypyNTeC2Wk%2Fimg.png", width= "900px"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unfreeze the base model</span></span><br><span class="line">base_model.trainable = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s important to recompile your model after you make any changes</span></span><br><span class="line"><span class="comment"># to the `trainable` attribute of any inner layer, so that your changes</span></span><br><span class="line"><span class="comment"># are taken into account</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = keras.optimizers.Adam(lr = <span class="number">.00001</span>),</span><br><span class="line"><span class="comment">#               optimizer=keras.optimizers.RMSprop(learning_rate = .00001),  # Very low learning rate</span></span><br><span class="line">              loss=keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              metrics=[keras.metrics.BinaryAccuracy()])</span><br></pre></td></tr></table></figure><h3 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a><a href="%22https://cs231n.github.io/neural-networks-3/#ada%22">optimizer</a></h3><p><img src = "/image/minibatch.jpg", width = "700px"></p><br><p><img src = "/image/optimizer.jpg", width = "700px"></p><p><img src = "https://cs231n.github.io/assets/nn3/opt2.gif",  width="400px"><img src = "https://cs231n.github.io/assets/nn3/opt1.gif", width = "400px"></p><p><a href="%22https://dalpo0814.tistory.com/29%22">reference</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_it, steps_per_epoch=<span class="number">12</span>, validation_data=test_it, validation_steps=<span class="number">4</span>, epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/2012/12 [==============================] - 3s 219ms/step - loss: 0.0849 - binary_accuracy: 0.9670 - val_loss: 0.0158 - val_binary_accuracy: 1.0000Epoch 2/2012/12 [==============================] - 2s 204ms/step - loss: 0.0053 - binary_accuracy: 1.0000 - val_loss: 0.0118 - val_binary_accuracy: 1.0000Epoch 3/2012/12 [==============================] - 2s 176ms/step - loss: 0.0020 - binary_accuracy: 1.0000 - val_loss: 0.0015 - val_binary_accuracy: 1.0000Epoch 4/2012/12 [==============================] - 2s 196ms/step - loss: 1.5012e-04 - binary_accuracy: 1.0000 - val_loss: 0.0154 - val_binary_accuracy: 1.0000Epoch 5/2012/12 [==============================] - 3s 211ms/step - loss: 1.7610e-04 - binary_accuracy: 1.0000 - val_loss: 0.0034 - val_binary_accuracy: 1.0000Epoch 6/2012/12 [==============================] - 2s 198ms/step - loss: 1.0886e-04 - binary_accuracy: 1.0000 - val_loss: 0.0275 - val_binary_accuracy: 1.0000Epoch 7/2012/12 [==============================] - 2s 178ms/step - loss: 2.9665e-05 - binary_accuracy: 1.0000 - val_loss: 0.0093 - val_binary_accuracy: 1.0000Epoch 8/2012/12 [==============================] - 2s 205ms/step - loss: 1.7676e-05 - binary_accuracy: 1.0000 - val_loss: 0.0108 - val_binary_accuracy: 1.0000Epoch 9/2012/12 [==============================] - 2s 182ms/step - loss: 3.1658e-05 - binary_accuracy: 1.0000 - val_loss: 8.7812e-04 - val_binary_accuracy: 1.0000Epoch 10/2012/12 [==============================] - 2s 195ms/step - loss: 3.5808e-05 - binary_accuracy: 1.0000 - val_loss: 0.0020 - val_binary_accuracy: 1.0000Epoch 11/2012/12 [==============================] - 2s 191ms/step - loss: 3.2332e-05 - binary_accuracy: 1.0000 - val_loss: 0.0474 - val_binary_accuracy: 0.9643Epoch 12/2012/12 [==============================] - 2s 202ms/step - loss: 1.3189e-04 - binary_accuracy: 1.0000 - val_loss: 0.0044 - val_binary_accuracy: 1.0000Epoch 13/2012/12 [==============================] - 2s 202ms/step - loss: 1.7362e-04 - binary_accuracy: 1.0000 - val_loss: 0.0073 - val_binary_accuracy: 1.0000Epoch 14/2012/12 [==============================] - 2s 194ms/step - loss: 2.4271e-05 - binary_accuracy: 1.0000 - val_loss: 0.0235 - val_binary_accuracy: 1.0000Epoch 15/2012/12 [==============================] - 3s 219ms/step - loss: 1.0316e-05 - binary_accuracy: 1.0000 - val_loss: 0.0388 - val_binary_accuracy: 1.0000Epoch 16/2012/12 [==============================] - 2s 187ms/step - loss: 2.3622e-05 - binary_accuracy: 1.0000 - val_loss: 0.0031 - val_binary_accuracy: 1.0000Epoch 17/2012/12 [==============================] - 2s 201ms/step - loss: 2.0185e-05 - binary_accuracy: 1.0000 - val_loss: 0.0413 - val_binary_accuracy: 0.9643Epoch 18/2012/12 [==============================] - 2s 175ms/step - loss: 3.8968e-05 - binary_accuracy: 1.0000 - val_loss: 0.0197 - val_binary_accuracy: 1.0000Epoch 19/2012/12 [==============================] - 2s 202ms/step - loss: 9.7374e-06 - binary_accuracy: 1.0000 - val_loss: 0.0902 - val_binary_accuracy: 0.9643Epoch 20/2012/12 [==============================] - 2s 206ms/step - loss: 1.0378e-04 - binary_accuracy: 1.0000 - val_loss: 0.0148 - val_binary_accuracy: 1.0000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputs = keras.Input(shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># Separately from setting trainable on the model, we set training to False</span></span><br><span class="line">x = base_model(inputs, training=<span class="literal">False</span>)</span><br><span class="line">x = keras.layers.Flatten(input_shape=(<span class="number">7</span>,<span class="number">7</span>,<span class="number">512</span>))(x)</span><br><span class="line"><span class="comment"># x = keras.layers.Dense(256, activation=&#x27;relu&#x27;,input_dim=(7*7*512))(x)</span></span><br><span class="line"><span class="comment"># x = keras.layers.Dropout(0.2)(x)</span></span><br><span class="line">outputs = keras.layers.Dense(<span class="number">1</span>,activation = <span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;model&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================input_2 (InputLayer)         [(None, 224, 224, 3)]     0_________________________________________________________________vgg16 (Model)                (None, 7, 7, 512)         14714688_________________________________________________________________flatten (Flatten)            (None, 25088)             0_________________________________________________________________dense (Dense)                (None, 1)                 25089=================================================================Total params: 14,739,777Trainable params: 25,089Non-trainable params: 14,714,688_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Important to use binary crossentropy and binary accuracy as we now have a binary classification problem</span></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line"><span class="comment">#     optimizer=keras.optimizers.Adam(),</span></span><br><span class="line">              loss=keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              metrics=[keras.metrics.BinaryAccuracy()])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="comment"># create a data generator</span></span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">        samplewise_center=<span class="literal">True</span>,  <span class="comment"># set each sample mean to 0</span></span><br><span class="line">        rotation_range=<span class="number">10</span>,  <span class="comment"># randomly rotate images in the range (degrees, 0 to 180)</span></span><br><span class="line">        zoom_range = <span class="number">0.1</span>, <span class="comment"># Randomly zoom image</span></span><br><span class="line">        width_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images horizontally (fraction of total width)</span></span><br><span class="line">        height_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images vertically (fraction of total height)</span></span><br><span class="line">        horizontal_flip=<span class="literal">True</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">        vertical_flip=<span class="literal">False</span>) <span class="comment"># we don&#x27;t expect Bo to be upside-down so we will not flip vertically</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load and iterate training dataset</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">train_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/train/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                        save_to_dir=&#x27;../sungjin/dog2/train/&#x27;, //2700여개의 파일이 생성됨</span></span><br><span class="line">                                       seed = seed,</span><br><span class="line">                                       target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                       color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                       class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                       batch_size=<span class="number">8</span>)</span><br><span class="line"><span class="comment"># load and iterate test dataset</span></span><br><span class="line">test_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/test/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                       save_to_dir=&#x27;../sungjin/dog2/test/&#x27;, // 1600여개의 파일이 생성됨</span></span><br><span class="line">                                      seed = seed,</span><br><span class="line">                                      target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                      color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                      class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                      batch_size=<span class="number">8</span>)</span><br></pre></td></tr></table></figure><pre><code>Found 139 images belonging to 2 classes.Found 20 images belonging to 2 classes.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_it, steps_per_epoch=<span class="number">12</span>, validation_data=test_it, validation_steps=<span class="number">4</span>, epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/2012/12 [==============================] - 3s 243ms/step - loss: 0.4516 - binary_accuracy: 0.8333 - val_loss: 0.8132 - val_binary_accuracy: 0.5000Epoch 2/2012/12 [==============================] - 2s 195ms/step - loss: 0.4823 - binary_accuracy: 0.8352 - val_loss: 0.8549 - val_binary_accuracy: 0.4583Epoch 3/2012/12 [==============================] - 2s 179ms/step - loss: 0.4232 - binary_accuracy: 0.8901 - val_loss: 0.8483 - val_binary_accuracy: 0.4643Epoch 4/2012/12 [==============================] - 2s 202ms/step - loss: 0.4383 - binary_accuracy: 0.8750 - val_loss: 0.8490 - val_binary_accuracy: 0.4643Epoch 5/2012/12 [==============================] - 2s 195ms/step - loss: 0.4781 - binary_accuracy: 0.8352 - val_loss: 0.8133 - val_binary_accuracy: 0.5000Epoch 6/2012/12 [==============================] - 2s 199ms/step - loss: 0.4341 - binary_accuracy: 0.8791 - val_loss: 0.7775 - val_binary_accuracy: 0.5357Epoch 7/2012/12 [==============================] - 2s 204ms/step - loss: 0.4526 - binary_accuracy: 0.8646 - val_loss: 0.7197 - val_binary_accuracy: 0.6071Epoch 8/2012/12 [==============================] - 2s 170ms/step - loss: 0.4012 - binary_accuracy: 0.9121 - val_loss: 0.8549 - val_binary_accuracy: 0.4583Epoch 9/2012/12 [==============================] - 2s 197ms/step - loss: 0.4823 - binary_accuracy: 0.8352 - val_loss: 0.8490 - val_binary_accuracy: 0.4643Epoch 10/2012/12 [==============================] - 2s 201ms/step - loss: 0.4070 - binary_accuracy: 0.9062 - val_loss: 0.8133 - val_binary_accuracy: 0.5000Epoch 11/2012/12 [==============================] - 2s 205ms/step - loss: 0.5111 - binary_accuracy: 0.8022 - val_loss: 0.7716 - val_binary_accuracy: 0.5417Epoch 12/2012/12 [==============================] - 2s 181ms/step - loss: 0.4341 - binary_accuracy: 0.8791 - val_loss: 0.8490 - val_binary_accuracy: 0.4643Epoch 13/2012/12 [==============================] - 3s 216ms/step - loss: 0.4383 - binary_accuracy: 0.8750 - val_loss: 0.8133 - val_binary_accuracy: 0.5000Epoch 14/2012/12 [==============================] - 2s 190ms/step - loss: 0.4451 - binary_accuracy: 0.8681 - val_loss: 0.8133 - val_binary_accuracy: 0.5000Epoch 15/2012/12 [==============================] - 2s 191ms/step - loss: 0.4603 - binary_accuracy: 0.8571 - val_loss: 0.8847 - val_binary_accuracy: 0.4286Epoch 16/2012/12 [==============================] - 2s 202ms/step - loss: 0.4278 - binary_accuracy: 0.8854 - val_loss: 0.8133 - val_binary_accuracy: 0.5000Epoch 17/2012/12 [==============================] - 2s 205ms/step - loss: 0.4451 - binary_accuracy: 0.8681 - val_loss: 0.8549 - val_binary_accuracy: 0.4583Epoch 18/2012/12 [==============================] - 2s 179ms/step - loss: 0.4781 - binary_accuracy: 0.8352 - val_loss: 0.8490 - val_binary_accuracy: 0.4643Epoch 19/2012/12 [==============================] - 2s 207ms/step - loss: 0.4695 - binary_accuracy: 0.8438 - val_loss: 0.7775 - val_binary_accuracy: 0.5357Epoch 20/2012/12 [==============================] - 2s 185ms/step - loss: 0.4232 - binary_accuracy: 0.8901 - val_loss: 0.8549 - val_binary_accuracy: 0.4583</code></pre><h2 id="Global-Average-Pooing-vs-Fully-Convolutional-Network"><a href="#Global-Average-Pooing-vs-Fully-Convolutional-Network" class="headerlink" title="Global Average Pooing vs Fully Convolutional Network"></a><code>Global Average Pooing</code> vs <code>Fully Convolutional Network</code></h2><p><img src = "/image/GAP.jpg", width = "400px"><img src = "/image/fullyConvolution.jpg", width = "400px"></p>]]></content>
      
      
      <categories>
          
          <category> AAIS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> AAIS </tag>
            
            <tag> keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Java] Day04 - 제어문</title>
      <link href="2021/01/25/java-day4/"/>
      <url>2021/01/25/java-day4/</url>
      
        <content type="html"><![CDATA[<h1 id="Day04-제어문"><a href="#Day04-제어문" class="headerlink" title="Day04 제어문"></a>Day04 제어문</h1><h2 id="Review-Day03"><a href="#Review-Day03" class="headerlink" title="Review Day03"></a>Review Day03</h2><hr><blockquote><p>Q1. <code>mid = (start + end)</code> / 2의 문제점은?</p></blockquote><br><p>만약 <code>int start = 2_100_000_000, int end = 2_100_000_000</code> 이라면 start와 end를 더하면 오버플로우가 일어날 수 있다.<br>따라서 중간값을 구하는 방법은 대체로 <code>mid = start + (end-start) / 2</code>로 한다.<br>한가지 신박한 방법은 java에서 제공하는 <code>&gt;&gt;&gt;</code>연산자인데 양수일 때, <code>mid = (start+end) &gt;&gt;&gt; 1</code>로 한다면 중간값을 구할 수 있다.<br><strong>하지만</strong> 앞선 방법을 사용하는 것이 가장 안전하고 직관적이다.</p><br><blockquote><p>Q2. numbers라는 int형 배열이 있다.<br>해당 배열이 들어있는 숫자들은 오직 한 숫자를 제외하고는 모두 두번씩 들어있다.<br>오직 한 번만 등장하는 숫자를 찾는 코드를 작성하여라.</p></blockquote><br><p>C++의 sstream을 사용하는 방법이 떠올랐지만(Java에도 있다), 선장님께서 비트연산으로 푸는 방법을 알려주셨다.</p><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">Hello hello = <span class="keyword">new</span> Hello();</span><br><span class="line">    <span class="keyword">int</span> result = hello.solution(<span class="keyword">new</span> <span class="keyword">int</span>[] &#123;<span class="number">5</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>&#125;);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">solution</span><span class="params">(<span class="keyword">int</span>[] numbers)</span></span>&#123;</span><br><span class="line">        <span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> number : numbers) &#123;</span><br><span class="line">            result ^= number;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><br><p>간단하다. 모든 배열안에 있는 것들끼리 XOR연산을 하게 된다면 두개가 존재하지 않는 한개의 원소만 반환하게되기 때문이다. (<code>5 ^ 5 = 0</code>, <code>5 ^ 0 = 5</code>다.)</p><br><h3 id="java13의-switch연산자"><a href="#java13의-switch연산자" class="headerlink" title="java13의 switch연산자"></a>java13의 switch연산자</h3><hr><p>일단 switch operator는 switch statement와 다른 것이다.</p><ol><li>switch statement</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(num)&#123;</span><br><span class="line">           <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">               returnNum = <span class="number">1</span>;</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">               returnNum = <span class="number">2</span>;</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">               returnNum = <span class="number">3</span>;</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><hr><ol start="2"><li>switch operator</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> value = <span class="keyword">switch</span>(str)&#123;</span><br><span class="line"><span class="keyword">case</span> <span class="string">&quot;hello?&quot;</span>:</span><br><span class="line">    yield <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">&quot;hi?&quot;</span>:</span><br><span class="line">    yield <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">    yield <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>직관적으로 어떤의미인지 알 수 있는데, 값을 반환하는 operator이라는 것이다. 앞선방법으로 <code>break</code>을 사용하지 않고 <code>yield</code>로 값을 산출할 수 있다. 또한, <code>var</code>을 사용하여 각각의 경우 다른 변수타입을 반환하는 것 또한 가능하다.</p><br><h2 id="Day04-제어문-1"><a href="#Day04-제어문-1" class="headerlink" title="Day04. 제어문"></a>Day04. 제어문</h2><hr><p>저번시간과 마찬가지로 시간이 없기 때문에.. if, while, 기타 등등 다양한 반복문과 조건문은 생략하도록 하겠다. <a href="https://kils-log-of-develop.tistory.com/349">다음의링크</a>에서 좋은 설명을 볼 수 있을 것이다.</p><h3 id="for-each문"><a href="#for-each문" class="headerlink" title="for each문"></a>for each문</h3><hr><p>for each는 J2SE 5.0 부터 추가되었다. for each 라는 키워드가 따로 있는 것은 아니고 동일한 for를 이용한다. 하지만 조건식 부분이 조금 다르다. 보통 다른 언어에서 for each 라고 많이 하므로 자바에서도 보통 for each문이라고 말한다.</p><br><p><strong>기존의 for문</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String[] numbers = &#123;<span class="string">&quot;one&quot;</span>, <span class="string">&quot;two&quot;</span>, <span class="string">&quot;three&quot;</span>&#125;;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;numbers.length; i++)&#123;</span><br><span class="line">    System.out.println(numbers[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><p><strong>for each문</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String[] numbers = &#123;<span class="string">&quot;one&quot;</span>, <span class="string">&quot;two&quot;</span>, <span class="string">&quot;three&quot;</span>&#125;;</span><br><span class="line"><span class="keyword">for</span>(String number: numbers) &#123;</span><br><span class="line">    System.out.println(number);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>단, for each문 같은경우에는 index로 iteration하는 것이 아니기 때문에 두개씩 건너뛰는 것이 불가능하다. 상황에 맞추어 사용하면 된다.</p><br><h3 id="LinkedList-구현하기"><a href="#LinkedList-구현하기" class="headerlink" title="LinkedList 구현하기"></a>LinkedList 구현하기</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">makeLinkedList</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">interface</span> <span class="title">LinkedList</span> </span>&#123;</span><br><span class="line">        <span class="function">ListNode <span class="title">add</span><span class="params">(ListNode head, ListNode nodeToAdd, <span class="keyword">int</span> position)</span></span>;</span><br><span class="line">        <span class="function">ListNode <span class="title">remove</span><span class="params">(ListNode head, <span class="keyword">int</span> positionToRemove)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(ListNode head, ListNode nodeTocheck)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">printList</span><span class="params">(ListNode head)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> data;</span><br><span class="line">        <span class="keyword">private</span> ListNode next;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">ListNode</span><span class="params">(<span class="keyword">int</span> input)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.data = input;</span><br><span class="line">            <span class="keyword">this</span>.next = <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LinkedListImpl</span> <span class="keyword">implements</span> <span class="title">LinkedList</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (head != <span class="keyword">null</span>) &#123;</span><br><span class="line">                System.out.println(head.data);</span><br><span class="line">                head = head.next;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">size</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">            ListNode node = head;</span><br><span class="line">            <span class="keyword">int</span> size = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span> (node.next != <span class="keyword">null</span>) &#123;</span><br><span class="line">                node = node.next;</span><br><span class="line">                size++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> size;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> ListNode <span class="title">add</span><span class="params">(ListNode head, ListNode nodeToAdd, <span class="keyword">int</span> position)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            ListNode node = head;</span><br><span class="line">            <span class="keyword">if</span> (position == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (head == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">return</span> nodeToAdd;</span><br><span class="line">                &#125;</span><br><span class="line">                ListNode add = nodeToAdd;</span><br><span class="line">                add.next = head;</span><br><span class="line">                head = add;</span><br><span class="line">                <span class="keyword">return</span> head;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; position - <span class="number">1</span>; i++) &#123;</span><br><span class="line">                node = node.next;</span><br><span class="line">            &#125;</span><br><span class="line">            nodeToAdd.next = node.next;</span><br><span class="line">            node.next = nodeToAdd;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(ListNode head, ListNode nodeTocheck)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (head != <span class="keyword">null</span>) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (head.data == nodeTocheck.data) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                head = head.next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> ListNode <span class="title">remove</span><span class="params">(ListNode head, <span class="keyword">int</span> positionToRemove)</span> </span>&#123;</span><br><span class="line">            ListNode node = head;</span><br><span class="line">            <span class="keyword">if</span> (positionToRemove == <span class="number">0</span>) &#123;</span><br><span class="line">                head = head.next;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; positionToRemove - <span class="number">1</span>; i++) &#123;</span><br><span class="line">                    node = node.next;</span><br><span class="line">                &#125;</span><br><span class="line">                ListNode delNode = node.next;</span><br><span class="line">                node.next = node.next.next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>다른것들도 천천히 구현해보겠다.</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java8 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[AAIS] E03. Convolution Layer - 2</title>
      <link href="2021/01/21/aais-03/"/>
      <url>2021/01/21/aais-03/</url>
      
        <content type="html"><![CDATA[<h1 id="Agian-CNN"><a href="#Agian-CNN" class="headerlink" title="Agian CNN"></a>Agian CNN</h1><h2 id="Review-the-reports"><a href="#Review-the-reports" class="headerlink" title="Review the reports"></a>Review the reports</h2><h3 id="Tooth-wise-age-group-prediction-with-CNN-model"><a href="#Tooth-wise-age-group-prediction-with-CNN-model" class="headerlink" title="Tooth-wise age-group prediction with CNN model"></a>Tooth-wise age-group prediction with CNN model</h3><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Tooth‑wise age‑group prediction with CNN model. As the frst molar is considered to be the most reliable tooth for estimating dental age, we selected it for developing a CNN model for the age-group determination.</span><br><span class="line">From a panoramic radiograph of each patient, image patches of teeth #16, #26, #36, and #46 were manually extracted. </span><br><span class="line">The goal of extracting image patches from the panoramic radiographs was to include complete contours of the teeth. </span><br><span class="line">As a result, a total of 4,312 image patches of frst molars were collected from 1078 patients for training, and every tooth patch was resized into a fxed size of 151×112. </span><br><span class="line">To minimize any unnecessary variance in the dataset and to improve the performance of the model, the dataset was augmented23, and we used the following selective data augmentation techniques: </span><br><span class="line">the tooth images were fipped lef and right, upside down, rotated, and reversed by 90°.</span><br><span class="line">Te residual deep neural network with 152 layers (ResNet-152) was trained to predict the age-group toothwise, that is, for each tooth.</span><br><span class="line">The weights of the network were initialized using pre-trained weights from ImageNet dataset24. </span><br><span class="line">Ten, the entire network was fne-tuned for the target age-group estimation problem. </span><br><span class="line">Although the ImageNet base dataset does not include teeth images, various studies have shown that the fne-tuning of a pretrained network with several images of ImageNet helps improve the performance in disease-related problem learning using medical images.</span><br><span class="line">The network was trained using the cross-entropy loss function and adaptive moment estimation (Adam) optimizer with a learning rate of 1e-5 and a batch size of 32. </span><br><span class="line">We trained the network for 40,000 iterations and validated it using validation data every 1000 iterations with a classifcation accuracy metric to determine whether to stop the training. </span><br><span class="line">The network showing the highest validation accuracy was selected as the fnal model and used for testing.</span><br></pre></td></tr></table></figure><p>첫번째 큰어금니가 치아 나이를 추론<sup><a href="#footnote_1">1</a></sup><sup><a href="#footnote_2">2</a></sup>하는데 가장 좋으므로, 이를 채택하여 CNN 모델을 생성하였다.<br>각각의 환자의 파노라마사진으로부터 #16, #26, #36, #46번 치아를 추출하였다.</p><p align = "center"><img src="http://postfiles9.naver.net/MjAxNzAzMTBfMTUx/MDAxNDg5MTE2MTY3MDE5.FJwhRPEDicEibFZ9haFXwlkpCUW8IDQkHMGBAKQyK_4g.hWk8acgCVJA2b125bulyofsgNMgk52gU5QDJ73MML2gg.JPEG.onedaydent/image_2923048291489116088889.jpg?type=w966" width = "500px"></p><p>파노라마 이미지로부터 image patch를 추출하는 목표는 완전한 이의 윤곽(contour)를 포함시키는 것이다. 결론적으로, 1078명의 이미지로부터 4,312개의 image patch들을 모았고 모든 image patch는 151 X 112로 사이즈를 리사이징(resize)하였다. 불필요한 데이터셋들을 줄이고 모델의 성능(performance)를 증가시키기위해 데이터셋이 추가<sup><a href="#footnote_3">3</a></sup>되었다.</p><p>ResNet-152<sup><a href="#footnote_3">3</a></sup>(The Residual deep neural network with 152 layers)는 각각의 치아의 age-group을 추정했다. 가중치(The weights of the network)는 ImageNet datet으로부터 이미 훈련된것들로부터 시작되었다. 그 후, 전체 네트워크는 age-group을 추정하기위한 모델로 변형(fine-tuned)되었다. 비록, ImageNetbase dataset은 이(teeth)의 이미지를 포함하지 않지만, 다수의 연구들이 의학이미지(medical image)들을 사용하여 질병에관련된 문제들을 푸는데 성능을 증가시켰다는 것을 보여줬다. ResNet은 <code>cross-entopy loss function</code>과 <code>adaptive moment estimation(Adam) optimizer</code>를 통해 학습되었다. <code>learning rate</code>은 1e-5이며 <code>batch size</code>는 32이다. 또한, 4만번의 반복학습을 하였고 1000회마다 분류(classification)정확도를 측정하였다. 가장 높은 validation accuracy를 보여준 네트워크를 최종모델로 선정하였다.</p><h2 id="ResNet-152"><a href="#ResNet-152" class="headerlink" title="ResNet-152?"></a>ResNet-152?</h2><p>위의 내용에 따르면 ResNet-152라는 것이 나온다. CNN이랑 다른건가?싶어 걱정이 앞섰지만 다행히도 CNN의 한 종류이다.<br><a href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition</a>에 따르면 <code>residual learning</code>을 통해 기존의 심층신경망보다 상당히 깊은 신경망에 대해서도 학습을 잘하고 있는 것을 보여준다. 152개의 layer로 구성되어있으며, 전년도 ILSVRC에서 우승한 GoogleLeNet보다 약 두배의 성능이 좋아졌다.</p><br><p align = "center"><img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcx1l7G%2FbtqzR2RurjQ%2FuRBKXJoxhDZdBjqI2BqWnK%2Fimg.png" width = "600px"></p><br><p>위 그림을 보면 레이어가 점점 깊어지는데 error_rate이 줄어드는 것을 확인할 수 있다. 그렇다면 레이어가 깊어질수록 더 좋은 결과를 보여주는 것이 아닌가?생각이 들 수 있다. 하지만, 앞선 포스트에서도 확인했듯이 CNN의 layer가 많아질수록(깊어질수록) 파라미터의 수가 급격히 늘어남을 확인했었다. 이는 overfitting의 문제가 아닐지라도 에러가 커지는 현상이 발생하게 된다. 무조건적으로 layer를 깊게하는 것이 아닌 다른 방법이 필요하다 생각한 것이고 <code>residual learning</code>을 하게 된 것이다.</p><p>따라서, ResNet팀은 두가지의 원칙을 세웠다고 한다.</p><ol><li>출력 feature-map의 크기가 같은 경우, 해당 모든 layer는 모두 동일한 수의 filter를 갖는다.</li><li>feature-map의 크기가 절반으로 작아지는 경우는 연산량의 균형을 맞추기 위해 필터의 수를 두배로 늘린다. Feature-map의 크기를 줄일 때는 pooling을 사용하는 대신에 convolution을 수행할 때, stride의 크기를 “2”로 하는 방식을 취한다.</li></ol><p>연산량에 대해서는 자세히 배운 후 다시 다뤄보도록 하겠다. 우선, maxpooling과 dropout이 이 네트워크에선 쓰이지 않았다고 한다. 쓰이지 않은 이유를 알기 전에 먼저 maxpooling과 dropout을 알아보겠다.</p><hr><br><h2 id="maxpooling2d"><a href="#maxpooling2d" class="headerlink" title="maxpooling2d"></a>maxpooling2d</h2><p>convolution layer의 출력 이미지에서 주요값만 뽑아 크기가 작은 출력영상으로 만드는 역할이다. 이 layer는 사소한 변화가 영향을 미치지 않도록 한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MaxPooling2D(pool_size = (<span class="number">2</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p><code>pool_size</code>가 (2,2)이면 입력영상의 크기가 절반으로 줄어든다.</p><p>예를 들어, <code>input_shape</code>이 4X4 이고, <code>pool_size</code>가 2,2일 때를 도식화 하면 다음과 같다. 녹색은 입력영상을, 노란색은 <code>pool_size</code>에 따라 나눈 경계를, pool마다 가장 큰값(max)을 선택하여 파란블럭으로 만들면, 그것이 <code>output</code>이 된다.</p><p align = "center"><img src ="http://tykimos.github.io/warehouse/2017-1-27_CNN_Layer_Talk_lego_12.png", width = "700px"></p><p>이것이 <code>translation</code>에 굉장히 좋은 결과를 얻게되는데 그 이유는 밑의 그림을 통해 한눈에 알아볼 수 있다.</p><p align = "center"><img src = "http://tykimos.github.io/warehouse/2017-1-27_CNN_Layer_Talk_lego_13.png", width = "700px"></p><p>결론적으로 분류 작업에 유리한 불변성질(invariance)를 얻을 수 있는 장점도 있다.</p><p>하지만 ResNet-152에서는 maxpooling을 가급적 사용하지 않았다(except 1 layer). maxpooling은 이미지 구성요소의 공간관계에 대한 정보를 잃기 때문이다. 이는 위치와 상관없이 객체를 동일하게 인식하지만, 방향(orientation)이나 비율(proportion)이 달라지면 서로 다른 객체로 인식하게 된다. 특히 시점(viewpoint)변화에 유독 취약하다. 이를 해결하기 위해 data augmentation을 사용하지만 학습시간이 증가하게 되는 단점이 생기게 된다.</p><p>제프리 힌턴교수는 CNN의 층을 쌓는 대신, 캡슐망을 고안해냈는데 이는 나의 범위를 넘어가므로 먼훗날.. 다시 다뤄보겠다.</p><hr><br><h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dropout(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>layer가 많아질 때 대표적으로 발생하는 문제는 overfitting이다. 아래 그림처럼 unseparable한 상황임에도 불구하고 억지로 separate하는 상황을 overfitting이라고 한다.</p><p><img src="/image/20210121_203242.jpg"></p><p>overfitting을 방지하는 방법은 세가지가 있다고 한다.</p><ol><li>training data를 늘리기</li><li>feature의 수를 줄이기</li><li><strong>regularization</strong></li></ol><p align= "center"><img src = "https://t1.daumcdn.net/cfile/tistory/2264014957A0248C2D"></p><p>이 중 Dropout은 regularization에 해당한다. <code>reLU</code>도 이에 해당한다. Dropout은 일부 neuron을 제거하여 다음 학습에 필요하지 않도록 만드는 것이다. 그렇다면 이게 왜 좋은 방법인 것인가?<br>“사공이 많으면 배가 산으로간다”라고 생각하면 좋다. 고양이임을 판단할 때 귀,꼬리,발톱,손,눈 등을 판단하는 여러 weight이 있다고 생각할 때, dropout을 통해 몇개의 노드들을 쉬게하는 것이다.<br>위의 예제에서는 20퍼센트의 neuron들을 쉬게하는것이다. 노드들의 정보는 남기고 connection만 삭제하는 <code>dropconnect</code>같은 방법도 있다고 한다.</p><p>위의 오버피팅을 줄이기 위해서 <code>ensenble</code>이라는 방법을 사용한다고 한다. 이는 ResNet-152에서도 사용했다고 하는데 2~5%정도의 예측률이 올라간다고 한다. 이또한 나중에 다뤄보겠다.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/">https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/</a><br><a href="https://www.kakaobrain.com/blog/9">https://www.kakaobrain.com/blog/9</a><br><a href="http://blog.creation.net/mxnet-part-5-vgc16-resnet152">http://blog.creation.net/mxnet-part-5-vgc16-resnet152</a><br><a href="https://doorbw.tistory.com/147">https://doorbw.tistory.com/147</a><br><a href="https://pythonkim.tistory.com/42">https://pythonkim.tistory.com/42</a></p><p><a name="footnote_1">1</a> : Shah, P. H. &amp; Venkatesh, R. Pulp/tooth ratio of mandibular frst and second molars on panoramic radiographs: an aid for forensic<br>age estimation. J. Forensic Dent. Sci. 8, 112. <a href="https://doi.org/10.4103/0975-1475.186374">https://doi.org/10.4103/0975-1475.186374</a> (2016).<br><a name="footnote_2">2</a> : Mathew, D. G. et al. Adult forensic age estimation using mandibular frst molar radiographs: a novel technique. J. Forensic Dent. Sci. 5, 56–59. <a href="https://doi.org/10.4103/0975-1475.114552">https://doi.org/10.4103/0975-1475.114552</a> (2013).<br><a name="footnote_3">3</a> : He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, 770–778, <a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.</p>]]></content>
      
      
      <categories>
          
          <category> AAIS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> AAIS </tag>
            
            <tag> tensorflow </tag>
            
            <tag> keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[AAIS] E02. Convolution Layer - 1</title>
      <link href="2021/01/21/aais-02/"/>
      <url>2021/01/21/aais-02/</url>
      
        <content type="html"><![CDATA[<h1 id="Conv2D-Layer"><a href="#Conv2D-Layer" class="headerlink" title="Conv2D Layer"></a>Conv2D Layer</h1><p>이번 포스트에서는 Conv2D Layer에 대해서 자세히 다뤄볼 예정이다.</p><h2 id="Conv2D-parameters"><a href="#Conv2D-parameters" class="headerlink" title="Conv2D parameters"></a>Conv2D parameters</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Conv2D(</span><br><span class="line">    filters, kernel_size, strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">    data_format=<span class="literal">None</span>, dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), groups=<span class="number">1</span>, activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>, kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">    bias_initializer=<span class="string">&#x27;zeros&#x27;</span>, kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>, activity_regularizer=<span class="literal">None</span>, kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br><ol><li><code>filters</code> : convolution filter의 수</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conv2D(<span class="number">1</span>, (<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">&#x27;same&#x27;</span>, input_shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>이 코드를 도식화 하면 다음과 같다.</p><p align = "center"><img src = "http://tykimos.github.io/warehouse/2017-1-27_CNN_Layer_Talk_lego_5.png"></p><p>필터의 개수가 채널의 수와도 연관이 있으므로 input_shape을 고려해서 필터의 개수를 지정해야 한다. 이 필터를 거치게 되면 다음과 같은 <code>feature_map</code>을 얻게 된다.</p><p align = "center"><img src = "https://wikidocs.net/images/page/64066/conv8.png"></p><p><a href="https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/">feature map을 도식화하는 방법</a>으로 눈으로 직접 <code>feature_map</code>을 보는 것도 좋을 것 같다.</p><ol start="2"><li><code>kernel_size</code> : convolution filter의 (행,열)</li></ol><p>위와 같은 경우에는 2 X 2 filter가 존재하는 것이다.</p><ol start="3"><li><code>strides</code> : filter가 stride하는 단위</li><li><code>padding</code> : 경계처리방법으로 filter를 거친 feature map의 크기를 결정짓는 요소이기도 하다.</li></ol><p align = "center"><img src ="https://wikidocs.net/images/page/64066/conv10.png"></p><p>option이 <code>valid</code>일 경우 유효한 영역만 출력이 되므로 출력이미지 사이즈가 입력 사이즈보다 작아질 것이다.<br>option이 <code>same</code>일 경우 입력사이즈와 동일하게 행,열을 추가하며 주로 <code>zero padding</code>을 사용한다. <code>zero padding</code>을 사용하면 좋은 점은 영상의 크기를 동일하게 맞출 뿐만 아니라 <strong>경계면의 정보까지 살릴 수가 있어</strong> 더 좋은 결과를 얻게 된다.</p><ol start="5"><li><code>input_shape</code> = (행, 열, 채널수)로 정의하며 모델에서 첫 레이어만 정의한다. 채널수는 흑백일경우 1, 컬러일경우 3으로 설정한다.</li><li><code>activation</code> = 활성화함수를 정의한 것으로 우리가 주로 다룰 것은 <code>relu</code>,<code>softmax</code>이다. 이는 다음에 다뤄보도록 하겠다.</li></ol><p>다른 parameter들은 필요할 때 살펴보도록 하겠다.</p><h2 id="parameter-수-계산하기"><a href="#parameter-수-계산하기" class="headerlink" title="parameter 수 계산하기"></a>parameter 수 계산하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization</span><br><span class="line"></span><br><span class="line">num_classes = <span class="number">25</span></span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(<span class="number">75</span> , (<span class="number">3</span>,<span class="number">3</span>) , strides = <span class="number">1</span> , padding = <span class="string">&#x27;same&#x27;</span> , activation = <span class="string">&#x27;relu&#x27;</span> , input_shape = (<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(MaxPool2D((<span class="number">2</span>,<span class="number">2</span>) , strides = <span class="number">2</span> , padding = <span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(Conv2D(<span class="number">50</span> , (<span class="number">3</span>,<span class="number">3</span>) , strides = <span class="number">1</span> , padding = <span class="string">&#x27;same&#x27;</span> , activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(MaxPool2D((<span class="number">2</span>,<span class="number">2</span>) , strides = <span class="number">2</span> , padding = <span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(Conv2D(<span class="number">25</span> , (<span class="number">3</span>,<span class="number">3</span>) , strides = <span class="number">1</span> , padding = <span class="string">&#x27;same&#x27;</span> , activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(MaxPool2D((<span class="number">2</span>,<span class="number">2</span>) , strides = <span class="number">2</span> , padding = <span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(units = <span class="number">512</span> , activation = <span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">model.add(Dense(units = num_classes , activation = <span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>                Model: &quot;sequential_4&quot;                _________________________________________________________________                Layer (type)                 Output Shape              Param #                =================================================================                conv2d_6 (Conv2D)            (None, 28, 28, 75)        750                _________________________________________________________________                batch_normalization (BatchNo (None, 28, 28, 75)        300                _________________________________________________________________                max_pooling2d (MaxPooling2D) (None, 14, 14, 75)        0                _________________________________________________________________                conv2d_7 (Conv2D)            (None, 14, 14, 50)        33800                _________________________________________________________________                dropout (Dropout)            (None, 14, 14, 50)        0                _________________________________________________________________                batch_normalization_1 (Batch (None, 14, 14, 50)        200                _________________________________________________________________                max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0                _________________________________________________________________                conv2d_8 (Conv2D)            (None, 7, 7, 25)          11275                _________________________________________________________________                batch_normalization_2 (Batch (None, 7, 7, 25)          100                _________________________________________________________________                max_pooling2d_2 (MaxPooling2 (None, 4, 4, 25)          0                _________________________________________________________________                flatten (Flatten)            (None, 400)               0                _________________________________________________________________                dense (Dense)                (None, 512)               205312                _________________________________________________________________                dropout_1 (Dropout)          (None, 512)               0                _________________________________________________________________                dense_1 (Dense)              (None, 25)                12825                =================================================================                Total params: 264,562                Trainable params: 264,262                Non-trainable params: 300                _________________________________________________________________</code></pre><p>위의 <code>summary</code>는 다른 data들을 학습시킨 결과로 배포할 수 없는 데이터들이기 때문에 <code>summary</code>를 통해서 param number만 계산해 보도록 하겠다.</p><br><hr><h3 id="convolution-layer-1의-activation-map-크기-계산"><a href="#convolution-layer-1의-activation-map-크기-계산" class="headerlink" title="convolution layer 1의 activation map 크기 계산"></a>convolution layer 1의 activation map 크기 계산</h3><p align = "center"><img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCGGUL%2FbtqFo5oGkOV%2F0ZFkRKpAeFeNPkYuzAAbWK%2Fimg.png"></p><p>Width Output Size = (W - F + 2P) / S + 1<br>W: input_volume_size(width)<br>F: kernel_size(width)<br>P: padding_size(width)<br>S: strides</p><p>첫번째 convolution layer를 지나면 <code>Width</code> = 28, <code>kernel_size(width)</code> = 3, <code>padding_size</code> = 1, <code>strides</code> = 1 이므로 계산하면 <code>width output</code>은 28이 된다.<br><code>filters</code>의 수가 75이므로 output size = (28,28,75)가 된다.</p><br><p>이를 간단한 그림으로 보이겠다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conv2D(<span class="number">3</span>, (<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">&#x27;same&#x27;</span>, input_shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p align = "center"><img src = "http://tykimos.github.io/warehouse/2017-1-27_CNN_Layer_Talk_lego_6.png"></p><p>필터가 3개라서 출력 이미지도 필터 수에 따라 3개로 늘어난다.<br>필터마다 고유한 특징을 뽑아 고유한 출력 이미지로 만들기 때문에 필터의 출력값을 더해서 하나의 이미지로 만들지 않는다.</p><p>위의 경우는 채널이 1개일 때고 채널이 여러개일 때를 도식화 한것은 다음과 같다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conv2D(<span class="number">2</span>, (<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">&#x27;same&#x27;</span>, input_shape=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>input_shape이 <code>(3,3,3)</code>이므로 3X3이미지가 3개의 채널로 존재한다는 것이다. 따라서 필터를 2개를 주고 필터의 사이즈를 2X2로 하게된다면 아래의 그림처럼 필터 1개마다 채널 1개씩 곱이 convolution이 진행되고 다시 한개의 layer로 합쳐지므로 결론적으로는 3X3사이즈의 2개의 채널이 만들어지므로 output_shape은 <code>(3,3,2)</code>일 것이다.</p><p align = "center"><img src = "http://tykimos.github.io/warehouse/2017-1-27_CNN_Layer_Talk_lego_10.png"></p><p>즉 필터의 개수만큼의 layer가 쌓이게 되고 중간에 컨벌루션곱이 이루어진 것이므로 feature map은 당연히 다른 것이다.</p><br><p>이제 파라미터의 수를 구해보겠다. 그 식은 다음과 같다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">number_parameters &#x3D; out_channels * (in_channels * kernel_h * kernel_w + 1)  # 1 for bias</span><br></pre></td></tr></table></figure><p>여기서 <code>out_channels</code>이란 필터의 개수와 동일하다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">in_channels &#x3D; 1</span><br><span class="line">out_channels &#x3D; 75</span><br><span class="line">kernel_h &#x3D; kernel_w &#x3D; 3</span><br><span class="line">number_parameters &#x3D; 75(1*3*3 + 1) &#x3D; 750</span><br></pre></td></tr></table></figure><p>우리의 <code>summary</code>에서 확인한 750은 위와같이 계산된 것이다.</p><br><hr><h3 id="convolution-layer-2의-activation-map-크기-계산"><a href="#convolution-layer-2의-activation-map-크기-계산" class="headerlink" title="convolution layer 2의 activation map 크기 계산"></a>convolution layer 2의 activation map 크기 계산</h3><p><code>max_pooling2d</code>를 지나면 <code>output_shape</code>은 <code>(14,14,75)</code>가 된다. 이는 conv2d의 <code>input_shape</code>이 된다. (그래서 첫번째 레이어에서만 input_shape을 정의하는 것이다.)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">in_channels &#x3D; 75</span><br><span class="line">out_channels &#x3D; 50</span><br><span class="line">kernel_h &#x3D; kernel_w &#x3D; 3</span><br><span class="line">number_parameters &#x3D; 75(50*3*3 + 1) &#x3D; 33825</span><br></pre></td></tr></table></figure><p>두번째 conv2d의 파라미터의 수도 잘계산되었다. 이처럼 <code>Layer</code>들을 지나다보면 <code>in_channels</code>의 수가 증가하여 <code>parameters</code>의 수가 급격히 증가하게 되는데 이를 다루는 방법은 다음에 다뤄보겠다~</p><br><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/">https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/</a><br><a href="https://underflow101.tistory.com/38?category=826164">https://underflow101.tistory.com/38?category=826164</a><br><a href="https://blog.naver.com/laonple/220587920012">https://blog.naver.com/laonple/220587920012</a></p>]]></content>
      
      
      <categories>
          
          <category> AAIS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> AAIS </tag>
            
            <tag> tensorflow </tag>
            
            <tag> keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Java]Day03 - 연산자</title>
      <link href="2021/01/20/java-day3/"/>
      <url>2021/01/20/java-day3/</url>
      
        <content type="html"><![CDATA[<h2 id="JAVA-NEWS"><a href="#JAVA-NEWS" class="headerlink" title="JAVA NEWS"></a>JAVA NEWS</h2><p>오늘은 들어가기에 앞서 스터디에서 다룬 내용을 살펴보겠다.</p><h3 id="Maven-Dependencies-Pop-Quiz"><a href="#Maven-Dependencies-Pop-Quiz" class="headerlink" title="Maven Dependencies Pop Quiz"></a><a href="http://andresalmiray.com/maven-dependencies-pop-quiz-results/">Maven Dependencies Pop Quiz</a></h3><p align = "center"><img src = "http://andresalmiray.com/wp-content/uploads/2020/03/pop-mvn01-768x629.png"></p><p>정답은 <code>28.2version</code>이다.<br>dependency는 동일한 groupId일 경우 가장 마지막에 있는 버젼이 위에 있는 것을 오버라이딩한다고 생각하면 좋다.</p><p align = "center"><img src = "http://andresalmiray.com/wp-content/uploads/2020/03/pop-mvn02-768x660.png"></p><p>정답은 <code>28.2version</code>이다.<br>dependency는 가장 POM파일에 명확하게 정의된 것으로 정의한다.<br>truth의 27.0.1version은 28.2v보다 depth가 깊다고 생각하면 된다.</p><p align = "center"><img src = "http://andresalmiray.com/wp-content/uploads/2020/03/pop-mvn03-768x734.png"></p><p>정답은 <code>27.0version</code>이다.<br>dependencyManagement(이하 DM)는 추이적(transitive)으로 정의된 dependency보다는 가깝다.</p><p align = "center"><img src = "http://andresalmiray.com/wp-content/uploads/2020/03/pop-mvn04-768x854.png"></p><p>정답은 <code>27.0version</code>이다.<br>DM은 dependency보다는 조금 멀다. DM은 어떤버젼을 쓸지 명시해놓은 것이다. 만약 guava 28.2-jre를 제거할 경우 27.0 version이 적용이 될것이다.</p><p>결론적으로는 dependency &gt; DM &gt; 추이적정의 순서대로 정의가 된다고 생각하면 된다.</p><h2 id="Review-Day02"><a href="#Review-Day02" class="headerlink" title="Review Day02"></a>Review Day02</h2><blockquote><p>Q1. Java에 unsigned이 존재하는가?</p></blockquote><p>정답은 <code>Yes</code>이다. 정확히는 java8 이전까지는 unsigned형이 존재하지 않는다. C에서는 <code>unsigned int</code>를 사용하면 됐지만 java에는 이가 존재하지 않는다. <code>int unsigned = Integer.parseUnsignedInt(&quot;21억이상&quot;)</code>이라고 할 시 <code>Integer.toUnsignedString(unsigned)</code>로 출력을 하는것은 가능하다. 하지만 이는 좋은 방법이 아니므로 그냥 <code>BigInteger</code>를 사용하자! (<strong>L을 넣어야한다.</strong>)</p><blockquote><p>Q2. <code>int number = 1_000;</code>이 정의가 되는가?</p></blockquote><p>정답은 <code>Yes</code>이다. 숫자를 표현할 때 가독성이 좋게 하기위해 언더바(_)를 통해 정의할 수 있다.</p><p>또 하나의 중요한 점은 Static 변수와 Instance변수의 라이프사이클이다.<br>Static의 라이프사이클은 클래스가 로딩된 시점이지만 Instance변수의 라이프사이클은 클래스가 생성된 시점이므로 instance가 생성되기 이전에는 instance변수가 존재하지 않는다 따라서 static은 instance 변수를 참조할 수 없다. 이는 중요한 내용이므로 모르면 찾아보자<br>오늘의 리뷰는 짧게 하고 DAY03 시작해보겠다.</p><h2 id="Day03-연산자"><a href="#Day03-연산자" class="headerlink" title="Day03. 연산자"></a>Day03. 연산자</h2><p>산술 연산자, 비트 연산자, 관계 연산자, 논리 연산자, assignment(=) operator, 3항 연산자 그리고 연산자 우선 순위는 넘어가보도록 할 것이다. 시간이 부족해서 다른 것들에 더 집중해보도록 하겠다. 이에 대해서는 <a href="https://www.notion.so/3-f3a94e0092664d8aa1debe7e88dec43b">이 블로그</a>에서 깔끔히 다뤘으므로 참고하면 좋겠다.</p><h3 id="instanceof-연산자"><a href="#instanceof-연산자" class="headerlink" title="instanceof 연산자"></a>instanceof 연산자</h3><p>참조변수가 참조하고 있는 인스턴스의 실제 타입을 알아보기 위해 instanceof 연산자를 사용한다.<br>주로 <strong>조건문</strong>에서 사용되며, 왼쪽에는 참조변수를 오른쪽에는 타입(class name)을 쓴다. 그리고 연산의 결과는 boolean형인 <code>true</code>나 <code>false</code>를 반환한다.</p><p> instanceof를 이용한 연산결과로 true를 얻었다는 것은 참조변수가 검사한 타입으로 형변환이 가능하다는 것을 뜻한다.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doWork</span><span class="params">(Car c)</span></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(c <span class="keyword">instanceof</span> FireEngine)&#123;</span><br><span class="line">    FireEngine fe = (FireEngine)c;</span><br><span class="line">        fe.water();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(c instance of Ambulance)&#123;</span><br><span class="line">    Ambulance a = (Ambulance)c;</span><br><span class="line">        a.siren();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>위의 코드는 Car타입의 참조변수인 c를 매개변수로 하는 메서드이다. 이 메서드가 호출될 때, 매개변수로 Car클래스 혹은 그 자손 클래스의 인스턴스를 넘겨받겠지만 메서드 내에서는 정확히 어떤 인스턴스인지 알 수가 없다. 따라서, <code>instanceof</code>연산자를 이용해서 참조변수 c가 가리키고 있는 인스턴스의 타입을 체크하고 적절히 형변환을 한 다음에 작업을 해야한다.</p><p>위의 코드에서는 c라는 참조변수가 FireEngine인지 Ambulance인지 구분을 시킨 후 각각에 맞춰서 형변환을 이룬 후 메서드를 실행한다. 이는 정말 중요한 작업이다.</p><p>그리고 또한 중요한 것은 만약 c라는 참조변수가 FireEngine이라 해도 <code>c instance of Car</code>의 구문에서도 <code>true</code>를 반환한다는 점과 모든 클래스의 조상클래스인 <code>Object</code>의 인스턴스 이기도 하다는 것이다. 즉, 실제 인스턴스와 같은 타입의 instanceof연산 이외에 조상타입의 instanceof연산에도 true를 결과로 얻으며, <strong>instanceof연산의 결과가 true라는 것은 검사한 타입으로의 형변환을 해도 문제가 없다는 뜻이다.</strong></p><h3 id="화살표-gt-연산자"><a href="#화살표-gt-연산자" class="headerlink" title="화살표(-&gt;) 연산자"></a>화살표(-&gt;) 연산자</h3><p>자바가 등장한 이후로 두 번의 큰 변화가 있었는데, 한번은 5버젼의 <code>지네릭스</code>와 8버젼의 <code>람다식</code>이다. -&gt;연산자는 이 람다식이다.<br>람다식의 도입으로 자바는 객체지향언어임과 동시에 함수형 언어가 되었다.</p><blockquote><p>람다식이란?</p></blockquote><p> 람다식에 대해 간단히 먼저 설명하자면, 메서드를 하나의 ‘식(Expression)’으로 표현한 것을 말한다. 람다식은 함수를 간략하면서도 명확한 식으로 표현할 수 있게 해준다.<br> 메서드를 람다식으로 표현하면 메서드의 이름과 반환값이 없어지므로, 람다식을 ‘Anonymous funcion’이라고도 한다.</p><p>아래와 같이 선언된 매개변수가 하나뿐인 경우에는 괄호()를 생략할 수 있다. 단, 매개변수의 타입이 있으면 괄호()를 생략할 수 없다.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(a) -&gt; a*a <span class="comment">// OK</span></span><br><span class="line">(<span class="keyword">int</span> a) -&gt; a*a <span class="comment">// Error</span></span><br></pre></td></tr></table></figure><p>마찬가지로 괄호{}안의 문장이 하나일 때는 괄호{}를 생략할 수 있다. 이 때 문장의 끝에 ‘;’를 붙이지 않아야 한다는 것에 주의하자.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(String name, <span class="keyword">int</span> i) -&gt; System.out.println(name+<span class="string">&quot;=&quot;</span>+i)</span><br></pre></td></tr></table></figure><p>그러나 괄호{}안의 문장이 return문일 경우에는 괄호를{}생략할 수 없다.</p><p>람다식에 대해서는 중요한 내용이므로 나아아중에 다시 제대로 다뤄보겠다.</p><h3 id="Optional"><a href="#Optional" class="headerlink" title="Optional"></a>Optional</h3><p>Optional도 정말 중요한 내용이지만 우선 지금 다루기엔 너무 내용이 많으므로 간략히 다뤄보겠다.<br>Optional class는 지네릭(generic)클래스(벌써 범위를 넘어갔다.)로써 ‘T타입의 객체’를 감싸는 래퍼(wrapper) 클래스이다. 그래서 Optional타입의 객체에는 <code>모든 타입의 참조변수</code>를 담을 수 있다.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Optional</span>&lt;<span class="title">T</span>&gt;</span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> T value; <span class="comment">// T타입의 참조변수</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>최종 연산의 결과를 그냥 반환하는 것이 아니라 Optional객체에 담아서 반환하는 것이다.이처럼 객체에 담아서 반환을 하면, 반환된 결과가 null인지 매번 if문으로 체크하는 대신 Optional에 정의된 메서드를 통해서 간단히 처리할 수 있다.<br> 즉, Null체크를 위한 if문 없이도 <code>NullPointerException</code>이 발생하지 않는 안전한 코드를 작성하는 것이 가능해진 것이다.<br>자세한 내용은 <a href="https://velog.io/@bluesky7017/%EC%9E%90%EB%B0%94%EA%B0%80-%EC%A0%9C%EA%B3%B5%ED%95%98%EB%8A%94-%EB%8B%A4%EC%96%91%ED%95%9C-%EC%97%B0%EC%82%B0%EC%9E%90%EB%A5%BC-%ED%95%99%EC%8A%B5%ED%95%98%EC%84%B8%EC%9A%94">이 블로그</a>에서 참고하면 좋을 것 같다.</p><h3 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h3><p>이번 연산자단원에서는 내가 크게 다룬 것은 없다. -&gt;연산자는 뒤에 람다식을 다루며 자세히 다뤄야할 내용이며 Optional도 그러하다. 진도를 빨리 나가기 위해 연산자에 대한 내용은 위에 참조된 링크들을 보면 좋을 것이다.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li>자바의 정석</li></ul>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java8 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[AAIS]E01.Using Deep Learning Tools</title>
      <link href="2021/01/20/aais-01/"/>
      <url>2021/01/20/aais-01/</url>
      
        <content type="html"><![CDATA[<h1 id="Using-Deep-Learning-Tools"><a href="#Using-Deep-Learning-Tools" class="headerlink" title="Using Deep Learning Tools"></a>Using Deep Learning Tools</h1><h2 id="들어가기에-앞서"><a href="#들어가기에-앞서" class="headerlink" title="들어가기에 앞서"></a>들어가기에 앞서</h2><p> 지난 시간에 CNN에 대해 간략하게 나마 알아보았다. <br><br>이번에는 교수님께서 제공해주신 예제들을 통해 친숙해져보도록 하겠다.</p><p><code>!nvidia-smi</code>는 nvidia gpu resource들을 모니터링하는 명령어이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><pre><code>Wed Jan 20 05:13:24 2021+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 || N/A   41C    P0    51W / 300W |   1561MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 || N/A   40C    P0    54W / 300W |    626MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 || N/A   40C    P0    50W / 300W |  27829MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 || N/A   40C    P0    49W / 300W |    652MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================|+-----------------------------------------------------------------------------+</code></pre><p>랩실에서 실제로 사용하고 있는 서버이기 때문에 리소스를 확인하고 적절히 분배해서 사용해야한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using gpu:/1</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><p>위와 같은 코드를 사용하면 GPU #1번을 사용하는 것이다. 또한, 한꺼번에 모든 메모리를 사용하는 것이 아닌 <code>tf.config.experimental.set_memory_growth</code>를 사용해 메모리를 증가시키며 사용한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using gpu:/1</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><p><code>MNIST dataset</code>은 튜링어워드를 수상하셨던 <code>Convolution network</code>의 창시자 <code>Yann LeCun</code>가 제공해주시는 데이터셋이다.<br><br>이를 활용하여 실습을 진행해보겠다.</p><p><code>MNIST dataset</code>은 손으로 쓴 숫자 이미지를 벡터로 나타낸 images와 그 이미지가 의미하는 숫자를 나타내는 labels로 이루어져 있다. 한개의 이미지는 <code>28*28(=784)의 픽셀</code>로 이루어져 있기 때문에 이는 784차원의 벡터로 저장되어 있고 진하기의 정도에 따라 0~1사이의 값이 들어있다. 예시는 다음과 같다.</p><p align="center"><img src="https://docs.ncloud.com/ko/tensorflow/images/tensorflow-1-3-102.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Conv2D, Flatten</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># the data, split between train and test sets</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;# of train data : &quot;</span>,x_train.shape)</span><br><span class="line">print(<span class="string">&quot;# of test data : &quot;</span>,x_test.shape)</span><br></pre></td></tr></table></figure><pre><code># of train data :  (60000, 28, 28)# of test data :  (10000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(x_train[<span class="number">509</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x7f14e8b00940&gt;</code></pre><p><img src="/image/output_8_1.png" alt="png"></p><p>우선 CNN을 적용시키기 이전에 데이터를 vectorization시킨 후 진행해보겠다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vectorization for ANN</span></span><br><span class="line">x_train_vectorize = x_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">x_test_vectorize = x_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.keras <span class="keyword">as</span> keras</span><br><span class="line">num_categories = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">y_train_onehot = keras.utils.to_categorical(y_train, num_categories)</span><br><span class="line">y_test_onehot = keras.utils.to_categorical(y_test, num_categories)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Dense(units=<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(Dense(units = <span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(units = <span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================dense (Dense)                (None, 512)               401920_________________________________________________________________dense_1 (Dense)              (None, 512)               262656_________________________________________________________________dense_2 (Dense)              (None, 10)                5130=================================================================Total params: 669,706Trainable params: 669,706Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train_vectorize, y_train_onehot,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=(x_test_vectorize, y_test_onehot))</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Epoch 1/201875/1875 [==============================] - 6s 3ms/step - loss: 0.3957 - accuracy: 0.9521 - val_loss: 0.5461 - val_accuracy: 0.9337Epoch 2/201875/1875 [==============================] - 6s 3ms/step - loss: 0.3798 - accuracy: 0.9524 - val_loss: 0.7209 - val_accuracy: 0.9506Epoch 3/201875/1875 [==============================] - 6s 3ms/step - loss: 0.3913 - accuracy: 0.9522 - val_loss: 0.5986 - val_accuracy: 0.9464Epoch 4/201875/1875 [==============================] - 6s 3ms/step - loss: 0.3965 - accuracy: 0.9505 - val_loss: 0.6736 - val_accuracy: 0.9391Epoch 5/201875/1875 [==============================] - 6s 3ms/step - loss: 0.4349 - accuracy: 0.9540 - val_loss: 0.7524 - val_accuracy: 0.9441Epoch 6/201875/1875 [==============================] - 6s 3ms/step - loss: 0.4307 - accuracy: 0.9506 - val_loss: 0.9400 - val_accuracy: 0.9461Epoch 7/201875/1875 [==============================] - 6s 3ms/step - loss: 0.4567 - accuracy: 0.9509 - val_loss: 0.8693 - val_accuracy: 0.9406Epoch 8/201875/1875 [==============================] - 6s 3ms/step - loss: 0.4598 - accuracy: 0.9475 - val_loss: 0.9901 - val_accuracy: 0.9225Epoch 9/201875/1875 [==============================] - 6s 3ms/step - loss: 0.4730 - accuracy: 0.9471 - val_loss: 1.5888 - val_accuracy: 0.9191Epoch 10/201875/1875 [==============================] - 6s 3ms/step - loss: 0.4903 - accuracy: 0.9363 - val_loss: 0.9527 - val_accuracy: 0.9295Epoch 11/201875/1875 [==============================] - 6s 3ms/step - loss: 0.5425 - accuracy: 0.9351 - val_loss: 1.4321 - val_accuracy: 0.9451Epoch 12/201875/1875 [==============================] - 6s 3ms/step - loss: 0.5320 - accuracy: 0.9397 - val_loss: 1.7940 - val_accuracy: 0.9428Epoch 13/201875/1875 [==============================] - 6s 3ms/step - loss: 0.5362 - accuracy: 0.9346 - val_loss: 1.5604 - val_accuracy: 0.8951Epoch 14/201875/1875 [==============================] - 6s 3ms/step - loss: 0.5934 - accuracy: 0.9328 - val_loss: 1.8964 - val_accuracy: 0.8971Epoch 15/201875/1875 [==============================] - 6s 3ms/step - loss: 0.5844 - accuracy: 0.9265 - val_loss: 1.7190 - val_accuracy: 0.9085Epoch 16/201875/1875 [==============================] - 6s 3ms/step - loss: 0.6048 - accuracy: 0.9211 - val_loss: 1.6869 - val_accuracy: 0.9072Epoch 17/201875/1875 [==============================] - 6s 3ms/step - loss: 0.5789 - accuracy: 0.9172 - val_loss: 2.8615 - val_accuracy: 0.8887Epoch 18/201875/1875 [==============================] - 6s 3ms/step - loss: 0.6705 - accuracy: 0.9189 - val_loss: 2.0492 - val_accuracy: 0.9045Epoch 19/201875/1875 [==============================] - 6s 3ms/step - loss: 0.6655 - accuracy: 0.9216 - val_loss: 1.9504 - val_accuracy: 0.9053Epoch 20/201875/1875 [==============================] - 6s 3ms/step - loss: 0.6559 - accuracy: 0.9233 - val_loss: 2.6255 - val_accuracy: 0.9164</code></pre><h1 id="Keras-Sequential-model"><a href="#Keras-Sequential-model" class="headerlink" title="Keras Sequential model"></a>Keras Sequential model</h1><h2 id="Specify-Input-Shape"><a href="#Specify-Input-Shape" class="headerlink" title="Specify Input Shape"></a>Specify Input Shape</h2><hr><p>Model should know the Input Shape. So, <code>Sequential</code>model’s first layer(after this, layer know the shape) shold give the information.</p><ul><li>Pass the <code>input_shape</code> to the first layer. This is a tuple containing shape information.(A tuple with an integer or <code>None</code> as an entry; <code>None</code> represents an arbitrary positive integer). The <code>input_shape</code> does not include batch dimension.</li><li>Some two-dimensional layers, such as <code>Dense</code>, can specify the input shape through the <code>input_dim</code> argument, and some three-dimensional layers(temporal) support the <code>input_dim</code> and <code>input_length</code> arguments.</li></ul><h2 id="Compile"><a href="#Compile" class="headerlink" title="Compile"></a>Compile</h2><hr><p>Before Learning, you must configure the learning process with the <code>compile</code> method. This Method accepts three arguments.</p><ol><li>optimizer : It can be a string identifier that represents a built-in optimizer (such as <code>rmsprop</code> or <code>adagrad</code>), or an instance of the <code>optimizer</code> class.</li><li>loss : Loss function. This is what the model wants to minimize. It can be the string identifier(<code>categorical_crossentropy</code> or <code>mse</code>, and so on) or an instance of the target function itself of the built-in loss function</li><li>metrics : List of Metrics. If you solve the classification problem, It is recommended to use <code>metrics = [&#39;accuracy&#39;]</code>. Metrics can be string identifiers for built-in metrics or user-defined metric functions.</li></ol><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><hr><p>The Keras model is trained based on input data and labels from the Numpy array. When you train a model, you typically use the <code>fit</code> function.</p><h3 id="For-the-better-explanation"><a href="#For-the-better-explanation" class="headerlink" title="For the better explanation"></a><a href="https://www.codeonweb.com/entry/eda18bec-7c7d-426f-ab98-90e18db6fdba">For the better explanation</a></h3><h1 id="Layer"><a href="#Layer" class="headerlink" title="Layer"></a>Layer</h1><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><hr><h3 id="1-ReLU-Rectified-Linear-Unit-activation-function"><a href="#1-ReLU-Rectified-Linear-Unit-activation-function" class="headerlink" title="1. ReLU(Rectified Linear Unit activation function)"></a>1. ReLU(Rectified Linear Unit activation function)</h3><p align ="center"><img src="https://mlnotebook.github.io/img/transferFunctions/relu.png"></p><ul><li><strong>Features</strong></li></ul><ol><li>It makes the vanishing gradient problem(tanh, sigmoid have) <strong>MUCH WORSE</strong>,since for all negative values the derivative is precisely zero.</li><li>Computational Cost is not significant.</li><li>In comparison to (sigmoid,tanh), convergence speed is much better.</li></ol><h3 id="2-softmax"><a href="#2-softmax" class="headerlink" title="2. softmax"></a>2. softmax</h3><ul><li><strong>Features</strong></li></ul><ol><li>A function that normalizes the output value of a neuron at the last stage for class classification.(normalize sigmoid function)</li><li>So, we can think that It converts the output into probability. (sum = 1.0)</li></ol><h2 id="결과에-대한-분석"><a href="#결과에-대한-분석" class="headerlink" title="결과에 대한 분석"></a>결과에 대한 분석</h2><p>우선 케라스의 <code>Sequential Model</code>, <code>compile</code>, <code>fit</code>과 <code>activation function</code>에 대해 간략히 적어보았다. 이전에 있어보이려고 영어로 적어봤는데 아까워서 그냥 붙여썼다.</p><p>결과에 대해 다뤄보자면, <code>epoch= 20</code>으로 수행한 결과 어느정도까지는 accuracy는 높아지고 loss는 낮아지는 좋은 현상이 일어났지만 그 이후로는 줄어들었다 늘어났다를 반복하여 결론적으로는 성능이 더 낮아졌다. 그 이유는 무엇일까?</p><p>정답은 이미지를 벡터로 표현했기 때문이다. 저번 시간에도 말했듯 이미지를 벡터화한다면 조금만 translation되어도 다른 이미지로 인식하므로 overfitting되는 결과가 나온다. 따라서 이를 방지하고자 CNN을 사용하는 것이다.</p><p>Convolution을 한 모델로 새롭게 해보자.</p><h2 id="New-Model-with-Convolution"><a href="#New-Model-with-Convolution" class="headerlink" title="New Model with Convolution"></a>New Model with Convolution</h2><p><strong>새로 들어가기에 앞서 이 서버는 다른분들도 사용하셔서 항상 리소스를 반환해줘야한다.<br>따라서 학습이 끝난 후 리소스를 반환해주자.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> IPython</span><br><span class="line">app = IPython.Application.instance()</span><br><span class="line">app.kernel.do_shutdown(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;status&#39;: &#39;ok&#39;, &#39;restart&#39;: True&#125;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using gpu:/1</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using gpu:/1</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Conv2D, Flatten</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># the data, split between train and test sets</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.keras <span class="keyword">as</span> keras</span><br><span class="line">num_categories = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">y_train_onehot = keras.utils.to_categorical(y_train, num_categories)</span><br><span class="line">y_test_onehot = keras.utils.to_categorical(y_test, num_categories)</span><br></pre></td></tr></table></figure><p>여기까지의 내용은 동일하다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line">model.add(Conv2D(<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x_train_forConv = x_train.reshape(<span class="number">60000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line">x_test_forConv = x_test.reshape(<span class="number">10000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train_forConv, y_train_onehot,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=(x_test_forConv, y_test_onehot))</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_1&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================conv2d_2 (Conv2D)            (None, 26, 26, 64)        640_________________________________________________________________conv2d_3 (Conv2D)            (None, 24, 24, 32)        18464_________________________________________________________________flatten_1 (Flatten)          (None, 18432)             0_________________________________________________________________dense_1 (Dense)              (None, 10)                184330=================================================================Total params: 203,434Trainable params: 203,434Non-trainable params: 0_________________________________________________________________Epoch 1/201875/1875 [==============================] - 7s 4ms/step - loss: 0.1776 - accuracy: 0.9570 - val_loss: 0.1147 - val_accuracy: 0.9729Epoch 2/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0854 - accuracy: 0.9766 - val_loss: 0.0864 - val_accuracy: 0.9778Epoch 3/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0745 - accuracy: 0.9797 - val_loss: 0.0905 - val_accuracy: 0.9735Epoch 4/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0724 - accuracy: 0.9802 - val_loss: 0.0919 - val_accuracy: 0.9781Epoch 5/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0700 - accuracy: 0.9812 - val_loss: 0.0837 - val_accuracy: 0.9746Epoch 6/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0690 - accuracy: 0.9818 - val_loss: 0.1036 - val_accuracy: 0.9742Epoch 7/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0690 - accuracy: 0.9818 - val_loss: 0.1009 - val_accuracy: 0.9750Epoch 8/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0689 - accuracy: 0.9819 - val_loss: 0.1134 - val_accuracy: 0.9746Epoch 9/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0700 - accuracy: 0.9819 - val_loss: 0.1004 - val_accuracy: 0.9760Epoch 10/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0692 - accuracy: 0.9826 - val_loss: 0.1298 - val_accuracy: 0.9750Epoch 11/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0681 - accuracy: 0.9827 - val_loss: 0.1055 - val_accuracy: 0.9732Epoch 12/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0688 - accuracy: 0.9820 - val_loss: 0.1205 - val_accuracy: 0.9725Epoch 13/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0682 - accuracy: 0.9830 - val_loss: 0.1140 - val_accuracy: 0.9751Epoch 14/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0671 - accuracy: 0.9830 - val_loss: 0.1266 - val_accuracy: 0.9729Epoch 15/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0664 - accuracy: 0.9831 - val_loss: 0.1417 - val_accuracy: 0.9723Epoch 16/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0639 - accuracy: 0.9837 - val_loss: 0.1319 - val_accuracy: 0.9729Epoch 17/201875/1875 [==============================] - 8s 4ms/step - loss: 0.0670 - accuracy: 0.9837 - val_loss: 0.1290 - val_accuracy: 0.9723Epoch 18/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0668 - accuracy: 0.9840 - val_loss: 0.1517 - val_accuracy: 0.9736Epoch 19/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0625 - accuracy: 0.9849 - val_loss: 0.1545 - val_accuracy: 0.9699Epoch 20/201875/1875 [==============================] - 7s 4ms/step - loss: 0.0640 - accuracy: 0.9845 - val_loss: 0.1448 - val_accuracy: 0.9691</code></pre><p>epoch를 20으로 했더니 train data들은 대부분 loss도 감소하고 accuracy도 증가하지만 <br><br>test data들은 loss도 들쭉날쭉에 accuracy는 감소하기까지한다.<br><br>아직 <code>overfitting</code>의 문제를 해결하지 못했을 뿐만아니라 <code>vanishing/exploding gradient</code>또한 해결하지 못했다. <br><br>이는 다음 실습에서 <code>batchnormalization</code>과 <code>dropdout</code>에 대해 공부해보고 좀 더 지켜보도록 하겠다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">90</span></span><br><span class="line">plt.imshow(x_test[n].reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&#x27;Greys&#x27;</span>, interpolation=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;정답은 : &#x27;</span>, np.argmax(model.predict(x_test[n].reshape((<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))))</span><br></pre></td></tr></table></figure><p><img src="/image/output_25_0.png" alt="png"></p><center>정답은 :  3</center><p>랜덤하게 추출해서 확인해보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">use_samples = np.random.randint(<span class="number">5000</span>,size = <span class="number">16</span>)</span><br><span class="line">samples_to_predict = []</span><br><span class="line"></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">nrows = ncols = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize = (<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sample <span class="keyword">in</span> use_samples:</span><br><span class="line">  count+=<span class="number">1</span></span><br><span class="line">  plt.subplot(nrows,ncols,count)</span><br><span class="line">  reshaped_image = x_test[sample].reshape((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">  plt.imshow(reshaped_image,cmap=<span class="string">&#x27;Greys&#x27;</span>, interpolation=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">  samples_to_predict.append(x_test[sample])</span><br><span class="line">  tmp = <span class="string">&quot;Label:&quot;</span> + <span class="built_in">str</span>(y_test[sample]) + <span class="string">&quot;, Prediction:&quot;</span> + <span class="built_in">str</span>(np.argmax(model.predict(x_test[sample].reshape((<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))))</span><br><span class="line">  plt.title(tmp)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/output_27_0.png" alt="png"></p><p>약간의 오차가 있는 듯하다. 이는 다른 <code>Layer</code>들을 공부하면서 줄여보도록 하겠다.</p><p>그리고 리소스반환도 필수다!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> IPython</span><br><span class="line">app = IPython.Application.instance()</span><br><span class="line">app.kernel.do_shutdown(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;status&#39;: &#39;ok&#39;, &#39;restart&#39;: True&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> AAIS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> AAIS </tag>
            
            <tag> tensorflow </tag>
            
            <tag> jupyter-notebook </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Algorithm] 오픈채팅방</title>
      <link href="2021/01/20/algorithm-02/"/>
      <url>2021/01/20/algorithm-02/</url>
      
        <content type="html"><![CDATA[<h1 id="오픈채팅방-2019-KAKAO-BLIND-RECRUITMENT"><a href="#오픈채팅방-2019-KAKAO-BLIND-RECRUITMENT" class="headerlink" title="오픈채팅방 (2019 KAKAO BLIND RECRUITMENT)"></a>오픈채팅방 (2019 KAKAO BLIND RECRUITMENT)</h1><hr><h2 id="문제설명"><a href="#문제설명" class="headerlink" title="문제설명"></a>문제설명</h2><p>카카오톡 오픈채팅방에서는 친구가 아닌 사람들과 대화를 할 수 있는데, 본래 닉네임이 아닌 가상의 닉네임을 사용하여 채팅방에 들어갈 수 있다.</p><p>신입사원인 김크루는 카카오톡 오픈 채팅방을 개설한 사람을 위해, 다양한 사람들이 들어오고, 나가는 것을 지켜볼 수 있는 관리자창을 만들기로 했다. 채팅방에 누군가 들어오면 다음 메시지가 출력된다.</p><p>[닉네임]님이 들어왔습니다.</p><p>채팅방에서 누군가 나가면 다음 메시지가 출력된다.</p><p>[닉네임]님이 나갔습니다.</p><p>채팅방에서 닉네임을 변경하는 방법은 다음과 같이 두 가지이다.</p><p>채팅방을 나간 후, 새로운 닉네임으로 다시 들어간다.<br>채팅방에서 닉네임을 변경한다.<br>닉네임을 변경할 때는 기존에 채팅방에 출력되어 있던 메시지의 닉네임도 전부 변경된다.</p><p>예를 들어, 채팅방에 Muzi와 Prodo라는 닉네임을 사용하는 사람이 순서대로 들어오면 채팅방에는 다음과 같이 메시지가 출력된다.</p><p>Muzi님이 들어왔습니다.<br>Prodo님이 들어왔습니다.</p><p>채팅방에 있던 사람이 나가면 채팅방에는 다음과 같이 메시지가 남는다.</p><p>Muzi님이 들어왔습니다.<br>Prodo님이 들어왔습니다.<br>Muzi님이 나갔습니다.</p><p>Muzi가 나간후 다시 들어올 때, Prodo 라는 닉네임으로 들어올 경우 기존에 채팅방에 남아있던 Muzi도 Prodo로 다음과 같이 변경된다.</p><p>Prodo님이 들어왔습니다.<br>Prodo님이 들어왔습니다.<br>Prodo님이 나갔습니다.<br>Prodo님이 들어왔습니다.</p><p>채팅방은 중복 닉네임을 허용하기 때문에, 현재 채팅방에는 Prodo라는 닉네임을 사용하는 사람이 두 명이 있다. 이제, 채팅방에 두 번째로 들어왔던 Prodo가 Ryan으로 닉네임을 변경하면 채팅방 메시지는 다음과 같이 변경된다.</p><p>Prodo님이 들어왔습니다.<br>Ryan님이 들어왔습니다.<br>Prodo님이 나갔습니다.<br>Prodo님이 들어왔습니다.</p><p>채팅방에 들어오고 나가거나, 닉네임을 변경한 기록이 담긴 문자열 배열 record가 매개변수로 주어질 때, 모든 기록이 처리된 후, 최종적으로 방을 개설한 사람이 보게 되는 메시지를 문자열 배열 형태로 return 하도록 solution 함수를 완성하라.</p><h2 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h2><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">record는 다음과 같은 문자열이 담긴 배열이며, 길이는 1 이상 100,000 이하이다.</span><br><span class="line">다음은 record에 담긴 문자열에 대한 설명이다.</span><br><span class="line"> - 모든 유저는 [유저 아이디]로 구분한다.</span><br><span class="line"> - [유저 아이디] 사용자가 [닉네임]으로 채팅방에 입장 - Enter [유저 아이디] [닉네임] (ex. Enter uid1234 Muzi)</span><br><span class="line"> - [유저 아이디] 사용자가 채팅방에서 퇴장 - Leave [유저 아이디] (ex. Leave uid1234)</span><br><span class="line"> - [유저 아이디] 사용자가 닉네임을 [닉네임]으로 변경 - Change [유저 아이디] [닉네임] (ex. Change uid1234 Muzi)</span><br><span class="line"> - 첫 단어는 Enter, Leave, Change 중 하나이다.</span><br><span class="line"> - 각 단어는 공백으로 구분되어 있으며, 알파벳 대문자, 소문자, 숫자로만 이루어져있다.</span><br><span class="line"> - 유저 아이디와 닉네임은 알파벳 대문자, 소문자를 구별한다.</span><br><span class="line"> - 유저 아이디와 닉네임의 길이는 1 이상 10 이하이다.</span><br><span class="line"> - 채팅방에서 나간 유저가 닉네임을 변경하는 등 잘못 된 입력은 주어지지 않는다.</span><br></pre></td></tr></table></figure><table><thead><tr><th align="left"><center>record</center></th><th align="center"><center>result</center></th></tr></thead><tbody><tr><td align="left">[“Enter uid1234 Muzi”, “Enter uid4567 Prodo”,”Leave uid1234”,”Enter uid1234 Prodo”,”Change uid4567 Ryan”]</td><td align="center"><center>[“Prodo님이 들어왔습니다.”, “Ryan님이 들어왔습니다.”, “Prodo님이 나갔습니다.”, “Prodo님이 들어왔습니다.”] </center></td></tr></tbody></table><h2 id="문제풀이"><a href="#문제풀이" class="headerlink" title="문제풀이"></a>문제풀이</h2><hr><p>문제풀이가 굉장히 간단해보였다. 나는 하드코딩을 하였지만 그래도 쉽게 풀릴 것이라 생각했다.<br>제한사항을 읽어봤더니 Enter, Change, Leave로 분기만 하면될 것이라 생각했다.</p><ol><li>Enter<ul><li>userId를 검색하여 이미 존재하는 id라면 모든 이름을 바꾼 후 추가</li><li>존재하지 않는다면 추가</li></ul></li><li>Change<ul><li>userId를 검색하여 이름을 모두 바꿈</li></ul></li><li>Leave<ul><li>userId를 검색하여 추가</li></ul></li></ol><p>우선 Leave는 id만 존재하고 닉네임이 존재하지 않아 벡터에 추가하였다.<br>그리고 맨마지막엔 translation을 하는 과정을 추가하였다.<br>따라서 다음과 같이 풀이하였다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;string&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">bool find(string s, string name ,vector&lt;vector&lt;string&gt;&gt; &amp;before) &#123;</span><br><span class="line">    bool answer &#x3D; false;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; before.size(); i++) &#123;</span><br><span class="line">        if (before[i][1] &#x3D;&#x3D; s) &#123;</span><br><span class="line">            answer &#x3D; true;</span><br><span class="line">            before[i][2] &#x3D; name;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return answer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">string append(string s, vector&lt;vector&lt;string&gt;&gt;&amp; before) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; before.size(); i++) &#123;</span><br><span class="line">        if (before[i][1] &#x3D;&#x3D; s)</span><br><span class="line">            return before[i][1];</span><br><span class="line">    &#125;</span><br><span class="line">    return &quot;잘못된 입력&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void change(string s, string name,vector&lt;vector&lt;string&gt;&gt;&amp; before) &#123;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; before.size(); i++) &#123;</span><br><span class="line">        if (before[i][1] &#x3D;&#x3D; s)</span><br><span class="line">            before[i][2] &#x3D; name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vector&lt;string&gt; solution(vector&lt;string&gt; record) &#123;</span><br><span class="line">    vector&lt;string&gt; answer;</span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; word;</span><br><span class="line">    vector&lt;vector&lt;string&gt;&gt; before;</span><br><span class="line"></span><br><span class="line">    for (int i &#x3D; 0; i &lt; record.size(); i++) &#123;</span><br><span class="line">        vector&lt;string&gt; s;</span><br><span class="line">        int count &#x3D; 0;</span><br><span class="line">        for (int j &#x3D; 0; j &lt; record[i].size()+1; j++) &#123;</span><br><span class="line">            if (record[i][j] &#x3D;&#x3D; &#39; &#39; || j&#x3D;&#x3D;record[i].size()) &#123;</span><br><span class="line">                s.emplace_back(record[i].substr(count, j-count));</span><br><span class="line">                count &#x3D; j + 1;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        word.emplace_back(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for (int i &#x3D; 0; i &lt; word.size(); i++) &#123;</span><br><span class="line">        if (word[i][0] &#x3D;&#x3D; &quot;Enter&quot;) &#123;</span><br><span class="line">            if (find(word[i][1],word[i][2], before)) &#123;</span><br><span class="line">                before.emplace_back(word[i]);</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line">            else</span><br><span class="line">                before.emplace_back(word[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        else if (word[i][0] &#x3D;&#x3D; &quot;Leave&quot;) &#123;</span><br><span class="line">            word[i].emplace_back(append(word[i][1], before));</span><br><span class="line">            before.emplace_back(word[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">            change(word[i][1],word[i][2], before);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; before.size(); i++) &#123;</span><br><span class="line">        string s;</span><br><span class="line">        s.append(before[i][2]);</span><br><span class="line">        s.append(&quot;님이 &quot;);</span><br><span class="line">        if (before[i][0] &#x3D;&#x3D; &quot;Enter&quot;) s.append(&quot;들어왔습니다.&quot;);</span><br><span class="line">        else s.append(&quot;나갔습니다.&quot;);</span><br><span class="line">        answer.emplace_back(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return answer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>일단 결과가 매우 안좋았다. 실패하는 경우도 있었을 뿐만 아니라 시간초과도 떴기 때문이다.</p><p>모를 땐 빨리 구글링이 답이다.</p><h3 id="BEST"><a href="#BEST" class="headerlink" title="BEST "></a><center><a href="https://life-with-coding.tistory.com/395">BEST</a> </center></h3><p>내가 처음보는 STL이 있었다.</p><hr><h3 id="StringStream"><a href="#StringStream" class="headerlink" title="StringStream"></a>StringStream</h3><p>해당 자료형에 맞는 필요한 정보를 추출하는 stream 클래스이다.<br><sstream>헤더파일이 필요하다.</p><p>stringstream 연산자/함수</p><ul><li>“&gt;&gt;” : 데이터를 추출하는 연산자</li><li>“&lt;&lt;” : 데이터를 삽입하는 연산자</li><li>“obj.str(target)” : obj의 string을 변환하여 target에 저장</li></ul><hr><h3 id="해결코드"><a href="#해결코드" class="headerlink" title="해결코드"></a>해결코드</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;string&gt;</span><br><span class="line">#include &lt;sstream&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">#include &lt;map&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">map&lt;string, string&gt; m;</span><br><span class="line"></span><br><span class="line">vector&lt;string&gt; solution(vector&lt;string&gt; record)</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;string&gt; answer;</span><br><span class="line">    string id, name, state;</span><br><span class="line">    stringstream ss;</span><br><span class="line"></span><br><span class="line">    for(int i&#x3D;0;i&lt;record.size();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ss.str(record[i]);</span><br><span class="line">        ss &gt;&gt; state; ss &gt;&gt; id; ss &gt;&gt; name;</span><br><span class="line">        if(state &#x3D;&#x3D; &quot;Enter&quot; || state &#x3D;&#x3D; &quot;Change&quot;)&#123;</span><br><span class="line">            m[id] &#x3D; name;</span><br><span class="line">        &#125;</span><br><span class="line">        ss.clear();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for(int i&#x3D;0;i&lt;record.size();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ss.str(record[i]);</span><br><span class="line">        ss &gt;&gt; state; ss &gt;&gt; id;</span><br><span class="line">        name &#x3D; m[id];</span><br><span class="line">        if(state &#x3D;&#x3D; &quot;Enter&quot;) &#123;</span><br><span class="line">            answer.push_back(name + &quot;님이 들어왔습니다.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        if(state &#x3D;&#x3D; &quot;Leave&quot;)&#123;</span><br><span class="line">            answer.push_back(name + &quot;님이 나갔습니다.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        ss.clear();</span><br><span class="line">    &#125;</span><br><span class="line">    return answer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>새로운 STL을 발견하여 기분은 좋았다. 빨리 코딩하려다보니 해시도 까먹고 후다닥 처리하다보니 저런 망작이 나왔다.<br>위의 코드를 참고하여 다른 문제들에 적용시켜보도록 해야겠다.</p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Programmers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Java]Day02 - 자바 데이터타입,변수 그리고 배열</title>
      <link href="2021/01/20/java-day2/"/>
      <url>2021/01/20/java-day2/</url>
      
        <content type="html"><![CDATA[<h2 id="Review-Day01"><a href="#Review-Day01" class="headerlink" title="Review Day01"></a>Review Day01</h2><p>오늘 포스트에 앞서 지난 시간에 다뤘던 Compile에 대해서 다시 한 번 다뤄보겠다.</p><blockquote><p>Q1. java14로 compile한 파일을 java8로 실행하면 어떻게 될 것인가?<br>Q2. java8로 compile한 파일을 java14로 실행하면 어떻게 될 것인가?</p></blockquote><p>Q1의 정답은 <code>Error</code>, Q2의 정답은 <code>Success</code>이다. 쉽게 생각하면 낮은 버젼의 컴파일러로 컴파일 시 높은 버젼의 자바로 실행은 가능하지만 높은 버젼의 컴파일러로 컴파일 시 낮은 버젼의 자바로 실행은 불가능하다.</p><blockquote><p>Q3. 그렇다면 java14버젼으로 compile시에는 java8로는 절대 실행불가능한 것인가?</p></blockquote><p>Q3의 정답은 <code>False</code>이다. <code>javac -source 1.6 -target 1.6</code>옵션을 줌으로써 컴파일시 실행도 가능해진다. 높은 버젼에서만 사용할 수 있는 기능도 컴파일러가 낮은버젼에 호환되도록 컴파일을 하기 때문에 실행이 가능하지는것이다. (모든 기능이 가능한 것은 아니니까 굳이 그러진말자!)<br>그러나 정말 안타깝게도, 일부 Maven plugin이 간혹 특정버젼으로 만들어지고 낮은버젼으로 호환이 안되어 프로젝트 시 확인을 해야한다.</p><p>높은 버젼의 컴파일시 낮은 버젼으로 실행하면 다음과 같은 에러가 뜬다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Error: A JNI error has occurred, please check your installation and try again</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.InsupportedClassVersionError: 파일명 has been compiled by a more recent version of the Java Runtime (class file version 58.0), this version of the Java Runtime only recognizes class file versions up to 52.0</span><br></pre></td></tr></table></figure><p>이는 위에 설명했던 그대로의 말을 의미한다.</p><blockquote><p>Q4. javac는 어디에 들어있는가?</p></blockquote><p>Q4의 정답은 <code>JDK</code>이다. <code>compile</code>을 하는 것은 개발자의 몫이기 때문에 JDK에만 들어가있다. JAVA9부터는 JRE는 따로 배포 안하므로 나중에 확인해보자.</p><blockquote><p>Q5. 바이트코드는 왜 바이트코드에요?</p></blockquote><p>Q5의 정답은 <code>opcode가 1바이트이기 때문</code>이다. 시스템프로그래밍 수업을 들었다면 알겠지만 기계어로 바꾸는 과정 중에 바이트코드나 어셈블리어같은 것들은 opcode라는 것이 존재한다. 이것이 1바이트이기 때문에 바이트코드라고 한다. 또한, <strong>java뿐만 아니라 다른 언어</strong>들도 적절한 컴파일러로 통해 바이트코드로 변환이 된다면 jvm은 이를 실행할 수 있다.</p><hr><h2 id="Day02-자바-데이터-타입-변수-그리고-배열"><a href="#Day02-자바-데이터-타입-변수-그리고-배열" class="headerlink" title="Day02. 자바 데이터 타입, 변수 그리고 배열"></a>Day02. 자바 데이터 타입, 변수 그리고 배열</h2><hr><h3 id="프리미티브-타입-종류와-값의-범위-그리고-기본-값"><a href="#프리미티브-타입-종류와-값의-범위-그리고-기본-값" class="headerlink" title="프리미티브 타입 종류와 값의 범위 그리고 기본 값"></a>프리미티브 타입 종류와 값의 범위 그리고 기본 값</h3><p>변수의 타입에는 크게 두가지로 나눌 수 있다.</p><ol><li>Primitive Type</li><li>Reference Type</li></ol><p>이중 Primitive Type에는 총 8개의 타입이 있으며, 크게 논리형, 문자형, 정수형, 실수형으로 구분된다.<br>문자형인 char는 문자를 내부적으로 정수(유니코드)로 저장하기 때문에 정수형과 별반 다르지 않으며, <strong>정수형 또는 실수형과 연산도 가능하다.</strong> 반면에 boolean은 다른 기본형과의 연산이 불가능하다. 즉, boolean을 제외한 나머지 7개의 기본형은 서로 연산과 변환이 가능하다. 정수는 가장 ㅁ많이 사용되므로 타입을 4가지나 제공한다. 각 타입마다 저장할 수 있는 값의 범위가 다르므로 저장할 값의 범위에 맞는 타입을 선택하면 되지만, 일반적으로 int를 많이 사용한다. 왜냐하면, int는 CPU가 가장 효율적으로 처리할 수 있는 타입이기 때문이다. 효율적인 실행보다 메모리를 절약하려면, byte나 short를 선택하자.</p><p>Reference Type은 다음과 같다.</p><blockquote><p>“The basic difference is that primitive variables store the actual values, whereas reference variables store the addresses of the objects they refer to.”    <a href="https://stackoverflow.com/a/32049775">출처</a></p></blockquote><h3 id="상수와-리터럴"><a href="#상수와-리터럴" class="headerlink" title="상수와 리터럴"></a>상수와 리터럴</h3><p>‘상수(constant)’는 변수와 마찬가지로 ‘값을 저장할 수 있는 공간’이다. 하지만 말그대로 변수는 변할 수 있는 공간이지만 상수는 변할 수 없는 공간이다. 상수의 이름은 모두 <strong>대문자</strong>로 하는 것이 암묵적인 관례이며, 여러 단어로 이루어져있는경우 ‘_’로 구분한다. (ex. <code>final int MAX_SPEED</code>)<br>‘리터럴(literal)’은 상수와 구분짓기 위해 만든 것으로 그 자체로 값을 의미한다.</p><blockquote><p>“A literal is the source code representation of a fixed value; literals are represented directly in your code without requiring computation. As shown below, it’s possible to assign a literal to a variable of a primitive type”   <a href="https://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html">출처</a></p></blockquote><h3 id="변수의-스코프와-라이프타임"><a href="#변수의-스코프와-라이프타임" class="headerlink" title="변수의 스코프와 라이프타임"></a>변수의 스코프와 라이프타임</h3><p>변수의 스코프란 말그대로 스코프이다. 즉, 변수가 살아있는 범위에 대해 말하는 것이다. 라이프타임과 스코프는 붙어다니는 단어들이다.<br>이제 우리는 변수를 instance, static, local로 나눌 수 있는데 이는 다음과 같이 쓸 수 있다. <a href="https://www.tutorialspoint.com/scope-and-lifetime-of-variables-in-java">출처</a></p><ol><li><p>Instance Variable<br>Scope : throughout the class except in static methods<br>Lifetime : until the object stays in memory.</p></li><li><p>Static(Class) Variable<br>Scope : throughout the class<br>Lifetime : until the end of the program or as long as the class is loaded in memory.</p></li><li><p>Local Variable<br>Scope : within the block in which it is declared<br>Lifetime : until the control leaves the block in which it is declared.</p></li></ol><h3 id="타입-변환-캐스팅-그리고-타입-프로모션"><a href="#타입-변환-캐스팅-그리고-타입-프로모션" class="headerlink" title="타입 변환, 캐스팅 그리고 타입 프로모션"></a>타입 변환, 캐스팅 그리고 타입 프로모션</h3><p>타입 변환이란 말 그대로 타입 변환이다.(Dejavu..) byte로 선언했던 변수를 int 또는 기타 type으로 변환시키는 것이다. 하지만 이는 굉장히 중요한 문제이다. C를 처음 접했었다면 처음에 부딪혔던 난관이 있을것이다. 0.5를 표현하는 것이다. int형 변수 a,b를 각각 1, 2로 했다면 우리는 0.5는 당연히 a/b일 것이라 생각했겠지만 절대 0.5가 출력되지 않는다. 이는 변수의 자료형이 int로 반환되기 때문에 절대 원하는 값인 0.5가 아닌 0이 출력이 되었던걸 경험했을 것이다. 그래서 필요한 것이 이런 타입변환이다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">double c &#x3D; (double)(a&#x2F;b);</span><br></pre></td></tr></table></figure><p>이런식으로 하는 것이 캐스팅이다. 굉장히 간단하지 않은가? 원하는 타입으로 앞에 괄호를 붙여쓰기만 하면되는 것이다.<br>하지만 세상은 그렇게 녹록치 않다. boolean을 제외한 Primitive Type간에는 자유롭게 변환이 가능하지만(물론 데이터 손실은 있을 수 있다.)  Primitive Type간에도 형변환을 하는것이 원칙이지만, 값의 범위가 작은 타입에서 큰 타입으로의 형변환은 생략이 가능하다. Reference Type간에는 implicit up-casting, explicit down-casting이 이루어진다.</p><h3 id="1차-및-2차-배열-선언하기"><a href="#1차-및-2차-배열-선언하기" class="headerlink" title="1차 및 2차 배열 선언하기"></a>1차 및 2차 배열 선언하기</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int[] a &#x3D; &#123;1,2,3&#125;;</span><br><span class="line">int[][] b &#x3D; &#123;&#123;2,3,4&#125;,&#123;5,6,7&#125;&#125;;</span><br><span class="line">이부분은 이정도로만 해두고 다음시간에 Review를 통해 더 자세히 다뤄보도록 하겠다.</span><br></pre></td></tr></table></figure><h3 id="타입-추론-var"><a href="#타입-추론-var" class="headerlink" title="타입 추론, var"></a>타입 추론, var</h3><p>코틀린을 써보며 자주 사용했던 <code>var</code>이다. 그때는 뭔지도 모르고 무조건! <code>var</code>로 선언했었다. 물론 차이가 있겠지만 우선 자바로 배워보도록 하겠다. 우선 <code>var</code>은 java10에서 생겨난 것이다. <code>var</code>로 선언한다면 런타임시 변수의 종류를 파악해 알아서 변환시켜준다.  타입추론이란 자바자체적으로 어떤 타입인지 예측하고, 그것을 사용하는 것을 의미한다.</p><p> <a href="https://johngrib.github.io/wiki/java10-var/">올바른 사용과 잘못된 사용</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">1</span>;            <span class="comment">// Legal</span></span><br><span class="line"><span class="keyword">var</span> b = <span class="number">2</span>, c = <span class="number">3.0</span>;   <span class="comment">// Illegal: multiple declarators</span></span><br><span class="line"><span class="keyword">var</span> d[] = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">4</span>]; <span class="comment">// Illegal: extra bracket pairs</span></span><br><span class="line"><span class="keyword">var</span> e;                <span class="comment">// Illegal: no initializer</span></span><br><span class="line"><span class="keyword">var</span> f = &#123; <span class="number">6</span> &#125;;        <span class="comment">// Illegal: array initializer</span></span><br><span class="line"><span class="keyword">var</span> g = (g = <span class="number">7</span>);      <span class="comment">// Illegal: self reference in initializer</span></span><br></pre></td></tr></table></figure><p> <a href="https://johngrib.github.io/wiki/java10-var/">타입</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a = <span class="number">1</span>;                <span class="comment">// a has type &#x27;int&#x27;</span></span><br><span class="line"><span class="keyword">var</span> b = java.util.List.of(<span class="number">1</span>, <span class="number">2</span>);  <span class="comment">// b has type &#x27;List&lt;Integer&gt;&#x27;</span></span><br><span class="line"><span class="keyword">var</span> c = <span class="string">&quot;x&quot;</span>.getClass();</span><br><span class="line">                          <span class="comment">// (see JLS 15.12.2.6)</span></span><br><span class="line"><span class="keyword">var</span> d = <span class="keyword">new</span> Object() &#123;&#125;;  <span class="comment">// d has the type of the anonymous class</span></span><br><span class="line"><span class="keyword">var</span> e = (CharSequence &amp; Comparable&lt;String&gt;) <span class="string">&quot;x&quot;</span>;</span><br><span class="line">                          <span class="comment">// e has type CharSequence &amp; Comparable&lt;String&gt;</span></span><br><span class="line"><span class="keyword">var</span> f = () -&gt; <span class="string">&quot;hello&quot;</span>;    <span class="comment">// Illegal: lambda not in an assignment context</span></span><br><span class="line"><span class="keyword">var</span> g = <span class="keyword">null</span>;             <span class="comment">// Illegal: null type</span></span><br></pre></td></tr></table></figure><p>엄청 편해보여서 모든 변수를 var로 선언해버리는 것이 어떻겠냐는 생각을 할 수 있겠지만 아쉽게도 그러면 안된다. 예상치못한 에러에 대처할 수 없어진다. 그렇다면 언제 사용하는 것이냐?에 대해서는 <a href="https://velog.io/@composite/Java-10-%EC%97%90%EC%84%9C-var-%EC%9E%AC%EB%8C%80%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0">다음의 글</a>을 살펴보는 것이 좋겠다. <code>제네릭</code>,<code>람다</code>,<code>foreach</code>구문 등 쓰는 부분이 굉장히 많다. 이는 추후에 더 자세히 다뤄보도록 하겠다.</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java8 </tag>
            
            <tag> java10 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[AAIS]Intro.</title>
      <link href="2021/01/19/aais-00/"/>
      <url>2021/01/19/aais-00/</url>
      
        <content type="html"><![CDATA[<h1 id="In-AAIS-Advanced-Application-for-Intelligence-Systems-Lab-at-Hanyang-University"><a href="#In-AAIS-Advanced-Application-for-Intelligence-Systems-Lab-at-Hanyang-University" class="headerlink" title="In AAIS(Advanced Application for Intelligence Systems) Lab at Hanyang University"></a>In AAIS(Advanced Application for Intelligence Systems) Lab at Hanyang University</h1><h2 id="들어가기에-앞서"><a href="#들어가기에-앞서" class="headerlink" title="들어가기에 앞서"></a>들어가기에 앞서</h2><p>이번 겨울방학에 무엇을 해야할 지 굉장히 생각이 많았다. 자바백엔드개발자로써 시작을 위해 자바8,스프링부트를 시작했고 이제 코딩테스트도 준비해야한다 생각해서 <code>Programmers</code>와 <code>Leetcode</code>의 코딩테스트 연습문제를 풀고 있다. 또한, 도커를 공부할 것이다.<br>아직 끝나지 않았다. 봉사활동도 하지않아서 봉사활동(온라인..)도 신청했고 계절학기도 신청해서 오늘자(210119)로 마무리 됐다. (선형대수 만세)<br>이렇게 할게 많은 와중에 노영균교수님께서 AAIS LAB 인턴을 모집하셔서 바로 신청을 했다.<br>비록 늦게 신청을 했지만 교수님께서 받아주셔서 인턴으로써 활동을 하게 되었다.</p><blockquote><p>이번방학 할 목록</p></blockquote><ol><li>AAIS LAB 인턴</li><li>JAVA8</li><li>스프링부트</li><li>도커</li><li>코딩테스트준비</li><li>봉사활동</li><li><del>계절학기</del></li></ol><p>일단 목표는 크게 잡고 하나씩 쳐낼(?) 것이다. 아직까지는 만족스러운 방학생활이다.</p><p>근황은 이쯤에서 마무리하고, 랩실에서 할 내용에 대해 간략히 정리해보겠다.</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>내가 다룰 주제는 <a href="https://www.nature.com/articles/s41598-020-80182-8">Age-gruop determination of living individuals using first molar images based on artificial intelligence</a>이다.<br>교수님께서 가장 쉽게 구현할 수 있다고 강조해주신 자료이다. 아직 텐서플로우는 커녕 이론도 잘 모르는 상태라 가장 끌렸다. 이것은 두번째 이유이고 첫번째로는 <strong>재미</strong>있어보였다. 추리장르를 좋아하는 사람으로써 치아의 나이를 추론한다는 내용은 굉장히 흥미로웠기 때문이다.</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>Dental age estimation of living individuals is difcult and challenging, and there is no consensus method in adults with permanent dentition. Thus, we aimed to provide an accurate and robust artifcial intelligence (AI)-based diagnostic system for age-group estimation by incorporating a convolutional neural network (CNN) using dental X-ray image patches of the frst molars extracted via panoramic radiography. The data set consisted of four frst molar images from the right and left sides of the maxilla and mandible of each of 1586 individuals across all age groups, which were extracted from their panoramic radiographs. The accuracy of the tooth-wise estimation was 89.05 to 90.27%. Performance accuracy was evaluated mainly using a majority voting system and area under curve (AUC) scores. The AUC scores ranged from 0.94 to 0.98 for all age groups, which indicates outstanding capacity. The learned features of CNNs were visualized as a heatmap, and revealed that CNNs focus on diferentiated anatomical parameters, including tooth pulp, alveolar bone level, or interdental space, depending on the age and location of the tooth. With this, we provided a deeper understanding of the most informative regions distinguished by age groups. The prediction accuracy and heat map analyses support that this AI-based age-group determination model is plausible and useful.</p></blockquote><p>내가 정말 정말 간단하게 요약하자면 1586명의 첫번째 어금니 4개를 데이터를 통해 나이구간을 추정한다는 것이다. 특정한 나이를 구하는 것이 아니라 나이구간을 구한다. CNN을 통해 학습시키며 잇속(tooth pulp), 이틀뼈(alveolar bone level), 또는 수특부(interdental space)를 <strong>나이와 치아의 위치</strong>에 따라 파라미터로 받는다.<br>여기서 중요하게 봐야할 것은 convolutional neural network(CNN)과 파라미터를 다르게 받는다는 점이다. 모든 이를 같은 특징을 통해 구별하는 것이 아니라 나이대, 치아의 위치에 따라서 다르게 구별한다는 것이다. 이는 나중에 다루고(잘 모른다..) 이번 포스트에서는 CNN으로 간단하게 시작해보겠다.</p><h2 id="CNN-Convolutional-Neural-Network"><a href="#CNN-Convolutional-Neural-Network" class="headerlink" title="CNN(Convolutional Neural Network)"></a>CNN(Convolutional Neural Network)</h2><p align="center"><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnIswa%2FbtqFrmYm9zI%2FcF0xZms06Zkdjy95Bt0K8k%2Fimg.gif"></p><p>위 그림과 같이 일반적으로 CNN모델은 3 * 3 필터를 주로 사용한다. 위 그림에서는 3개의 channel에서 3개의 convolutional filter를 사용하여 곱을하고 그 결과를 모두 합하는 것이다.<br>위에서의 곱이란 Convolution Product(합성곱)을 말한다. 그 수학적 공식은 다음과 같다.</p><p align="center"><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fd6EbAa%2FbtqFpBf5GwW%2Fb2ne6NbSP2mbQA9hT0NvpK%2Fimg.jpg"></p><br><p>수학적인 얘기는 아직 잘 모를뿐더러 나는 이론적인 얘기보단 용도와 결과에 대해서만 우선! 다룰 것이다.</p><p align="center"><img src="https://blogfiles.pstatic.net/20160105_9/laonple_14519535933045r2XJ_JPEG/%C0%CC%B9%CC%C1%F6_207.jpg?type=w2" height = "300px" width = "600px"></p><p align="center"><img src="https://blogfiles.pstatic.net/20160105_103/laonple_1451953593820FopUd_JPEG/%C0%CC%B9%CC%C1%F6_208.jpg?type=w2" height = "300px" width = "600px"></p><p>CNN을 사용하는 이유는 위의 그림에서의 이유가 가장 크다. 같은 A라는 문자를 입력한다 해도 한 픽셀만 translation되어도 다른 이미지로 인식을 하기 때문에 문제가 된다. data가 굉장히 많아 많은 양을 학습시켰다고 하더라도 약간이라도 translation된다면 전혀 모르는 데이터가 되기 때문이다. 따라서 이를 사용하게 된다.</p><p>앞선 움짤(.gif)을 봤듯이 필터를 이용하여 얻은 output을 Maxpooling Layer을 통과시켜 이미지의 특징점을 잡을 수 있다고 한다.</p><p>여기서 Pooling Layer란 차례로 처리되는 데이터의 크기를 줄이는 것을 말한다. 이 과정으로 모델의 전체 파라미터의 수를 크게 줄일 수 있다.<br>풀링에는 MaxPooling과 AveragePooling이 존재하는데, MaxPooling은 해당 영역에서 최댓값을 찾는 방법이고, AveragePooling은 해당 영역의 평균값을 계산하는 방법이다.<br>이러한 과정을 거치는 이유에 대해서 잠깐 말하자면 더 높은 정확도를 위해서는 필터가 많아야 하는데, 필터가 늘어날 수록 Feature Map이 늘어난다.이는 딥러닝 모델의 Dimension이 늘어난다는 것이고, High Dimension 모델은 그만큼 파라미터의 수 또한 늘어난는 것이다.이는 Over fitting의 문제점 뿐만이 아니라, 모델의 사이즈와 레이턴시에도 큰 영향을 끼친다. 따라서 차원을 감소시킬 필요성이 있는데,이것을 하는 방법이 Pooling Layer를 활용하는 것이다. 오버피팅에 대해서는 다음에 또 batchnormalization을 다루며 할것이다.</p><p>우선 간단하게 정리한 내용은 위와 같다. 아직 모르는 내용도 많고 겉핥기식 공부라 많이 부족한 글이다. 나중에 이 글을 봤을 때 조금이나마 노력했다라는 생각이 들었다면 오늘의 나는 만족할 것이다.</p>]]></content>
      
      
      <categories>
          
          <category> AAIS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> AAIS </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Java]Day01 - JVM이란?</title>
      <link href="2021/01/19/java-day1/"/>
      <url>2021/01/19/java-day1/</url>
      
        <content type="html"><![CDATA[<h2 id="1-JVM이란"><a href="#1-JVM이란" class="headerlink" title="1. JVM이란?"></a>1. JVM이란?</h2><h3 id="JVM의-용도와-정의"><a href="#JVM의-용도와-정의" class="headerlink" title="JVM의 용도와 정의"></a>JVM의 용도와 정의</h3><hr><p>JVM에는 2가지 기본 기능이 있다.<br> 첫번째로 자바 프로그램이 어느 기기, 또는 어느 운영체제 상에서도 실행될 수 있게 하는 것과 프로그램 메모리를 관리하고 최적화하는 것이다. 이에 대해서는 JVM 구성요소에서 자세히 다뤄보겠다.<br> 가장 유명한 원칙인 “한 번 작성해, 어디에서나 실행한다.(Write Once, Run Anywhere, WORA)”가 바로 이것이다. 즉, 자바 애플리케이션은 JVM 위에서 동작하기 때문에 JVM에 종속적이나, OS와 하드웨어와는 독립적이라 프로그램의 변경없이 실행이 가능하다.<br>자바가 나온 시점에는 메모리관리와 OS와 하드웨어 의존성이 있었기 때문에 이는 큰 장점이기도 했다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- 기술적 정의 : JVM은 코드를 실행하고 해당 코드에 대해 런타임 환경을 제공하는 소프트웨어 프로그램에 대한 사양(Specification)이다.</span><br><span class="line">- 일반적 정의 : JVM은 자바 프로그램을 실행하는 방법이다. JVM의 설정을 구성한 다음 설정사항에 따라 실행 중에 프로그램 리소스를 관리한다.</span><br></pre></td></tr></table></figure><h3 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a><strong>Garbage Collection</strong></h3><p> 자바 이전에는 프로그래머가 모든 프로그램 메모리를 관리했었다. 자바에서는 JVM이 프로그램 메모리를 관리한다. JVM은 가비지 컬렉션이란 프로세스를 통해 메모리를 관리하며, 이는 프로그램에서 사용되지 않는 메모리를 지속적으로 찾아내서 제거한다. 즉, 실행중인 JVM내부에서 일어난다.<br> 그럼 왜 많은 개발자들이 C나 C++같이 메모리관리를 직접해야하는 언어를 사용하냐는 생각이 들 수 있다. 하지만 편리함에는 대가를 치루는 법. 앞서 말했듯 JVM이 지속적으로 가비지 컬렉션을 하기 때문에 시간이 그만큼 많이 들어간다. 하지만, 직접관리를 한다면 그 시간을 줄일 수 있을 것이다. 그래서 초창기 자바의 가비지컬렉션에 대해 속도측면에서 많이 까였다. 하지만, 지금은 많은 개발과 최적화를 통해 크게 개선되어 메탈에 가까워졌다.</p><hr><h3 id="컴파일-하는-방법"><a href="#컴파일-하는-방법" class="headerlink" title="컴파일 하는 방법"></a>컴파일 하는 방법</h3><p><a href="https://zerodark.tistory.com/14">컴파일 하는 과정</a><br>위 링크에 컴파일 하는 과정을 잘 요약해주었다.</p><p>컴파일 하는 과정은 다음과 같다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 소스코드를 작성한다.(.java)</span><br><span class="line">2. 컴파일러를 통해 바이트코드로 컴파일한다. (.class)</span><br><span class="line">3. java명령어로 프로그램을 실행한다.</span><br></pre></td></tr></table></figure><p>대부분의 나같은 주니어개발자들은 intelliJ나 이클립스같은 IDE를 통해 컴파일부터 실행까지 모두 하지만 위의 과정들은 당연히 알아두어야한다.</p><hr><h3 id="바이트코드란"><a href="#바이트코드란" class="headerlink" title="바이트코드란?"></a>바이트코드란?</h3><p>앞선 과정중에 <code>바이트코드</code>라는 것이나온다. 보통 C나 C++을 배우다보면 이런 것은 들어본 적이 없다. 바이트 코드의 정의는 다음과 같다.</p><blockquote><p>바이트코드(Bytecode, portable code, p-code)는 특정 하드웨어가 아닌 가상 컴퓨터에서 돌아가는 실행 프로그램을 위한 이진 표현법이다. 하드웨어가 아닌 소프트웨어에 의해 처리되기 때문에, 보통 기계어보다 더 추상적이다.<br>다음의 자바코드가 있다고 하자.</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">outer:</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">2</span>; j &lt; i; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i % j == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">continue</span> outer;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println (i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>이는 다음과 같은 바이트코드로 번역된다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">0:   iconst_2</span><br><span class="line">1:   istore_1</span><br><span class="line">2:   iload_1</span><br><span class="line">3:   sipush  1000</span><br><span class="line">6:   if_icmpge       44</span><br><span class="line">9:   iconst_2</span><br><span class="line">10:  istore_2</span><br><span class="line">11:  iload_2</span><br><span class="line">12:  iload_1</span><br><span class="line">13:  if_icmpge       31</span><br><span class="line">16:  iload_1</span><br><span class="line">17:  iload_2</span><br><span class="line">18:  irem</span><br><span class="line">19:  ifne    25</span><br><span class="line">22:  goto    38</span><br><span class="line">25:  iinc    2, 1</span><br><span class="line">28:  goto    11</span><br><span class="line">31:  getstatic       #84; &#x2F;&#x2F; Field java&#x2F;lang&#x2F;System.out:Ljava&#x2F;io&#x2F;PrintStream;</span><br><span class="line">34:  iload_1</span><br><span class="line">35:  invokevirtual   #85; &#x2F;&#x2F; Method java&#x2F;io&#x2F;PrintStream.println:(I)V</span><br><span class="line">38:  iinc    1, 1</span><br><span class="line">41:  goto    2</span><br><span class="line">44:  return</span><br></pre></td></tr></table></figure><p>바이트코드도 컴파일의 과정이므로 당연히 사람이 읽기 쉽도록 쓰인 소스코드보다 덜 추상적이며, 더 간결하고, 더 컴퓨터 중심적이다.(어셈블리어와 비슷한형태다.) 실은 아까 보여준 컴파일 과정중에 한가지 과정이 생략되었는데, 바로 실행전에 JIT컴파일러로 기계코드로 바꾸는 것이다. 이에 대해서는 다음 과정에서 살펴보겠다.</p><hr><h3 id="JIT-컴파일러란"><a href="#JIT-컴파일러란" class="headerlink" title="JIT 컴파일러란?"></a>JIT 컴파일러란?</h3><p>정의는 다음과 같다.</p><blockquote><p>JIT 컴파일(just-in-time compilation) 또는 동적 번역(dynamic translation)은 프로그램을 실제 실행하는 시점에 기계어로 번역하는 컴파일 기법이다.</p></blockquote><p>이런 기계어 변환은 코드가 실행되는 과정에 실시간으로 일어나며(그래서 Just-In-Time이다), 전체 코드의 필요한 부분만 변환한다. 기계어로 변환된 코드는 캐시에 저장되기 때문에 재사용시 컴파일을 다시 할 필요가 없다.</p><p>일반적인 <code>인터프러터 언어(예시: cpython)</code>는 바이트코드나 소스코드를 최적화 과정이 없기 번역하기 때문에 성능이 낮다. 반면 <code>정적으로 컴파일하는 언어(예시: c 언어)</code>는 실행 전에 무조건 컴파일을 해야하기 때문에 다양한 플랫폼에 맞게 컴파일을 하려면 시간이 오래 걸린다. <code>동적 컴파일 환경</code>은 실행 과정에서 컴파일을 할 수 있기 위해 만들어졌다. JIT는 정적 컴파일러 만큼 빠르면서 인터프러터 언어의 빠른 응답속도를 추구하기 위해 사용한다. 바이트코드 컴파일러가 시간이 많이 소요되는 최적화를 미리 해주기 때문에 바이트코드에서 기계어 번역은 훨씬 빠르게 진행될 수 있다. 또한 바이트코드는 이식성이 뛰어나 가상 머신이 설치되어 있으면 빠르게 실행할 수 있다.</p><hr><h3 id="JVM-구성요소"><a href="#JVM-구성요소" class="headerlink" title="JVM 구성요소"></a>JVM 구성요소</h3><p align="center"><img src="https://d2.naver.com/content/images/2015/06/helloworld-1230-1.png"></p><p>JVM에는 3가지 측면이 있다고 할 수 있다. 표준(Specification), 구현(Implementation ) 그리고 인스턴스(Instance)인데, 각각에 대해 살펴보자.</p><h4 id="1-표준-Specification"><a href="#1-표준-Specification" class="headerlink" title="1. 표준(Specification)"></a>1. 표준(Specification)</h4><p>첫째, JVM은 소프트웨어 사양이다. 다소 순환적인 방식으로, JVM 사양은 구현에 있어 최대한의 창조성을 허용하기 위해, JVM 구현 세부사항이 사양 안에 정의되어 있지 않다고 강조하고 있다.<br>결국, JVM이 해야만 하는 일은 자바 프로그램을 정확하게 실행하는 것뿐이다. 간단해 보인다, 심지어 겉으로 보기에는 단순해 보이기도 하지만, 자바 언어의 능력과 유연성을 고려할 때, 이것은 엄청나게 힘든 일이다.</p><h4 id="2-구현-Implementation"><a href="#2-구현-Implementation" class="headerlink" title="2. 구현(Implementation)"></a>2. 구현(Implementation)</h4><p>JVM 사양 구현은 실제 소프트웨어 프로그램을 도출하며, 이것이 JVM 구현이다. 실제로, 오픈소스와 특정 업체 고유의 JVM 구현이 다수 존재한다. 오픈JDK의 핫스팟(HotSpot) JVM은 참조 구현이며, 세계에서 가장 철저하게 증명된 코드기반 중 하나로 남아있다. 핫스팟은 가장 널리 사용되는 JVM이기도 하다.</p><p>오라클의 라이선스가 부여된 JDK를 포함해, 라이선스가 부여되는 거의 모든 JVM은 오픈JDK와 핫스팟 JVM의 포크(Fork)로 생성된 것이다. 오픈JDK로부터 허가받은 포크를 생성하는 개발자들은 종종 운영체제 고유의 성능 개선사항들을 추가하려는 욕구에 의해 동기 부여된다. 일반적으로, 개발자는 JRE(Java Runtime Environment) 번들의 한 부분으로 JVM을 다운로드해 설치한다.</p><h4 id="3-인스턴스-Instance"><a href="#3-인스턴스-Instance" class="headerlink" title="3. 인스턴스(Instance)"></a>3. 인스턴스(Instance)</h4><p>JVM 스펙이 구현돼서 소프트웨어 제품으로 릴리즈되면, 개발자는 그것을 하나의 프로그램처럼 다운로드해 실행할 수 있다. 이렇게 다운로드 된 프로그램이 하나의 JVM 인스턴스(또는 인스턴스화된 버전)이다.</p><p>개발자들이 “JVM”에 대해 말하는 경우, 대부분의 경우에는 소프트웨어 개발 환경 또는 제품화 환경에서 실행되는 하나의 JVM 인스턴스를 지칭한다. “아난드, 그 서버에 있는 JVM은 메모리를 얼마나 사용하고 있어?” 또는 “순환 호출(Circular Call)을 하는 바람에 스택 오버플로우 에러가 내 JVM을 망가뜨렸다니, 믿을 수가 없군. 이런 초보적인 실수를 하다니!”라고 말할 지도 모른다.</p><hr><h3 id="JDK와-JRE의-차이"><a href="#JDK와-JRE의-차이" class="headerlink" title="JDK와 JRE의 차이"></a>JDK와 JRE의 차이</h3><p>간단하게 설명하면 다음과 같다.</p><p><strong>JRE란?</strong><br>먼저 JRE는 Java Runtime Environment의 약자로 자바 프로그램을 실행시켜주는 환경을 구성해주는 도구다. 즉 JAVA를 개발할 필요는 없는데, 실행은 시켜줘야 하는 경우에는 꼭 JRE가 있어야 한다. 반면 JAVA 개발시 꼭 필요한 것이 있는데 그것이 바로 JDK다.</p><p><strong>JDK란?</strong><br>JDK는 Java Development Kit의 약자로 말그대로 자바 개발시 필요한 툴킷을 제공하는 도구모음이다. 개발하려면 당연히 실행도 시켜야 하므로 JDK 안에는 JRE가 포함되어 있다. </p><p>사용자 입장에서 요약하자면<br>JAVA로 만들어진 프로그램을 실행만 시킬 것이라면  JRE만 설치하면 되고<br>JAVA 개발자라면 JDK를 설치하면 된다.</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java8 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Algorithm] 멀쩡한사각형</title>
      <link href="2021/01/10/algorithm_210119/"/>
      <url>2021/01/10/algorithm_210119/</url>
      
        <content type="html"><![CDATA[<h1 id="멀쩡한-사각형-Summer-Winter-Coding-2019"><a href="#멀쩡한-사각형-Summer-Winter-Coding-2019" class="headerlink" title="멀쩡한 사각형 (Summer/Winter Coding(2019))"></a>멀쩡한 사각형 (Summer/Winter Coding(2019))</h1><hr><h2 id="문제설명"><a href="#문제설명" class="headerlink" title="문제설명"></a>문제설명</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> 가로 길이가 Wcm, 세로 길이가 Hcm인 직사각형 종이가 있습니다. 종이에는 가로, 세로 방향과 평행하게 격자 형태로 선이 그어져 있으며, 모든 격자칸은 1cm x 1cm 크기입니다. 이 종이를 격자 선을 따라 1cm × 1cm의 정사각형으로 잘라 사용할 예정이었는데, 누군가가 이 종이를 대각선 꼭지점 2개를 잇는 방향으로 잘라 놓았습니다.</span><br><span class="line"> 그러므로 현재 직사각형 종이는 크기가 같은 직각삼각형 2개로 나누어진 상태입니다. 새로운 종이를 구할 수 없는 상태이기 때문에, 이 종이에서 원래 종이의 가로, 세로 방향과 평행하게 1cm × 1cm로 잘라 사용할 수 있는 만큼만 사용하기로 하였습니다.</span><br><span class="line">가로의 길이 W와 세로의 길이 H가 주어질 때, 사용할 수 있는 정사각형의 개수를 구하는 solution 함수를 완성해 주세요.</span><br></pre></td></tr></table></figure><hr><h2 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W, H : 1억 이하의 자연수</span><br></pre></td></tr></table></figure><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><table><thead><tr><th>W</th><th align="center">H</th><th align="right">result</th></tr></thead><tbody><tr><td>8</td><td align="center">12</td><td align="right">80</td></tr></tbody></table><hr><h2 id="입출력-예-설명"><a href="#입출력-예-설명" class="headerlink" title="입출력 예 설명"></a>입출력 예 설명</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">입출력 예 #1</span><br><span class="line">가로가 8, 세로가 12인 직사각형을 대각선 방향으로 자르면 총 16개 정사각형을 사용할 수 없게 됩니다. 원래 직사각형에서는 96개의 정사각형을 만들 수 있었으므로, 96 - 16 &#x3D; 80 을 반환합니다.</span><br></pre></td></tr></table></figure><p><img src="https://grepp-programmers.s3.amazonaws.com/files/production/ee895b2cd9/567420db-20f4-4064-afc3-af54c4a46016.png" alt="example"></p><hr><h2 id="문제풀이"><a href="#문제풀이" class="headerlink" title="문제풀이"></a>문제풀이</h2><p>처음에 문제를 봤을 때 바로 떠오른 생각이 최대공약수 개념이었다.<br>위에 있는 그림은 스크롤을 내리지 못해 미처 발견하지 못했고 혼자서 생각해보았다.</p><p>5분 정도 혼자 고민을 한 끝에 최대공약수로는 판단할 수 없다고 생각을 했다.</p><p>따라서, 구글링을 통해 [대각선이 지나는 점의 개수]를 찾는 방법을 찾았다.<br>[대각선이 지나는 점의 개수] : <a href="https://m.blog.naver.com/orbis1020/220664563768">https://m.blog.naver.com/orbis1020/220664563768</a><br><br><br><img src="https://mblogthumb-phinf.pstatic.net/20160324_159/orbis1020_1458747302465IxfMV_PNG/%B0%DD%C0%DA.png?type=w2" alt="example"></p><p>초등학교 5학년의 수학문제라는 점에서 자괴감이 들었다. 나는. 4학년이니까 괜찮다! (..대학교)</p><p>본론으로 다시 넘어와서 점의 개수는 위의 그림처럼 격자점의 유무에 따라 달라진다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(1) 격자점이 존재할 경우 : 가로와 세로의 최대공약수가 2일 때</span><br><span class="line">(2) 격자점이 존재하지 않을 경우 : 두 수의 최대공약수가 1일 때</span><br><span class="line"></span><br><span class="line">(1)의 경우에는 &#39;(가로)+(세로)-1&#39;</span><br><span class="line">(2)의 경우에는 &#39;(가로)+(세로)-(가로와 세로의 최대공약수)</span><br></pre></td></tr></table></figure><p>하지만 둘 다 코드에선 같은 최대공약수를 빼므로 결론적으론 (가로) + (세로) - (최대공약수)임을 알 수 있다.</p><p>최대 공약수를 구하는 방법은 다양한 방법이 있겠지만 Level1에서 배웠던 유클리드호제법을 사용하여 구했다. (Level1_C++_gcdlcm)</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">long long gcd(long long a, long long b)</span><br><span class="line">&#123;</span><br><span class="line">    long c;</span><br><span class="line"></span><br><span class="line">    while (b !&#x3D; 0)</span><br><span class="line">    &#123;</span><br><span class="line">        c &#x3D; a % b;</span><br><span class="line">        a &#x3D; b;</span><br><span class="line">        b &#x3D; c;</span><br><span class="line">    &#125;</span><br><span class="line">    return a;</span><br><span class="line">&#125;</span><br><span class="line">long long solution(int w, int h) &#123;</span><br><span class="line">    long long W &#x3D; w;</span><br><span class="line">    long long H &#x3D; h;</span><br><span class="line"></span><br><span class="line">    return (W * H) - ((W + H) - gcd(W, H));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Programmers </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
