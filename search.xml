<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>[Java]Day01 - JVM이란?</title>
      <link href="2021/01/19/java-day1/"/>
      <url>2021/01/19/java-day1/</url>
      
        <content type="html"><![CDATA[<h2 id="1-JVM이란"><a href="#1-JVM이란" class="headerlink" title="1. JVM이란?"></a>1. JVM이란?</h2><hr><h3 id="JVM의-용도와-정의"><a href="#JVM의-용도와-정의" class="headerlink" title="JVM의 용도와 정의"></a>JVM의 용도와 정의</h3><hr><p>JVM에는 2가지 기본 기능이 있다.<br> 첫번째로 자바 프로그램이 어느 기기, 또는 어느 운영체제 상에서도 실행될 수 있게 하는 것과 프로그램 메모리를 관리하고 최적화하는 것이다. 이에 대해서는 JVM 구성요소에서 자세히 다뤄보겠다.<br> 가장 유명한 원칙인 “한 번 작성해, 어디에서나 실행한다.(Write Once, Run Anywhere, WORA)”가 바로 이것이다. 즉, 자바 애플리케이션은 JVM 위에서 동작하기 때문에 JVM에 종속적이나, OS와 하드웨어와는 독립적이라 프로그램의 변경없이 실행이 가능하다.<br>자바가 나온 시점에는 메모리관리와 OS와 하드웨어 의존성이 있었기 때문에 이는 큰 장점이기도 했다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- 기술적 정의 : JVM은 코드를 실행하고 해당 코드에 대해 런타임 환경을 제공하는 소프트웨어 프로그램에 대한 사양(Specification)이다.</span><br><span class="line">- 일반적 정의 : JVM은 자바 프로그램을 실행하는 방법이다. JVM의 설정을 구성한 다음 설정사항에 따라 실행 중에 프로그램 리소스를 관리한다.</span><br></pre></td></tr></table></figure><h3 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a><strong>Garbage Collection</strong></h3><p> 자바 이전에는 프로그래머가 모든 프로그램 메모리를 관리했었다. 자바에서는 JVM이 프로그램 메모리를 관리한다. JVM은 가비지 컬렉션이란 프로세스를 통해 메모리를 관리하며, 이는 프로그램에서 사용되지 않는 메모리를 지속적으로 찾아내서 제거한다. 즉, 실행중인 JVM내부에서 일어난다.<br> 그럼 왜 많은 개발자들이 C나 C++같이 메모리관리를 직접해야하는 언어를 사용하냐는 생각이 들 수 있다. 하지만 편리함에는 대가를 치루는 법. 앞서 말했듯 JVM이 지속적으로 가비지 컬렉션을 하기 때문에 시간이 그만큼 많이 들어간다. 하지만, 직접관리를 한다면 그 시간을 줄일 수 있을 것이다. 그래서 초창기 자바의 가비지컬렉션에 대해 속도측면에서 많이 까였다. 하지만, 지금은 많은 개발과 최적화를 통해 크게 개선되어 메탈에 가까워졌다.</p><hr><h3 id="컴파일-하는-방법"><a href="#컴파일-하는-방법" class="headerlink" title="컴파일 하는 방법"></a>컴파일 하는 방법</h3><p><a href="https://zerodark.tistory.com/14">컴파일 하는 과정</a><br>위 링크에 컴파일 하는 과정을 잘 요약해주었다.</p><p>컴파일 하는 과정은 다음과 같다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 소스코드를 작성한다.(.java)</span><br><span class="line">2. 컴파일러를 통해 바이트코드로 컴파일한다. (.class)</span><br><span class="line">3. java명령어로 프로그램을 실행한다.</span><br></pre></td></tr></table></figure><p>대부분의 나같은 주니어개발자들은 intelliJ나 이클립스같은 IDE를 통해 컴파일부터 실행까지 모두 하지만 위의 과정들은 당연히 알아두어야한다.</p><hr><h3 id="바이트코드란"><a href="#바이트코드란" class="headerlink" title="바이트코드란?"></a>바이트코드란?</h3><p>앞선 과정중에 <code>바이트코드</code>라는 것이나온다. 보통 C나 C++을 배우다보면 이런 것은 들어본 적이 없다. 바이트 코드의 정의는 다음과 같다.</p><blockquote><p>바이트코드(Bytecode, portable code, p-code)는 특정 하드웨어가 아닌 가상 컴퓨터에서 돌아가는 실행 프로그램을 위한 이진 표현법이다. 하드웨어가 아닌 소프트웨어에 의해 처리되기 때문에, 보통 기계어보다 더 추상적이다.<br>다음의 자바코드가 있다고 하자.</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">outer:</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">2</span>; j &lt; i; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i % j == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">continue</span> outer;</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println (i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>이는 다음과 같은 바이트코드로 번역된다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">0:   iconst_2</span><br><span class="line">1:   istore_1</span><br><span class="line">2:   iload_1</span><br><span class="line">3:   sipush  1000</span><br><span class="line">6:   if_icmpge       44</span><br><span class="line">9:   iconst_2</span><br><span class="line">10:  istore_2</span><br><span class="line">11:  iload_2</span><br><span class="line">12:  iload_1</span><br><span class="line">13:  if_icmpge       31</span><br><span class="line">16:  iload_1</span><br><span class="line">17:  iload_2</span><br><span class="line">18:  irem</span><br><span class="line">19:  ifne    25</span><br><span class="line">22:  goto    38</span><br><span class="line">25:  iinc    2, 1</span><br><span class="line">28:  goto    11</span><br><span class="line">31:  getstatic       #84; &#x2F;&#x2F; Field java&#x2F;lang&#x2F;System.out:Ljava&#x2F;io&#x2F;PrintStream;</span><br><span class="line">34:  iload_1</span><br><span class="line">35:  invokevirtual   #85; &#x2F;&#x2F; Method java&#x2F;io&#x2F;PrintStream.println:(I)V</span><br><span class="line">38:  iinc    1, 1</span><br><span class="line">41:  goto    2</span><br><span class="line">44:  return</span><br></pre></td></tr></table></figure><p>바이트코드도 컴파일의 과정이므로 당연히 사람이 읽기 쉽도록 쓰인 소스코드보다 덜 추상적이며, 더 간결하고, 더 컴퓨터 중심적이다.(어셈블리어와 비슷한형태다.) 실은 아까 보여준 컴파일 과정중에 한가지 과정이 생략되었는데, 바로 실행전에 JIT컴파일러로 기계코드로 바꾸는 것이다. 이에 대해서는 다음 과정에서 살펴보겠다.</p><hr><h3 id="JIT-컴파일러란"><a href="#JIT-컴파일러란" class="headerlink" title="JIT 컴파일러란?"></a>JIT 컴파일러란?</h3><p>정의는 다음과 같다.</p><blockquote><p>JIT 컴파일(just-in-time compilation) 또는 동적 번역(dynamic translation)은 프로그램을 실제 실행하는 시점에 기계어로 번역하는 컴파일 기법이다.</p></blockquote><p>이런 기계어 변환은 코드가 실행되는 과정에 실시간으로 일어나며(그래서 Just-In-Time이다), 전체 코드의 필요한 부분만 변환한다. 기계어로 변환된 코드는 캐시에 저장되기 때문에 재사용시 컴파일을 다시 할 필요가 없다.</p><p>일반적인 <code>인터프러터 언어(예시: cpython)</code>는 바이트코드나 소스코드를 최적화 과정이 없기 번역하기 때문에 성능이 낮다. 반면 <code>정적으로 컴파일하는 언어(예시: c 언어)</code>는 실행 전에 무조건 컴파일을 해야하기 때문에 다양한 플랫폼에 맞게 컴파일을 하려면 시간이 오래 걸린다. <code>동적 컴파일 환경</code>은 실행 과정에서 컴파일을 할 수 있기 위해 만들어졌다. JIT는 정적 컴파일러 만큼 빠르면서 인터프러터 언어의 빠른 응답속도를 추구하기 위해 사용한다. 바이트코드 컴파일러가 시간이 많이 소요되는 최적화를 미리 해주기 때문에 바이트코드에서 기계어 번역은 훨씬 빠르게 진행될 수 있다. 또한 바이트코드는 이식성이 뛰어나 가상 머신이 설치되어 있으면 빠르게 실행할 수 있다.</p><hr><h3 id="JVM-구성요소"><a href="#JVM-구성요소" class="headerlink" title="JVM 구성요소"></a>JVM 구성요소</h3><p align="center"><img src="https://d2.naver.com/content/images/2015/06/helloworld-1230-1.png"></p><p>JVM에는 3가지 측면이 있다고 할 수 있다. 표준(Specification), 구현(Implementation ) 그리고 인스턴스(Instance)인데, 각각에 대해 살펴보자.</p><h4 id="1-표준-Specification"><a href="#1-표준-Specification" class="headerlink" title="1. 표준(Specification)"></a>1. 표준(Specification)</h4><p>첫째, JVM은 소프트웨어 사양이다. 다소 순환적인 방식으로, JVM 사양은 구현에 있어 최대한의 창조성을 허용하기 위해, JVM 구현 세부사항이 사양 안에 정의되어 있지 않다고 강조하고 있다.<br>결국, JVM이 해야만 하는 일은 자바 프로그램을 정확하게 실행하는 것뿐이다. 간단해 보인다, 심지어 겉으로 보기에는 단순해 보이기도 하지만, 자바 언어의 능력과 유연성을 고려할 때, 이것은 엄청나게 힘든 일이다.</p><h4 id="2-구현-Implementation"><a href="#2-구현-Implementation" class="headerlink" title="2. 구현(Implementation)"></a>2. 구현(Implementation)</h4><p>JVM 사양 구현은 실제 소프트웨어 프로그램을 도출하며, 이것이 JVM 구현이다. 실제로, 오픈소스와 특정 업체 고유의 JVM 구현이 다수 존재한다. 오픈JDK의 핫스팟(HotSpot) JVM은 참조 구현이며, 세계에서 가장 철저하게 증명된 코드기반 중 하나로 남아있다. 핫스팟은 가장 널리 사용되는 JVM이기도 하다.</p><p>오라클의 라이선스가 부여된 JDK를 포함해, 라이선스가 부여되는 거의 모든 JVM은 오픈JDK와 핫스팟 JVM의 포크(Fork)로 생성된 것이다. 오픈JDK로부터 허가받은 포크를 생성하는 개발자들은 종종 운영체제 고유의 성능 개선사항들을 추가하려는 욕구에 의해 동기 부여된다. 일반적으로, 개발자는 JRE(Java Runtime Environment) 번들의 한 부분으로 JVM을 다운로드해 설치한다.</p><h4 id="3-인스턴스-Instance"><a href="#3-인스턴스-Instance" class="headerlink" title="3. 인스턴스(Instance)"></a>3. 인스턴스(Instance)</h4><p>JVM 스펙이 구현돼서 소프트웨어 제품으로 릴리즈되면, 개발자는 그것을 하나의 프로그램처럼 다운로드해 실행할 수 있다. 이렇게 다운로드 된 프로그램이 하나의 JVM 인스턴스(또는 인스턴스화된 버전)이다.</p><p>개발자들이 “JVM”에 대해 말하는 경우, 대부분의 경우에는 소프트웨어 개발 환경 또는 제품화 환경에서 실행되는 하나의 JVM 인스턴스를 지칭한다. “아난드, 그 서버에 있는 JVM은 메모리를 얼마나 사용하고 있어?” 또는 “순환 호출(Circular Call)을 하는 바람에 스택 오버플로우 에러가 내 JVM을 망가뜨렸다니, 믿을 수가 없군. 이런 초보적인 실수를 하다니!”라고 말할 지도 모른다.</p><hr><h3 id="JDK와-JRE의-차이"><a href="#JDK와-JRE의-차이" class="headerlink" title="JDK와 JRE의 차이"></a>JDK와 JRE의 차이</h3><p>간단하게 설명하면 다음과 같다.</p><p><strong>JRE란?</strong><br>먼저 JRE는 Java Runtime Environment의 약자로 자바 프로그램을 실행시켜주는 환경을 구성해주는 도구다. 즉 JAVA를 개발할 필요는 없는데, 실행은 시켜줘야 하는 경우에는 꼭 JRE가 있어야 한다. 반면 JAVA 개발시 꼭 필요한 것이 있는데 그것이 바로 JDK다.</p><p><strong>JDK란?</strong><br>JDK는 Java Development Kit의 약자로 말그대로 자바 개발시 필요한 툴킷을 제공하는 도구모음이다. 개발하려면 당연히 실행도 시켜야 하므로 JDK 안에는 JRE가 포함되어 있다. </p><p>사용자 입장에서 요약하자면<br>JAVA로 만들어진 프로그램을 실행만 시킬 것이라면  JRE만 설치하면 되고<br>JAVA 개발자라면 JDK를 설치하면 된다.</p>]]></content>
      
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Using Deep Learning Tools</title>
      <link href="2021/01/17/Using-Deep-Learning-Tools/"/>
      <url>2021/01/17/Using-Deep-Learning-Tools/</url>
      
        <content type="html"><![CDATA[<h1 id="Using-Deep-Learning-Tools"><a href="#Using-Deep-Learning-Tools" class="headerlink" title="Using Deep Learning Tools"></a>Using Deep Learning Tools</h1><h3 id="2021-01-17"><a href="#2021-01-17" class="headerlink" title="2021-01-17"></a>2021-01-17</h3><p>KIAS CAC Winter School 2020</p><p>Dates: 2020-12-16</p><p>Author: Yung-Kyun Noh</p><p>Department of Computer Science, Hanyang University</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before start, check the gpu.</span></span><br><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><pre><code>Sun Jan 17 08:19:51 2021+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 || N/A   41C    P0    50W / 300W |   1254MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 || N/A   40C    P0    54W / 300W |    319MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 || N/A   40C    P0    50W / 300W |  30762MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 || N/A   40C    P0    51W / 300W |    345MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================|+-----------------------------------------------------------------------------+</code></pre><h1 id="Start"><a href="#Start" class="headerlink" title="Start"></a>Start</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Conv2D, Flatten</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># using gpu:/3</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the data, split between train and test sets</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;# of train data : &quot;</span>,x_train.shape)</span><br><span class="line">print(<span class="string">&quot;# of test data : &quot;</span>,x_test.shape)</span><br></pre></td></tr></table></figure><pre><code># of train data :  (60000, 28, 28)# of test data :  (10000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(x_train[<span class="number">10</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x7fcb20d60160&gt;</code></pre><p><img src="output_7_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vectorization for ANN</span></span><br><span class="line">x_train_vectorize = x_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">x_test_vectorize = x_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.keras <span class="keyword">as</span> keras</span><br><span class="line">num_categories = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">y_train_onehot = keras.utils.to_categorical(y_train, num_categories)</span><br><span class="line">y_test_onehot = keras.utils.to_categorical(y_test, num_categories)</span><br><span class="line"></span><br><span class="line">print(y_train_onehot, y_test_onehot)</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. ... 0. 0. 0.] [1. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 1. 0.]] [[0. 0. 0. ... 1. 0. 0.] [0. 0. 1. ... 0. 0. 0.] [0. 1. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]</code></pre><h1 id="Keras-Sequential-model"><a href="#Keras-Sequential-model" class="headerlink" title="Keras Sequential model"></a>Keras Sequential model</h1><h2 id="Specify-Input-Shape"><a href="#Specify-Input-Shape" class="headerlink" title="Specify Input Shape"></a>Specify Input Shape</h2><hr><p>Model should know the Input Shape. So, <code>Sequential</code>model’s first layer(after this, layer know the shape) shold give the information.</p><ul><li>Pass the <code>input_shape</code> to the first layer. This is a tuple containing shape information.(A tuple with an integer or <code>None</code> as an entry; <code>None</code> represents an arbitrary positive integer). The <code>input_shape</code> does not include batch dimension.</li><li>Some two-dimensional layers, such as <code>Dense</code>, can specify the input shape through the <code>input_dim</code> argument, and some three-dimensional layers(temporal) support the <code>input_dim</code> and <code>input_length</code> arguments.</li></ul><h2 id="Compile"><a href="#Compile" class="headerlink" title="Compile"></a>Compile</h2><hr><p>Before Learning, you must configure the learning process with the <code>compile</code> method. This Method accepts three arguments.</p><ol><li>optimizer : It can be a string identifier that represents a built-in optimizer (such as <code>rmsprop</code> or <code>adagrad</code>), or an instance of the <code>optimizer</code> class.</li><li>loss : Loss function. This is what the model wants to minimize. It can be the string identifier(<code>categorical_crossentropy</code> or <code>mse</code>, and so on) or an instance of the target function itself of the built-in loss function</li><li>metrics : List of Metrics. If you solve the classification problem, It is recommended to use <code>metrics = [&#39;accuracy&#39;]</code>. Metrics can be string identifiers for built-in metrics or user-defined metric functions.</li></ol><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><hr><p>The Keras model is trained based on input data and labels from the Numpy array. When you train a model, you typically use the <code>fit</code> function.</p><h3 id="For-the-better-explanation"><a href="#For-the-better-explanation" class="headerlink" title="For the better explanation"></a><a href="https://www.codeonweb.com/entry/eda18bec-7c7d-426f-ab98-90e18db6fdba">For the better explanation</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Dense(units=<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(Dense(units = <span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(units = <span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================dense (Dense)                (None, 512)               401920_________________________________________________________________dense_1 (Dense)              (None, 512)               262656_________________________________________________________________dense_2 (Dense)              (None, 10)                5130=================================================================Total params: 669,706Trainable params: 669,706Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train_vectorize, y_train_onehot,</span><br><span class="line">                    epochs=<span class="number">5</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=(x_test_vectorize, y_test_onehot))</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/51875/1875 [==============================] - 6s 3ms/step - loss: 2.1721 - accuracy: 0.8889 - val_loss: 0.5640 - val_accuracy: 0.9274Epoch 2/51875/1875 [==============================] - 6s 3ms/step - loss: 0.5268 - accuracy: 0.9308 - val_loss: 0.4797 - val_accuracy: 0.9368Epoch 3/51875/1875 [==============================] - 6s 3ms/step - loss: 0.4393 - accuracy: 0.9416 - val_loss: 0.4109 - val_accuracy: 0.9521Epoch 4/51875/1875 [==============================] - 6s 3ms/step - loss: 0.3902 - accuracy: 0.9464 - val_loss: 0.8686 - val_accuracy: 0.9227Epoch 5/51875/1875 [==============================] - 6s 3ms/step - loss: 0.4102 - accuracy: 0.9485 - val_loss: 0.4740 - val_accuracy: 0.9440</code></pre><h1 id="Layer"><a href="#Layer" class="headerlink" title="Layer"></a>Layer</h1><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><hr><h3 id="1-ReLU-Rectified-Linear-Unit-activation-function"><a href="#1-ReLU-Rectified-Linear-Unit-activation-function" class="headerlink" title="1. ReLU(Rectified Linear Unit activation function)"></a>1. ReLU(Rectified Linear Unit activation function)</h3><p><img src="attachment:d3e5fdb1-84ea-4563-9a6d-4d95d7198352.png" alt="image.png"></p><ul><li><strong>Features</strong></li></ul><ol><li>It makes the vanishing gradient problem(tanh, sigmoid have) <strong>MUCH WORSE</strong>,since for all negative values the derivative is precisely zero.</li><li>Computational Cost is not significant.</li><li>In comparison to (sigmoid,tanh), convergence speed is much better.</li></ol><h3 id="2-softmax"><a href="#2-softmax" class="headerlink" title="2. softmax"></a>2. softmax</h3><p><img src="attachment:aef09041-a830-4c7d-93ac-71bb9e56fde4.png" alt="image.png"></p><ul><li><strong>Features</strong></li></ul><ol><li>A function that normalizes the output value of a neuron at the last stage for class classification.(normalize sigmoid function)</li><li>So, we can think that It converts the output into probability. (sum = 1.0)</li></ol><h1 id="New-model-with-convolution"><a href="#New-model-with-convolution" class="headerlink" title="New model with convolution"></a>New model with convolution</h1><h1 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h1><hr><h2 id="Conv2D-Layer"><a href="#Conv2D-Layer" class="headerlink" title="Conv2D Layer"></a>Conv2D Layer</h2><h4 id="For-the-better-explanation-1"><a href="#For-the-better-explanation-1" class="headerlink" title="For the better explanation"></a><a href="https://underflow101.tistory.com/40?category=826164">For the better explanation</a></h4><p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/d6EbAa/btqFpBf5GwW/b2ne6NbSP2mbQA9hT0NvpK/img.jpg"></p><ul><li><strong>Features</strong></li></ul><ol><li>By receiving images in N-dimensions as they are, then result output is N-dimensional data. so it can learning feature-bearing data better.</li><li>Input option</li></ol><ul><li>stride : control cross-correlation</li><li>padding : control # of implicit zero-padding for each dimmension’s padding # of points</li><li>dilation : control spacing between kernel points</li><li>groups : control connection between input and output.</li></ul><h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Conv2D(</span><br><span class="line">    filters, kernel_size, strides&#x3D;(1, 1), padding&#x3D;&#39;valid&#39;, data_format&#x3D;None,</span><br><span class="line">    dilation_rate&#x3D;(1, 1), activation&#x3D;None, use_bias&#x3D;True,</span><br><span class="line">    kernel_initializer&#x3D;&#39;glorot_uniform&#39;, bias_initializer&#x3D;&#39;zeros&#39;,</span><br><span class="line">    kernel_regularizer&#x3D;None, bias_regularizer&#x3D;None, activity_regularizer&#x3D;None,</span><br><span class="line">    kernel_constraint&#x3D;None, bias_constraint&#x3D;None, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line">model.add(Conv2D(<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_1&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================conv2d (Conv2D)              (None, 26, 26, 64)        640_________________________________________________________________conv2d_1 (Conv2D)            (None, 24, 24, 32)        18464_________________________________________________________________flatten (Flatten)            (None, 18432)             0_________________________________________________________________dense_3 (Dense)              (None, 10)                184330=================================================================Total params: 203,434Trainable params: 203,434Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x_train_forConv = x_train.reshape(<span class="number">60000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line">x_test_forConv = x_test.reshape(<span class="number">10000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train_forConv, y_train_onehot,</span><br><span class="line">                    epochs=<span class="number">5</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=(x_test_forConv, y_test_onehot))</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/51875/1875 [==============================] - 7s 4ms/step - loss: 0.3264 - accuracy: 0.9541 - val_loss: 0.0932 - val_accuracy: 0.9720Epoch 2/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0837 - accuracy: 0.9772 - val_loss: 0.1164 - val_accuracy: 0.9672Epoch 3/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0735 - accuracy: 0.9800 - val_loss: 0.0732 - val_accuracy: 0.9771Epoch 4/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0719 - accuracy: 0.9805 - val_loss: 0.0846 - val_accuracy: 0.9783Epoch 5/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0757 - accuracy: 0.9801 - val_loss: 0.0877 - val_accuracy: 0.9762</code></pre><h2 id="It-shutdown-the-kernel-you-must-do-this-for-the-GPU-resources"><a href="#It-shutdown-the-kernel-you-must-do-this-for-the-GPU-resources" class="headerlink" title="It shutdown the kernel. you must do this for the GPU resources."></a>It shutdown the kernel. you must do this for the GPU resources.</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> IPython</span><br><span class="line">app = IPython.Application.instance()</span><br><span class="line">app.kernel.do_shutdown(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;status&#39;: &#39;ok&#39;, &#39;restart&#39;: True&#125;</code></pre><h2 id="After"><a href="#After" class="headerlink" title="After"></a>After</h2><p>gpu:/3  1340 Mib ———&gt; 345 Mib</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><pre><code>Sun Jan 17 08:22:46 2021+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 || N/A   41C    P0    50W / 300W |   1254MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 || N/A   40C    P0    54W / 300W |    319MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 || N/A   40C    P0    50W / 300W |  30762MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 || N/A   40C    P0    51W / 300W |    345MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================|+-----------------------------------------------------------------------------+</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Algorithm] 멀쩡한사각형</title>
      <link href="2021/01/10/algorithm_210119/"/>
      <url>2021/01/10/algorithm_210119/</url>
      
        <content type="html"><![CDATA[<h1 id="멀쩡한-사각형-Summer-Winter-Coding-2019"><a href="#멀쩡한-사각형-Summer-Winter-Coding-2019" class="headerlink" title="멀쩡한 사각형 (Summer/Winter Coding(2019))"></a>멀쩡한 사각형 (Summer/Winter Coding(2019))</h1><hr><h2 id="문제설명"><a href="#문제설명" class="headerlink" title="문제설명"></a>문제설명</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> 가로 길이가 Wcm, 세로 길이가 Hcm인 직사각형 종이가 있습니다. 종이에는 가로, 세로 방향과 평행하게 격자 형태로 선이 그어져 있으며, 모든 격자칸은 1cm x 1cm 크기입니다. 이 종이를 격자 선을 따라 1cm × 1cm의 정사각형으로 잘라 사용할 예정이었는데, 누군가가 이 종이를 대각선 꼭지점 2개를 잇는 방향으로 잘라 놓았습니다.</span><br><span class="line"> 그러므로 현재 직사각형 종이는 크기가 같은 직각삼각형 2개로 나누어진 상태입니다. 새로운 종이를 구할 수 없는 상태이기 때문에, 이 종이에서 원래 종이의 가로, 세로 방향과 평행하게 1cm × 1cm로 잘라 사용할 수 있는 만큼만 사용하기로 하였습니다.</span><br><span class="line">가로의 길이 W와 세로의 길이 H가 주어질 때, 사용할 수 있는 정사각형의 개수를 구하는 solution 함수를 완성해 주세요.</span><br></pre></td></tr></table></figure><hr><h2 id="제한사항"><a href="#제한사항" class="headerlink" title="제한사항"></a>제한사항</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W, H : 1억 이하의 자연수</span><br></pre></td></tr></table></figure><h2 id="입출력-예"><a href="#입출력-예" class="headerlink" title="입출력 예"></a>입출력 예</h2><table><thead><tr><th>W</th><th align="center">H</th><th align="right">result</th></tr></thead><tbody><tr><td>8</td><td align="center">12</td><td align="right">80</td></tr></tbody></table><hr><h2 id="입출력-예-설명"><a href="#입출력-예-설명" class="headerlink" title="입출력 예 설명"></a>입출력 예 설명</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">입출력 예 #1</span><br><span class="line">가로가 8, 세로가 12인 직사각형을 대각선 방향으로 자르면 총 16개 정사각형을 사용할 수 없게 됩니다. 원래 직사각형에서는 96개의 정사각형을 만들 수 있었으므로, 96 - 16 &#x3D; 80 을 반환합니다.</span><br></pre></td></tr></table></figure><p><img src="https://grepp-programmers.s3.amazonaws.com/files/production/ee895b2cd9/567420db-20f4-4064-afc3-af54c4a46016.png" alt="example"></p><hr><h2 id="문제풀이"><a href="#문제풀이" class="headerlink" title="문제풀이"></a>문제풀이</h2><p>처음에 문제를 봤을 때 바로 떠오른 생각이 최대공약수 개념이었다.<br>위에 있는 그림은 스크롤을 내리지 못해 미처 발견하지 못했고 혼자서 생각해보았다.</p><p>5분 정도 혼자 고민을 한 끝에 최대공약수로는 판단할 수 없다고 생각을 했다.</p><p>따라서, 구글링을 통해 [대각선이 지나는 점의 개수]를 찾는 방법을 찾았다.<br>[대각선이 지나는 점의 개수] : <a href="https://m.blog.naver.com/orbis1020/220664563768">https://m.blog.naver.com/orbis1020/220664563768</a><br><br><br><img src="https://mblogthumb-phinf.pstatic.net/20160324_159/orbis1020_1458747302465IxfMV_PNG/%B0%DD%C0%DA.png?type=w2" alt="example"></p><p>초등학교 5학년의 수학문제라는 점에서 자괴감이 들었다. 나는. 4학년이니까 괜찮다! (..대학교)</p><p>본론으로 다시 넘어와서 점의 개수는 위의 그림처럼 격자점의 유무에 따라 달라진다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(1) 격자점이 존재할 경우 : 가로와 세로의 최대공약수가 2일 때</span><br><span class="line">(2) 격자점이 존재하지 않을 경우 : 두 수의 최대공약수가 1일 때</span><br><span class="line"></span><br><span class="line">(1)의 경우에는 &#39;(가로)+(세로)-1&#39;</span><br><span class="line">(2)의 경우에는 &#39;(가로)+(세로)-(가로와 세로의 최대공약수)</span><br></pre></td></tr></table></figure><p>하지만 둘 다 코드에선 같은 최대공약수를 빼므로 결론적으론 (가로) + (세로) - (최대공약수)임을 알 수 있다.</p><p>최대 공약수를 구하는 방법은 다양한 방법이 있겠지만 Level1에서 배웠던 유클리드호제법을 사용하여 구했다. (Level1_C++_gcdlcm)</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">long long gcd(long long a, long long b)</span><br><span class="line">&#123;</span><br><span class="line">    long c;</span><br><span class="line"></span><br><span class="line">    while (b !&#x3D; 0)</span><br><span class="line">    &#123;</span><br><span class="line">        c &#x3D; a % b;</span><br><span class="line">        a &#x3D; b;</span><br><span class="line">        b &#x3D; c;</span><br><span class="line">    &#125;</span><br><span class="line">    return a;</span><br><span class="line">&#125;</span><br><span class="line">long long solution(int w, int h) &#123;</span><br><span class="line">    long long W &#x3D; w;</span><br><span class="line">    long long H &#x3D; h;</span><br><span class="line"></span><br><span class="line">    return (W * H) - ((W + H) - gcd(W, H));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Programmers </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
