<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Using Deep Learning Tools</title>
      <link href="2021/01/17/Using-Deep-Learning-Tools/"/>
      <url>2021/01/17/Using-Deep-Learning-Tools/</url>
      
        <content type="html"><![CDATA[<h1 id="Using-Deep-Learning-Tools"><a href="#Using-Deep-Learning-Tools" class="headerlink" title="Using Deep Learning Tools"></a>Using Deep Learning Tools</h1><h3 id="2021-01-17"><a href="#2021-01-17" class="headerlink" title="2021-01-17"></a>2021-01-17</h3><p>KIAS CAC Winter School 2020</p><p>Dates: 2020-12-16</p><p>Author: Yung-Kyun Noh</p><p>Department of Computer Science, Hanyang University</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before start, check the gpu.</span></span><br><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><pre><code>Sun Jan 17 08:19:51 2021+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 || N/A   41C    P0    50W / 300W |   1254MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 || N/A   40C    P0    54W / 300W |    319MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 || N/A   40C    P0    50W / 300W |  30762MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 || N/A   40C    P0    51W / 300W |    345MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================|+-----------------------------------------------------------------------------+</code></pre><h1 id="Start"><a href="#Start" class="headerlink" title="Start"></a>Start</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Conv2D, Flatten</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># using gpu:/3</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">&#x27;GPU&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpus[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">  <span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the data, split between train and test sets</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;# of train data : &quot;</span>,x_train.shape)</span><br><span class="line">print(<span class="string">&quot;# of test data : &quot;</span>,x_test.shape)</span><br></pre></td></tr></table></figure><pre><code># of train data :  (60000, 28, 28)# of test data :  (10000, 28, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(x_train[<span class="number">10</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x7fcb20d60160&gt;</code></pre><p><img src="output_7_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vectorization for ANN</span></span><br><span class="line">x_train_vectorize = x_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line">x_test_vectorize = x_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow.keras <span class="keyword">as</span> keras</span><br><span class="line">num_categories = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">y_train_onehot = keras.utils.to_categorical(y_train, num_categories)</span><br><span class="line">y_test_onehot = keras.utils.to_categorical(y_test, num_categories)</span><br><span class="line"></span><br><span class="line">print(y_train_onehot, y_test_onehot)</span><br></pre></td></tr></table></figure><pre><code>[[0. 0. 0. ... 0. 0. 0.] [1. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 1. 0.]] [[0. 0. 0. ... 1. 0. 0.] [0. 0. 1. ... 0. 0. 0.] [0. 1. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]]</code></pre><h1 id="Keras-Sequential-model"><a href="#Keras-Sequential-model" class="headerlink" title="Keras Sequential model"></a>Keras Sequential model</h1><h2 id="Specify-Input-Shape"><a href="#Specify-Input-Shape" class="headerlink" title="Specify Input Shape"></a>Specify Input Shape</h2><hr><p>Model should know the Input Shape. So, <code>Sequential</code>model’s first layer(after this, layer know the shape) shold give the information.</p><ul><li>Pass the <code>input_shape</code> to the first layer. This is a tuple containing shape information.(A tuple with an integer or <code>None</code> as an entry; <code>None</code> represents an arbitrary positive integer). The <code>input_shape</code> does not include batch dimension.</li><li>Some two-dimensional layers, such as <code>Dense</code>, can specify the input shape through the <code>input_dim</code> argument, and some three-dimensional layers(temporal) support the <code>input_dim</code> and <code>input_length</code> arguments.</li></ul><h2 id="Compile"><a href="#Compile" class="headerlink" title="Compile"></a>Compile</h2><hr><p>Before Learning, you must configure the learning process with the <code>compile</code> method. This Method accepts three arguments.</p><ol><li>optimizer : It can be a string identifier that represents a built-in optimizer (such as <code>rmsprop</code> or <code>adagrad</code>), or an instance of the <code>optimizer</code> class.</li><li>loss : Loss function. This is what the model wants to minimize. It can be the string identifier(<code>categorical_crossentropy</code> or <code>mse</code>, and so on) or an instance of the target function itself of the built-in loss function</li><li>metrics : List of Metrics. If you solve the classification problem, It is recommended to use <code>metrics = [&#39;accuracy&#39;]</code>. Metrics can be string identifiers for built-in metrics or user-defined metric functions.</li></ol><h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><hr><p>The Keras model is trained based on input data and labels from the Numpy array. When you train a model, you typically use the <code>fit</code> function.</p><h3 id="For-the-better-explanation"><a href="#For-the-better-explanation" class="headerlink" title="For the better explanation"></a><a href="https://www.codeonweb.com/entry/eda18bec-7c7d-426f-ab98-90e18db6fdba">For the better explanation</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Dense(units=<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(Dense(units = <span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(units = <span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================dense (Dense)                (None, 512)               401920_________________________________________________________________dense_1 (Dense)              (None, 512)               262656_________________________________________________________________dense_2 (Dense)              (None, 10)                5130=================================================================Total params: 669,706Trainable params: 669,706Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train_vectorize, y_train_onehot,</span><br><span class="line">                    epochs=<span class="number">5</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=(x_test_vectorize, y_test_onehot))</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/51875/1875 [==============================] - 6s 3ms/step - loss: 2.1721 - accuracy: 0.8889 - val_loss: 0.5640 - val_accuracy: 0.9274Epoch 2/51875/1875 [==============================] - 6s 3ms/step - loss: 0.5268 - accuracy: 0.9308 - val_loss: 0.4797 - val_accuracy: 0.9368Epoch 3/51875/1875 [==============================] - 6s 3ms/step - loss: 0.4393 - accuracy: 0.9416 - val_loss: 0.4109 - val_accuracy: 0.9521Epoch 4/51875/1875 [==============================] - 6s 3ms/step - loss: 0.3902 - accuracy: 0.9464 - val_loss: 0.8686 - val_accuracy: 0.9227Epoch 5/51875/1875 [==============================] - 6s 3ms/step - loss: 0.4102 - accuracy: 0.9485 - val_loss: 0.4740 - val_accuracy: 0.9440</code></pre><h1 id="Layer"><a href="#Layer" class="headerlink" title="Layer"></a>Layer</h1><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><hr><h3 id="1-ReLU-Rectified-Linear-Unit-activation-function"><a href="#1-ReLU-Rectified-Linear-Unit-activation-function" class="headerlink" title="1. ReLU(Rectified Linear Unit activation function)"></a>1. ReLU(Rectified Linear Unit activation function)</h3><p><img src="attachment:d3e5fdb1-84ea-4563-9a6d-4d95d7198352.png" alt="image.png"></p><ul><li><strong>Features</strong></li></ul><ol><li>It makes the vanishing gradient problem(tanh, sigmoid have) <strong>MUCH WORSE</strong>,since for all negative values the derivative is precisely zero.</li><li>Computational Cost is not significant.</li><li>In comparison to (sigmoid,tanh), convergence speed is much better.</li></ol><h3 id="2-softmax"><a href="#2-softmax" class="headerlink" title="2. softmax"></a>2. softmax</h3><p><img src="attachment:aef09041-a830-4c7d-93ac-71bb9e56fde4.png" alt="image.png"></p><ul><li><strong>Features</strong></li></ul><ol><li>A function that normalizes the output value of a neuron at the last stage for class classification.(normalize sigmoid function)</li><li>So, we can think that It converts the output into probability. (sum = 1.0)</li></ol><h1 id="New-model-with-convolution"><a href="#New-model-with-convolution" class="headerlink" title="New model with convolution"></a>New model with convolution</h1><h1 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h1><hr><h2 id="Conv2D-Layer"><a href="#Conv2D-Layer" class="headerlink" title="Conv2D Layer"></a>Conv2D Layer</h2><h4 id="For-the-better-explanation-1"><a href="#For-the-better-explanation-1" class="headerlink" title="For the better explanation"></a><a href="https://underflow101.tistory.com/40?category=826164">For the better explanation</a></h4><p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/d6EbAa/btqFpBf5GwW/b2ne6NbSP2mbQA9hT0NvpK/img.jpg"></p><ul><li><strong>Features</strong></li></ul><ol><li>By receiving images in N-dimensions as they are, then result output is N-dimensional data. so it can learning feature-bearing data better.</li><li>Input option</li></ol><ul><li>stride : control cross-correlation</li><li>padding : control # of implicit zero-padding for each dimmension’s padding # of points</li><li>dilation : control spacing between kernel points</li><li>groups : control connection between input and output.</li></ul><h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.Conv2D(</span><br><span class="line">    filters, kernel_size, strides&#x3D;(1, 1), padding&#x3D;&#39;valid&#39;, data_format&#x3D;None,</span><br><span class="line">    dilation_rate&#x3D;(1, 1), activation&#x3D;None, use_bias&#x3D;True,</span><br><span class="line">    kernel_initializer&#x3D;&#39;glorot_uniform&#39;, bias_initializer&#x3D;&#39;zeros&#39;,</span><br><span class="line">    kernel_regularizer&#x3D;None, bias_regularizer&#x3D;None, activity_regularizer&#x3D;None,</span><br><span class="line">    kernel_constraint&#x3D;None, bias_constraint&#x3D;None, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line">model.add(Conv2D(<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_1&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================conv2d (Conv2D)              (None, 26, 26, 64)        640_________________________________________________________________conv2d_1 (Conv2D)            (None, 24, 24, 32)        18464_________________________________________________________________flatten (Flatten)            (None, 18432)             0_________________________________________________________________dense_3 (Dense)              (None, 10)                184330=================================================================Total params: 203,434Trainable params: 203,434Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">x_train_forConv = x_train.reshape(<span class="number">60000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line">x_test_forConv = x_test.reshape(<span class="number">10000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(x_train_forConv, y_train_onehot,</span><br><span class="line">                    epochs=<span class="number">5</span>,</span><br><span class="line">                    verbose=<span class="number">1</span>,</span><br><span class="line">                    validation_data=(x_test_forConv, y_test_onehot))</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/51875/1875 [==============================] - 7s 4ms/step - loss: 0.3264 - accuracy: 0.9541 - val_loss: 0.0932 - val_accuracy: 0.9720Epoch 2/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0837 - accuracy: 0.9772 - val_loss: 0.1164 - val_accuracy: 0.9672Epoch 3/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0735 - accuracy: 0.9800 - val_loss: 0.0732 - val_accuracy: 0.9771Epoch 4/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0719 - accuracy: 0.9805 - val_loss: 0.0846 - val_accuracy: 0.9783Epoch 5/51875/1875 [==============================] - 7s 4ms/step - loss: 0.0757 - accuracy: 0.9801 - val_loss: 0.0877 - val_accuracy: 0.9762</code></pre><h2 id="It-shutdown-the-kernel-you-must-do-this-for-the-GPU-resources"><a href="#It-shutdown-the-kernel-you-must-do-this-for-the-GPU-resources" class="headerlink" title="It shutdown the kernel. you must do this for the GPU resources."></a>It shutdown the kernel. you must do this for the GPU resources.</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> IPython</span><br><span class="line">app = IPython.Application.instance()</span><br><span class="line">app.kernel.do_shutdown(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;status&#39;: &#39;ok&#39;, &#39;restart&#39;: True&#125;</code></pre><h2 id="After"><a href="#After" class="headerlink" title="After"></a>After</h2><p>gpu:/3  1340 Mib ———&gt; 345 Mib</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><pre><code>Sun Jan 17 08:22:46 2021+-----------------------------------------------------------------------------+| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 || N/A   41C    P0    50W / 300W |   1254MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 || N/A   40C    P0    54W / 300W |    319MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 || N/A   40C    P0    50W / 300W |  30762MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------+|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 || N/A   40C    P0    51W / 300W |    345MiB / 32478MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================|+-----------------------------------------------------------------------------+</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithm  ������ �簢��</title>
      <link href="2021/01/10/Algorithm-%EB%A9%80%EC%A9%A1%ED%95%9C-%EC%82%AC%EA%B0%81%ED%98%95/"/>
      <url>2021/01/10/Algorithm-%EB%A9%80%EC%A9%A1%ED%95%9C-%EC%82%AC%EA%B0%81%ED%98%95/</url>
      
        <content type="html"><![CDATA[<h1 id="������-�簢��-Summer-Winter-Coding-2019"><a href="#������-�簢��-Summer-Winter-Coding-2019" class="headerlink" title="������ �簢�� (Summer/Winter Coding(2019))"></a>������ �簢�� (Summer/Winter Coding(2019))</h1><p>[TOC]</p><hr><h2 id="��������"><a href="#��������" class="headerlink" title="��������"></a>��������</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> ���� ���̰� Wcm, ���� ���̰� Hcm�� ���簢�� ���̰� �ֽ��ϴ�. ���̿��� ����, ���� ����� �����ϰ� ���� ���·� ���� �׾��� ������, ��� ����ĭ�� 1cm x 1cm ũ���Դϴ�. �� ���̸� ���� ���� ���� 1cm �� 1cm�� ���簢������ �߶� ����� �����̾��µ�, �������� �� ���̸� �밢�� ������ 2���� �մ� �������� �߶� ���ҽ��ϴ�.</span><br><span class="line"> �׷��Ƿ� ���� ���簢�� ���̴� ũ�Ⱑ ���� �����ﰢ�� 2���� �������� �����Դϴ�. ���ο� ���̸� ���� �� ���� �����̱� ������, �� ���̿��� ���� ������ ����, ���� ����� �����ϰ� 1cm �� 1cm�� �߶� ����� �� �ִ� ��ŭ�� ����ϱ�� �Ͽ����ϴ�.</span><br><span class="line">������ ���� W�� ������ ���� H�� �־��� ��, ����� �� �ִ� ���簢���� ������ ���ϴ� solution �Լ��� �ϼ��� �ּ���.</span><br></pre></td></tr></table></figure><hr><h2 id="���ѻ���"><a href="#���ѻ���" class="headerlink" title="���ѻ���"></a>���ѻ���</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W, H : 1�� ������ �ڿ���</span><br></pre></td></tr></table></figure><h2 id="�����-��"><a href="#�����-��" class="headerlink" title="����� ��"></a>����� ��</h2><table><thead><tr><th>W</th><th align="center">H</th><th align="right">result</th></tr></thead><tbody><tr><td>8</td><td align="center">12</td><td align="right">80</td></tr></tbody></table><hr><h2 id="�����-��-����"><a href="#�����-��-����" class="headerlink" title="����� �� ����"></a>����� �� ����</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">����� �� #1</span><br><span class="line">���ΰ� 8, ���ΰ� 12�� ���簢���� �밢�� �������� �ڸ��� �� 16�� ���簢���� ����� �� ���� �˴ϴ�. ���� ���簢�������� 96���� ���簢���� ���� �� �־����Ƿ�, 96 - 16 &#x3D; 80 �� ��ȯ�մϴ�.</span><br></pre></td></tr></table></figure><p><img src="https://grepp-programmers.s3.amazonaws.com/files/production/ee895b2cd9/567420db-20f4-4064-afc3-af54c4a46016.png" alt="example"></p><hr><h2 id="����Ǯ��"><a href="#����Ǯ��" class="headerlink" title="����Ǯ��"></a>����Ǯ��</h2><p>ó���� ������ ���� �� �ٷ� ������ ������ �ִ����� �����̾���.<br>���� �ִ� �׸��� ��ũ���� ������ ���� ��ó �߰����� ���߰� ȥ�ڼ� �����غ��Ҵ�.</p><p>5�� ���� ȥ�� ������ �� ���� �ִ������δ� �Ǵ��� �� ���ٰ� ������ �ߴ�.</p><p>����, ���۸��� ���� [�밢���� ������ ���� ����]�� ã�� ����� ã�Ҵ�.<br>[�밢���� ������ ���� ����] : <a href="https://m.blog.naver.com/orbis1020/220664563768">https://m.blog.naver.com/orbis1020/220664563768</a><br><br><br><img src="https://mblogthumb-phinf.pstatic.net/20160324_159/orbis1020_1458747302465IxfMV_PNG/%B0%DD%C0%DA.png?type=w2" alt="example"></p><p>�ʵ��б� 5�г��� ���й������ ������ �ڱ����� �����. ����. 4�г��̴ϱ� ������! (..���б�)</p><p>�������� �ٽ� �Ѿ�ͼ� ���� ������ ���� �׸�ó�� �������� ������ ���� �޶�����.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(1) �������� ������ ��� : ���ο� ������ �ִ������� 2�� ��</span><br><span class="line">(2) �������� �������� ���� ��� : �� ���� �ִ������� 1�� ��</span><br><span class="line"></span><br><span class="line">(1)�� ��쿡�� &#39;(����)+(����)-1&#39;</span><br><span class="line">(2)�� ��쿡�� &#39;(����)+(����)-(���ο� ������ �ִ�����)</span><br></pre></td></tr></table></figure><p>������ �� �� �ڵ忡�� ���� �ִ������� ���Ƿ� ��������� (����) + (����) - (�ִ�����)���� �� �� �ִ�.</p><p>�ִ� ������� ���ϴ� ����� �پ��� ����� �ְ����� Level1���� ����� ��Ŭ����ȣ������ ����Ͽ� ���ߴ�. (Level1_C++_gcdlcm)</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">long long gcd(long long a, long long b)</span><br><span class="line">&#123;</span><br><span class="line">    long c;</span><br><span class="line"></span><br><span class="line">    while (b !&#x3D; 0)</span><br><span class="line">    &#123;</span><br><span class="line">        c &#x3D; a % b;</span><br><span class="line">        a &#x3D; b;</span><br><span class="line">        b &#x3D; c;</span><br><span class="line">    &#125;</span><br><span class="line">    return a;</span><br><span class="line">&#125;</span><br><span class="line">long long solution(int w, int h) &#123;</span><br><span class="line">    long long W &#x3D; w;</span><br><span class="line">    long long H &#x3D; h;</span><br><span class="line"></span><br><span class="line">    return (W * H) - ((W + H) - gcd(W, H));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
