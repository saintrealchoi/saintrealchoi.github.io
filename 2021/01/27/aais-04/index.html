<!DOCTYPE html>
<html lang="zh-CN">

  <head>
  <meta charset="utf-8">
  <meta name="author" content="zchengsite, 1451426471@qq.com" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  
  <title>[AAIS] E04. Transfer Learning - 1 | 성진이의 기술 블로그</title>

  
    <link rel="apple-touch-icon" href="/images/favicon.png">
    <link rel="icon" href="/images/favicon.png">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/iconfont/iconfont.css">
<link rel="stylesheet" href="/css/github-markdown.css">
<link rel="stylesheet" href="/css/highlight.css">


  <!-- jquery3.3.1 -->
  <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

  <!-- fancybox -->
  <link href="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.css" rel="stylesheet">
  <script async src="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.js"></script>
  
<script src="/js/fancybox.js"></script>


  

<meta name="generator" content="Hexo 5.3.0"></head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/">
      <!-- 头像取消懒加载，添加no-lazy -->
      
        <img no-lazy src="/images/avatar.png" alt="">
      
    </a>
    <div class="nickname"><a href="/">성진이의 기술 블로그</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/">
          <a href="/">Home</a>
        </li>
      
        <li class="nav-item" data-path="/archives/">
          <a href="/archives/">Posts</a>
        </li>
      
        <li class="nav-item" data-path="/categories/">
          <a href="/categories/">Categories</a>
        </li>
      
        <li class="nav-item" data-path="/tags/">
          <a href="/tags/">Tags</a>
        </li>
      
        <li class="nav-item" data-path="/about/">
          <a href="/about/">About</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->



  

  

  

  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">[AAIS] E04. Transfer Learning - 1</div>
      <div class="post-attach">
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime" title="업데이트 시간"></i>
          2021-01-27
        </span>
        
              <span class="post-categories">
                <i class="iconfont icon-bookmark" title="카테고리"></i>
                
                <span class="span--category">
                  <a href="/categories/AAIS/" title="AAIS">
                    <b>#</b> AAIS
                  </a>
                </span>
                
              </span>
          
              <span class="post-tags">
                <i class="iconfont icon-tags" title="태그"></i>
                
                <span class="span--tag">
                  <a href="/tags/AI/" title="AI">
                    <b>#</b> AI
                  </a>
                </span>
                
                <span class="span--tag">
                  <a href="/tags/AAIS/" title="AAIS">
                    <b>#</b> AAIS
                  </a>
                </span>
                
                <span class="span--tag">
                  <a href="/tags/keras/" title="keras">
                    <b>#</b> keras
                  </a>
                </span>
                
              </span>
          
      </div>
      <div class="markdown-body">
        <h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><br>

<h2 id="About-VGG16"><a href="#About-VGG16" class="headerlink" title="About VGG16"></a>About VGG16</h2><hr>
<p><a target="_blank" rel="noopener" href="https://analysisbugs.tistory.com/65?category=839091">https://analysisbugs.tistory.com/65?category=839091</a><br><br><a target="_blank" rel="noopener" href="https://bskyvision.com/504">https://bskyvision.com/504</a><br><br><a target="_blank" rel="noopener" href="https://medium.com/@msmapark2/vgg16-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a">https://medium.com/@msmapark2/vgg16-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a</a></p>
<p>이전의 모델들은 대부분 7x7 filter, 11x11 filter를 사용하였으나 이부분부터 3x3 filter를 사용하였습니다.</p>
<p>예시로 10x10을 7x7 filter를 통과시키면 feature map이 4x4가 만들어지는데 필요한 파라미터 수는 7x7 = 49이고<br>10x10모델을 3x3 filter를 한번통과시키면 8x8 map &gt; 두번통과시키면 6x6 map &gt; 세번통과시키면 4x4 featur map이 만들어지는데 필요한 파라미터수는 3x3x3 = 27로 크게 줄어듭니다.<br><br><br><br></p>
<blockquote>
<p>Q.이렇게 할 경우 왜 좋은가? : 3x3일경우 패딩이 1이라서 경계선의 추론이 더 쉬워진다?  + relu를 세번통과하기 때문에 <strong>비 선형성이 증가하게 된다.</strong></p>
</blockquote>
<br>
<br>

<hr>
<h2 id="transfer-learning"><a href="#transfer-learning" class="headerlink" title="transfer learning"></a>transfer learning</h2><ol>
<li>모델을 가져와서 ‘include_top=False’를 통해 분류층(classification)을 삭제시킨다.</li>
<li>‘model.trainable = False’로 설정시켜서 동결(freezing)을 시킨다.</li>
<li>global_average_pooling과 dense layer를 쌓은 후 컴파일한다.</li>
<li>ImageDataGenerator를 통해 적은량의 데이터를 data augmentation을 진행시킨 후 학습시킨다.</li>
<li>‘model.trainable = True’로 설정시켜서 동결을 해제한다.</li>
<li>learning_rate을 매우 낮게 설정하여 fine-tuning하기</li>
</ol>
<br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure>
<pre><code>Mon Jan 25 13:30:28 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |
| N/A   41C    P0    49W / 300W |    947MiB / 32478MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |
| N/A   39C    P0    40W / 300W |     12MiB / 32478MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |
| N/A   40C    P0    52W / 300W |   5134MiB / 32478MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |
| N/A   39C    P0    38W / 300W |     38MiB / 32478MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+</code></pre>
<hr>
<h3 id="1-classification층을-삭제"><a href="#1-classification층을-삭제" class="headerlink" title="1. classification층을 삭제"></a>1. classification층을 삭제</h3><br>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.application.VGG16(weights,input_shape,include_top)</span><br></pre></td></tr></table></figure>
<p><code>include_top</code>은 최상단 Layer인 fully connected layer를 포함시키는지 여부를 전달시키는 것으로 pre-trained된 것을 새롭게 prediction하기위해 이 layer는 제거시켜줍니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line">base_model = keras.applications.VGG16(</span><br><span class="line">    weights=<span class="string">&#x27;imagenet&#x27;</span>,  <span class="comment"># Load weights pre-trained on ImageNet.</span></span><br><span class="line">    input_shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>),</span><br><span class="line">    include_top=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 224, 224, 3)]     0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________</code></pre>
<p>이전과 다르게 <code>classification</code> layer이 삭제되었다. (<code>Flatten</code> ~ <code>Dense</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_model.trainable = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h3 id="2-기존의-네트워크-동결시키기"><a href="#2-기존의-네트워크-동결시키기" class="headerlink" title="2. 기존의 네트워크 동결시키기"></a>2. 기존의 네트워크 동결시키기</h3><p><code>trainable</code>옵션을 False로 하여 새로운 학습을 할 때에 기존의 학습된 네트워크가 같이 학습되지 않도록 합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inputs = keras.Input(shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># Separately from setting trainable on the model, we set training to False</span></span><br><span class="line">x = base_model(inputs, training=<span class="literal">False</span>)</span><br><span class="line">x = keras.layers.GlobalAveragePooling2D()(x)</span><br><span class="line"><span class="comment"># A Dense classifier with a single unit (binary classification)</span></span><br><span class="line">outputs = keras.layers.Dense(<span class="number">1</span>)(x)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_2 (InputLayer)         [(None, 224, 224, 3)]     0
_________________________________________________________________
vgg16 (Model)                (None, 7, 7, 512)         14714688
_________________________________________________________________
global_average_pooling2d (Gl (None, 512)               0
_________________________________________________________________
dense (Dense)                (None, 1)                 513
=================================================================
Total params: 14,715,201
Trainable params: 513
Non-trainable params: 14,714,688
_________________________________________________________________</code></pre>
<h4 id="about-layer"><a href="#about-layer" class="headerlink" title="about layer"></a>about layer</h4><ol>
<li>GlobalAveragePooing2D</li>
</ol>
<ul>
<li>기존의 fully convolutional Network는 많은 parameter수와 계산이 오래걸리고, 위치의 정보가 모두 사라지는 단점이 있습니다.</li>
<li>같은 chennel의 feature들의 평균을 내어 채널의 개수만큼 원소를 가지는 벡터를 출력 (이경우에는 입력 층이 (7,7,512)이므로 (512,)를 출력시키는 layer)</li>
<li>fully connected layer를 없애기 위한 방법</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/keras/_impl/keras/layers/pooling.py">코드</a></li>
</ul>
<ol start="2">
<li>Dense(1)</li>
</ol>
<ul>
<li>binary로 클래스를 분류할 것이기 때문에 0과 1로 classification</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Important to use binary crossentropy and binary accuracy as we now have a binary classification problem</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>), metrics=[keras.metrics.BinaryAccuracy()])</span><br></pre></td></tr></table></figure>
<h3 id="before-data-augmentation"><a href="#before-data-augmentation" class="headerlink" title="before data augmentation"></a>before data augmentation</h3><hr>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="/image/check.jpg" alt="check"></p>
<p>위 그림과같이 <code>ImageDataGenerator.flow_from_directory</code>를 사용할 때에 hidden folder가 있는지 확인해야합니다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dog</span><br><span class="line">        test</span><br><span class="line">                bo</span><br><span class="line">                not_bo</span><br><span class="line">        train</span><br><span class="line">                bo</span><br><span class="line">                not_bo</span><br></pre></td></tr></table></figure>
<p>위와 같이 directory별로 class를 구분하기 때문에 <code>.ipynb_checkpoints</code>폴더를 삭제해야합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="comment"># create a data generator</span></span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">        samplewise_center=<span class="literal">True</span>,  <span class="comment"># set each sample mean to 0</span></span><br><span class="line">        rotation_range=<span class="number">10</span>,  <span class="comment"># randomly rotate images in the range (degrees, 0 to 180)</span></span><br><span class="line">        zoom_range = <span class="number">0.1</span>, <span class="comment"># Randomly zoom image</span></span><br><span class="line">        width_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images horizontally (fraction of total width)</span></span><br><span class="line">        height_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images vertically (fraction of total height)</span></span><br><span class="line">        horizontal_flip=<span class="literal">True</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">        vertical_flip=<span class="literal">False</span>) <span class="comment"># we don&#x27;t expect Bo to be upside-down so we will not flip vertically</span></span><br></pre></td></tr></table></figure>
<h2 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load and iterate training dataset</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">train_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/train/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                        save_to_dir=&#x27;../sungjin/dog2/train/&#x27;, //2700여개의 파일이 생성됨</span></span><br><span class="line">                                       seed = seed,</span><br><span class="line">                                       target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                       color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                       class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                       batch_size=<span class="number">8</span>)</span><br><span class="line"><span class="comment"># load and iterate test dataset</span></span><br><span class="line">test_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/test/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                       save_to_dir=&#x27;../sungjin/dog2/test/&#x27;, // 1600여개의 파일이 생성됨</span></span><br><span class="line">                                      seed = seed,</span><br><span class="line">                                      target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                      color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                      class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                      batch_size=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 139 images belonging to 2 classes.
Found 20 images belonging to 2 classes.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_it, steps_per_epoch=<span class="number">12</span>, validation_data=test_it, validation_steps=<span class="number">4</span>, epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
12/12 [==============================] - 3s 225ms/step - loss: 1.4831 - binary_accuracy: 0.6562 - val_loss: 3.5233 - val_binary_accuracy: 0.5357
Epoch 2/20
12/12 [==============================] - 2s 178ms/step - loss: 1.1998 - binary_accuracy: 0.7473 - val_loss: 1.9760 - val_binary_accuracy: 0.6667
Epoch 3/20
12/12 [==============================] - 2s 161ms/step - loss: 0.4746 - binary_accuracy: 0.8462 - val_loss: 1.3999 - val_binary_accuracy: 0.7143
Epoch 4/20
12/12 [==============================] - 2s 172ms/step - loss: 0.3132 - binary_accuracy: 0.8646 - val_loss: 1.0363 - val_binary_accuracy: 0.7857
Epoch 5/20
12/12 [==============================] - 2s 175ms/step - loss: 0.2785 - binary_accuracy: 0.9011 - val_loss: 1.0750 - val_binary_accuracy: 0.7917
Epoch 6/20
12/12 [==============================] - 2s 178ms/step - loss: 0.0651 - binary_accuracy: 0.9670 - val_loss: 1.3541 - val_binary_accuracy: 0.7143
Epoch 7/20
12/12 [==============================] - 2s 198ms/step - loss: 0.1798 - binary_accuracy: 0.9167 - val_loss: 0.9755 - val_binary_accuracy: 0.8214
Epoch 8/20
12/12 [==============================] - 2s 166ms/step - loss: 0.0315 - binary_accuracy: 0.9890 - val_loss: 0.5196 - val_binary_accuracy: 0.9167
Epoch 9/20
12/12 [==============================] - 2s 179ms/step - loss: 0.0822 - binary_accuracy: 0.9341 - val_loss: 0.3601 - val_binary_accuracy: 0.8929
Epoch 10/20
12/12 [==============================] - 2s 181ms/step - loss: 0.0169 - binary_accuracy: 0.9896 - val_loss: 0.3563 - val_binary_accuracy: 0.9286
Epoch 11/20
12/12 [==============================] - 2s 191ms/step - loss: 0.0437 - binary_accuracy: 0.9890 - val_loss: 0.4602 - val_binary_accuracy: 0.8333
Epoch 12/20
12/12 [==============================] - 2s 158ms/step - loss: 0.0405 - binary_accuracy: 0.9890 - val_loss: 0.2497 - val_binary_accuracy: 0.8929
Epoch 13/20
12/12 [==============================] - 2s 191ms/step - loss: 0.0083 - binary_accuracy: 1.0000 - val_loss: 0.4642 - val_binary_accuracy: 0.8571
Epoch 14/20
12/12 [==============================] - 2s 163ms/step - loss: 0.0133 - binary_accuracy: 0.9890 - val_loss: 0.2259 - val_binary_accuracy: 0.9167
Epoch 15/20
12/12 [==============================] - 2s 178ms/step - loss: 0.0050 - binary_accuracy: 1.0000 - val_loss: 0.2705 - val_binary_accuracy: 0.8929
Epoch 16/20
12/12 [==============================] - 2s 185ms/step - loss: 0.0027 - binary_accuracy: 1.0000 - val_loss: 0.2617 - val_binary_accuracy: 0.8929
Epoch 17/20
12/12 [==============================] - 2s 179ms/step - loss: 0.0203 - binary_accuracy: 0.9890 - val_loss: 0.1751 - val_binary_accuracy: 0.9583
Epoch 18/20
12/12 [==============================] - 2s 173ms/step - loss: 0.0055 - binary_accuracy: 1.0000 - val_loss: 0.0705 - val_binary_accuracy: 1.0000
Epoch 19/20
12/12 [==============================] - 2s 199ms/step - loss: 0.0015 - binary_accuracy: 1.0000 - val_loss: 0.0275 - val_binary_accuracy: 1.0000
Epoch 20/20
12/12 [==============================] - 2s 189ms/step - loss: 0.0056 - binary_accuracy: 1.0000 - val_loss: 0.1403 - val_binary_accuracy: 0.9583</code></pre>
<h2 id="기존의-네트워크-동결-해제-및-fine-tuning"><a href="#기존의-네트워크-동결-해제-및-fine-tuning" class="headerlink" title="기존의 네트워크 동결 해제 및 fine-tuning"></a>기존의 네트워크 동결 해제 및 fine-tuning</h2><p>새롭게 컴파일할 때 기존의 네트워크를 unfreezing하여 fine-tuning(미세조정)한다. <br><br>단, 이때 다음과 같은 세가지의 전략이 있다.</p>
<p align = "center"><img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FH4rr7%2FbtqvBNchX4H%2FRmikaKBNm2N6VYD9JbI800%2Fimg.png", width="700px"></p>

<hr>
<ol>
<li>전체모델을 학습시키기</li>
</ol>
<ul>
<li>이 방법은 pre-trained model의 구조만 사용하고 새로운 데이터셋에 맞게 전부 새로 학습시키는 방법입니다. 큰 사이즈의 데이터셋이 필요합니다.</li>
</ul>
<br>

<ol start="2">
<li>일부의 층만 학습시키고 나머지는 동결시키기</li>
</ol>
<ul>
<li>초기 레이어는 추상적인(일반적인)특징을 추출하고 후기 레이어는 구체적이고 특유한 특징을 추출하는데 이런 특성을 이용해서 선택적으로 재학습시킵니다.</li>
<li>데이터셋이 작고 모델의 파라미터가 많다면 오버피팅이 될 수 있으므로 적은양의 계층을 학습시킵니다.</li>
<li>데이터셋이 크고 그에 비해 모델의 파라미터가 적다면 오버피팅의 가능성이 적으므로 더 많은 양의 계층을 학습시킵니다.</li>
</ul>
<br>

<ol start="3">
<li>전체 동결시키기 (classification만 학습시키기)</li>
</ol>
<ul>
<li>기존의 네트워크는 건들지 않고 그대로 두면서 특징 추출 메커니즘으로써 활용하고, classifier만 재학습시키는 방법을 씁니다.</li>
<li>데이터셋이 너무 작을때나 새로운 문제가 이전 모델이 이미 학습한 데이터셋과 매우 비슷할 때 사용합니다.</li>
</ul>
<hr>
<p align = "center"><img src = "https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FxjNT2%2FbtqvC7gR5Vw%2FWW1IlF9M9DTCypyNTeC2Wk%2Fimg.png", width= "900px"></p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unfreeze the base model</span></span><br><span class="line">base_model.trainable = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s important to recompile your model after you make any changes</span></span><br><span class="line"><span class="comment"># to the `trainable` attribute of any inner layer, so that your changes</span></span><br><span class="line"><span class="comment"># are taken into account</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = keras.optimizers.Adam(lr = <span class="number">.00001</span>),</span><br><span class="line"><span class="comment">#               optimizer=keras.optimizers.RMSprop(learning_rate = .00001),  # Very low learning rate</span></span><br><span class="line">              loss=keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              metrics=[keras.metrics.BinaryAccuracy()])</span><br></pre></td></tr></table></figure>
<h3 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a><a href="%22https://cs231n.github.io/neural-networks-3/#ada%22">optimizer</a></h3><p><img src = "/image/minibatch.jpg", width = "700px"></p>

<br>

<p><img src = "/image/optimizer.jpg", width = "700px"></p>

<p><img src = "https://cs231n.github.io/assets/nn3/opt2.gif",  width="400px"><img src = "https://cs231n.github.io/assets/nn3/opt1.gif", width = "400px"></p>

<p><a href="%22https://dalpo0814.tistory.com/29%22">reference</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_it, steps_per_epoch=<span class="number">12</span>, validation_data=test_it, validation_steps=<span class="number">4</span>, epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
12/12 [==============================] - 3s 219ms/step - loss: 0.0849 - binary_accuracy: 0.9670 - val_loss: 0.0158 - val_binary_accuracy: 1.0000
Epoch 2/20
12/12 [==============================] - 2s 204ms/step - loss: 0.0053 - binary_accuracy: 1.0000 - val_loss: 0.0118 - val_binary_accuracy: 1.0000
Epoch 3/20
12/12 [==============================] - 2s 176ms/step - loss: 0.0020 - binary_accuracy: 1.0000 - val_loss: 0.0015 - val_binary_accuracy: 1.0000
Epoch 4/20
12/12 [==============================] - 2s 196ms/step - loss: 1.5012e-04 - binary_accuracy: 1.0000 - val_loss: 0.0154 - val_binary_accuracy: 1.0000
Epoch 5/20
12/12 [==============================] - 3s 211ms/step - loss: 1.7610e-04 - binary_accuracy: 1.0000 - val_loss: 0.0034 - val_binary_accuracy: 1.0000
Epoch 6/20
12/12 [==============================] - 2s 198ms/step - loss: 1.0886e-04 - binary_accuracy: 1.0000 - val_loss: 0.0275 - val_binary_accuracy: 1.0000
Epoch 7/20
12/12 [==============================] - 2s 178ms/step - loss: 2.9665e-05 - binary_accuracy: 1.0000 - val_loss: 0.0093 - val_binary_accuracy: 1.0000
Epoch 8/20
12/12 [==============================] - 2s 205ms/step - loss: 1.7676e-05 - binary_accuracy: 1.0000 - val_loss: 0.0108 - val_binary_accuracy: 1.0000
Epoch 9/20
12/12 [==============================] - 2s 182ms/step - loss: 3.1658e-05 - binary_accuracy: 1.0000 - val_loss: 8.7812e-04 - val_binary_accuracy: 1.0000
Epoch 10/20
12/12 [==============================] - 2s 195ms/step - loss: 3.5808e-05 - binary_accuracy: 1.0000 - val_loss: 0.0020 - val_binary_accuracy: 1.0000
Epoch 11/20
12/12 [==============================] - 2s 191ms/step - loss: 3.2332e-05 - binary_accuracy: 1.0000 - val_loss: 0.0474 - val_binary_accuracy: 0.9643
Epoch 12/20
12/12 [==============================] - 2s 202ms/step - loss: 1.3189e-04 - binary_accuracy: 1.0000 - val_loss: 0.0044 - val_binary_accuracy: 1.0000
Epoch 13/20
12/12 [==============================] - 2s 202ms/step - loss: 1.7362e-04 - binary_accuracy: 1.0000 - val_loss: 0.0073 - val_binary_accuracy: 1.0000
Epoch 14/20
12/12 [==============================] - 2s 194ms/step - loss: 2.4271e-05 - binary_accuracy: 1.0000 - val_loss: 0.0235 - val_binary_accuracy: 1.0000
Epoch 15/20
12/12 [==============================] - 3s 219ms/step - loss: 1.0316e-05 - binary_accuracy: 1.0000 - val_loss: 0.0388 - val_binary_accuracy: 1.0000
Epoch 16/20
12/12 [==============================] - 2s 187ms/step - loss: 2.3622e-05 - binary_accuracy: 1.0000 - val_loss: 0.0031 - val_binary_accuracy: 1.0000
Epoch 17/20
12/12 [==============================] - 2s 201ms/step - loss: 2.0185e-05 - binary_accuracy: 1.0000 - val_loss: 0.0413 - val_binary_accuracy: 0.9643
Epoch 18/20
12/12 [==============================] - 2s 175ms/step - loss: 3.8968e-05 - binary_accuracy: 1.0000 - val_loss: 0.0197 - val_binary_accuracy: 1.0000
Epoch 19/20
12/12 [==============================] - 2s 202ms/step - loss: 9.7374e-06 - binary_accuracy: 1.0000 - val_loss: 0.0902 - val_binary_accuracy: 0.9643
Epoch 20/20
12/12 [==============================] - 2s 206ms/step - loss: 1.0378e-04 - binary_accuracy: 1.0000 - val_loss: 0.0148 - val_binary_accuracy: 1.0000</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputs = keras.Input(shape=(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># Separately from setting trainable on the model, we set training to False</span></span><br><span class="line">x = base_model(inputs, training=<span class="literal">False</span>)</span><br><span class="line">x = keras.layers.Flatten(input_shape=(<span class="number">7</span>,<span class="number">7</span>,<span class="number">512</span>))(x)</span><br><span class="line"><span class="comment"># x = keras.layers.Dense(256, activation=&#x27;relu&#x27;,input_dim=(7*7*512))(x)</span></span><br><span class="line"><span class="comment"># x = keras.layers.Dropout(0.2)(x)</span></span><br><span class="line">outputs = keras.layers.Dense(<span class="number">1</span>,activation = <span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_2 (InputLayer)         [(None, 224, 224, 3)]     0
_________________________________________________________________
vgg16 (Model)                (None, 7, 7, 512)         14714688
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
dense (Dense)                (None, 1)                 25089
=================================================================
Total params: 14,739,777
Trainable params: 25,089
Non-trainable params: 14,714,688
_________________________________________________________________</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Important to use binary crossentropy and binary accuracy as we now have a binary classification problem</span></span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line"><span class="comment">#     optimizer=keras.optimizers.Adam(),</span></span><br><span class="line">              loss=keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              metrics=[keras.metrics.BinaryAccuracy()])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="comment"># create a data generator</span></span><br><span class="line">datagen = ImageDataGenerator(</span><br><span class="line">        samplewise_center=<span class="literal">True</span>,  <span class="comment"># set each sample mean to 0</span></span><br><span class="line">        rotation_range=<span class="number">10</span>,  <span class="comment"># randomly rotate images in the range (degrees, 0 to 180)</span></span><br><span class="line">        zoom_range = <span class="number">0.1</span>, <span class="comment"># Randomly zoom image</span></span><br><span class="line">        width_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images horizontally (fraction of total width)</span></span><br><span class="line">        height_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images vertically (fraction of total height)</span></span><br><span class="line">        horizontal_flip=<span class="literal">True</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">        vertical_flip=<span class="literal">False</span>) <span class="comment"># we don&#x27;t expect Bo to be upside-down so we will not flip vertically</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load and iterate training dataset</span></span><br><span class="line"></span><br><span class="line">seed = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">train_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/train/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                        save_to_dir=&#x27;../sungjin/dog2/train/&#x27;, //2700여개의 파일이 생성됨</span></span><br><span class="line">                                       seed = seed,</span><br><span class="line">                                       target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                       color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                       class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                       batch_size=<span class="number">8</span>)</span><br><span class="line"><span class="comment"># load and iterate test dataset</span></span><br><span class="line">test_it = datagen.flow_from_directory(<span class="string">&#x27;../sungjin/dog/test/&#x27;</span>,</span><br><span class="line"><span class="comment">#                                       save_to_dir=&#x27;../sungjin/dog2/test/&#x27;, // 1600여개의 파일이 생성됨</span></span><br><span class="line">                                      seed = seed,</span><br><span class="line">                                      target_size=(<span class="number">224</span>, <span class="number">224</span>),</span><br><span class="line">                                      color_mode=<span class="string">&#x27;rgb&#x27;</span>,</span><br><span class="line">                                      class_mode=<span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                                      batch_size=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Found 139 images belonging to 2 classes.
Found 20 images belonging to 2 classes.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_it, steps_per_epoch=<span class="number">12</span>, validation_data=test_it, validation_steps=<span class="number">4</span>, epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
12/12 [==============================] - 3s 243ms/step - loss: 0.4516 - binary_accuracy: 0.8333 - val_loss: 0.8132 - val_binary_accuracy: 0.5000
Epoch 2/20
12/12 [==============================] - 2s 195ms/step - loss: 0.4823 - binary_accuracy: 0.8352 - val_loss: 0.8549 - val_binary_accuracy: 0.4583
Epoch 3/20
12/12 [==============================] - 2s 179ms/step - loss: 0.4232 - binary_accuracy: 0.8901 - val_loss: 0.8483 - val_binary_accuracy: 0.4643
Epoch 4/20
12/12 [==============================] - 2s 202ms/step - loss: 0.4383 - binary_accuracy: 0.8750 - val_loss: 0.8490 - val_binary_accuracy: 0.4643
Epoch 5/20
12/12 [==============================] - 2s 195ms/step - loss: 0.4781 - binary_accuracy: 0.8352 - val_loss: 0.8133 - val_binary_accuracy: 0.5000
Epoch 6/20
12/12 [==============================] - 2s 199ms/step - loss: 0.4341 - binary_accuracy: 0.8791 - val_loss: 0.7775 - val_binary_accuracy: 0.5357
Epoch 7/20
12/12 [==============================] - 2s 204ms/step - loss: 0.4526 - binary_accuracy: 0.8646 - val_loss: 0.7197 - val_binary_accuracy: 0.6071
Epoch 8/20
12/12 [==============================] - 2s 170ms/step - loss: 0.4012 - binary_accuracy: 0.9121 - val_loss: 0.8549 - val_binary_accuracy: 0.4583
Epoch 9/20
12/12 [==============================] - 2s 197ms/step - loss: 0.4823 - binary_accuracy: 0.8352 - val_loss: 0.8490 - val_binary_accuracy: 0.4643
Epoch 10/20
12/12 [==============================] - 2s 201ms/step - loss: 0.4070 - binary_accuracy: 0.9062 - val_loss: 0.8133 - val_binary_accuracy: 0.5000
Epoch 11/20
12/12 [==============================] - 2s 205ms/step - loss: 0.5111 - binary_accuracy: 0.8022 - val_loss: 0.7716 - val_binary_accuracy: 0.5417
Epoch 12/20
12/12 [==============================] - 2s 181ms/step - loss: 0.4341 - binary_accuracy: 0.8791 - val_loss: 0.8490 - val_binary_accuracy: 0.4643
Epoch 13/20
12/12 [==============================] - 3s 216ms/step - loss: 0.4383 - binary_accuracy: 0.8750 - val_loss: 0.8133 - val_binary_accuracy: 0.5000
Epoch 14/20
12/12 [==============================] - 2s 190ms/step - loss: 0.4451 - binary_accuracy: 0.8681 - val_loss: 0.8133 - val_binary_accuracy: 0.5000
Epoch 15/20
12/12 [==============================] - 2s 191ms/step - loss: 0.4603 - binary_accuracy: 0.8571 - val_loss: 0.8847 - val_binary_accuracy: 0.4286
Epoch 16/20
12/12 [==============================] - 2s 202ms/step - loss: 0.4278 - binary_accuracy: 0.8854 - val_loss: 0.8133 - val_binary_accuracy: 0.5000
Epoch 17/20
12/12 [==============================] - 2s 205ms/step - loss: 0.4451 - binary_accuracy: 0.8681 - val_loss: 0.8549 - val_binary_accuracy: 0.4583
Epoch 18/20
12/12 [==============================] - 2s 179ms/step - loss: 0.4781 - binary_accuracy: 0.8352 - val_loss: 0.8490 - val_binary_accuracy: 0.4643
Epoch 19/20
12/12 [==============================] - 2s 207ms/step - loss: 0.4695 - binary_accuracy: 0.8438 - val_loss: 0.7775 - val_binary_accuracy: 0.5357
Epoch 20/20
12/12 [==============================] - 2s 185ms/step - loss: 0.4232 - binary_accuracy: 0.8901 - val_loss: 0.8549 - val_binary_accuracy: 0.4583</code></pre>
<h2 id="Global-Average-Pooing-vs-Fully-Convolutional-Network"><a href="#Global-Average-Pooing-vs-Fully-Convolutional-Network" class="headerlink" title="Global Average Pooing vs Fully Convolutional Network"></a><code>Global Average Pooing</code> vs <code>Fully Convolutional Network</code></h2><p><img src = "/image/GAP.jpg", width = "400px"><img src = "/image/fullyConvolution.jpg", width = "400px"></p>




      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/2021/01/25/java-day4/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>PREV</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime" title="업데이트 시간"></i>
              2021-01-27
            </span>
            
                  <span class="post-categories">
                    <i class="iconfont icon-bookmark" title="카테고리"></i>
                    
                    <span class="span--category">
                      <a href="/categories/AAIS/" title="AAIS">
                        <b>#</b> AAIS
                      </a>
                    </span>
                    
                  </span>
              
                  <span class="post-tags">
                    <i class="iconfont icon-tags" title="태그"></i>
                    
                    <span class="span--tag">
                      <a href="/tags/AI/" title="AI">
                        <b>#</b> AI
                      </a>
                    </span>
                    
                    <span class="span--tag">
                      <a href="/tags/AAIS/" title="AAIS">
                        <b>#</b> AAIS
                      </a>
                    </span>
                    
                    <span class="span--tag">
                      <a href="/tags/keras/" title="keras">
                        <b>#</b> keras
                      </a>
                    </span>
                    
                  </span>
              
          </div>
          <div class="post-foot-prev">
            
              <a href="/2021/01/29/aais-05/" target="_self">
                <span>NEXT</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    
  <div class="post-catalog" id="catalog">
    <div class="title">목록</div>
    <div class="catalog-content">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transfer-Learning"><span class="toc-text">Transfer Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#About-VGG16"><span class="toc-text">About VGG16</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transfer-learning"><span class="toc-text">transfer learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-classification%EC%B8%B5%EC%9D%84-%EC%82%AD%EC%A0%9C"><span class="toc-text">1. classification층을 삭제</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%EA%B8%B0%EC%A1%B4%EC%9D%98-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EB%8F%99%EA%B2%B0%EC%8B%9C%ED%82%A4%EA%B8%B0"><span class="toc-text">2. 기존의 네트워크 동결시키기</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#about-layer"><span class="toc-text">about layer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#before-data-augmentation"><span class="toc-text">before data augmentation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#data-augmentation"><span class="toc-text">data augmentation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%EA%B8%B0%EC%A1%B4%EC%9D%98-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EB%8F%99%EA%B2%B0-%ED%95%B4%EC%A0%9C-%EB%B0%8F-fine-tuning"><span class="toc-text">기존의 네트워크 동결 해제 및 fine-tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#optimizer"><span class="toc-text">optimizer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Global-Average-Pooing-vs-Fully-Convolutional-Network"><span class="toc-text">Global Average Pooing vs Fully Convolutional Network</span></a></li></ol></li></ol>
    </div>
  </div>

  
<script src="/js/catalog.js"></script>




    
      <div class="comments-container">
        





      </div>
    
  </div>


        <div class="footer">
  <div class="social">
    <ul>
      
        <li>
          <a title="github" target="_blank" rel="noopener" href="https://github.com/saintrealchoi">
            <i class="iconfont icon-github"></i>
          </a>
        </li>
      
        <li>
          <a title="instagram" href="">
            <i class="iconfont icon-instagram"></i>
          </a>
        </li>
      
        <li>
          <a title="rss" href="/atom.xml">
            <i class="iconfont icon-rss"></i>
          </a>
        </li>
      
    </ul>
  </div>
  
    <div class="footer-more">
      <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Copyright © Oranges 2020</a>
    </div>
  
    <div class="footer-more">
      <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Theme by Oranges | Powered by Hexo</a>
    </div>
  
</div>

      </div>

      <div class="back-to-top hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/js/backtotop.js"></script>



      
  <div class="search-icon" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        
          <input type="text" class="search-input" id="search-input" placeholder="검색">
        
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile()
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        // inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)

    var searchFunc = function (path, search_id, content_id) {
      'use strict';
      var $input = document.getElementById(search_id);
      var $resultContent = document.getElementById(content_id);
      $resultContent.innerHTML = "<ul><span class='local-search-empty'>첫 번째 검색, 인덱스 파일을 불러오고 있습니다. 잠시 후 …<span></ul>";
      $.ajax({
        // 0x01. load xml file
        url: path,
        dataType: "xml",
        success: function (xmlResponse) {
          // 0x02. parse xml file
          var datas = $("entry", xmlResponse).map(function () {
            return {
              title: $("title", this).text(),
              content: $("content", this).text(),
              url: $("url", this).text()
            };
          }).get();
          $resultContent.innerHTML = "";

          $input.addEventListener('input', function () {
            // 0x03. parse query to keywords list
            var str = '<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length <= 0) {
              return;
            }
            // 0x04. perform local searching
            datas.forEach(function (data) {
              var isMatch = true;
              var content_index = [];
              if (!data.title || data.title.trim() === '') {
                data.title = "Untitled";
              }
              var orig_data_title = data.title.trim();
              var data_title = orig_data_title.toLowerCase();
              var orig_data_content = data.content.trim().replace(/<[^>]+>/g, "");
              var data_content = orig_data_content.toLowerCase();
              var data_url = data.url;
              var index_title = -1;
              var index_content = -1;
              var first_occur = -1;
              // only match artiles with not empty contents
              if (data_content !== '') {
                keywords.forEach(function (keyword, i) {
                  index_title = data_title.indexOf(keyword);
                  index_content = data_content.indexOf(keyword);

                  if (index_title < 0 && index_content < 0) {
                    isMatch = false;
                  } else {
                    if (index_content < 0) {
                      index_content = 0;
                    }
                    if (i == 0) {
                      first_occur = index_content;
                    }
                    // content_index.push({index_content:index_content, keyword_len:keyword_len});
                  }
                });
              } else {
                isMatch = false;
              }
              // 0x05. show search results
              if (isMatch) {
                str += "<li><a href='" + data_url + "' class='search-result-title'>" + orig_data_title + "</a>";
                var content = orig_data_content;
                if (first_occur >= 0) {
                  // cut out 100 characters
                  var start = first_occur - 20;
                  var end = first_occur + 80;

                  if (start < 0) {
                    start = 0;
                  }

                  if (start == 0) {
                    end = 100;
                  }

                  if (end > content.length) {
                    end = content.length;
                  }

                  var match_content = content.substr(start, end);

                  // highlight all keywords
                  keywords.forEach(function (keyword) {
                    var regS = new RegExp(keyword, "gi");
                    match_content = match_content.replace(regS, "<span class=\"search-keyword\">" + keyword + "</span>");
                  });

                  str += "<p class=\"search-result-abstract\">" + match_content + "...</p>"
                }
                str += "</li>";
              }
            });
            str += "</ul>";
            if (str.indexOf('<li>') === -1) {
              return $resultContent.innerHTML = "<ul><span class='local-search-empty'>검색어를 찾을 수 없습니다. 검색어를 바꿔 보세요<span></ul>";
            }
            $resultContent.innerHTML = str;
          });
        }
      });
      $(document).on('click', '#search-close-icon', function() {
        $('#search-input').val('');
        $('#search-result').html('');
      });
    }

    var getSearchFile = function() {
        var path = "/search.xml";
        searchFunc(path, 'search-input', 'search-result');
    }
  </script>




    </div>
  <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=i;var e=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}i(),n.addEventListener("scroll",function(){var t,e;t=i,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>
</html>
